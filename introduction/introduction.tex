\chapter{Introduction}

Cross-validation (CV) is a popular method for model selection.  It works by estimating the prediction error of each model under consideration and then choosing the model with the best performance.  In unsuperfised contexts, though, there is no clear notion as to what exactly ``prediction error'' is.  Therefore, it is difficult to employ cross-validation for model selection in unsupervised or exploratory contexts.  In this thesis, we take a look at some extensions of cross-validation to unsupervised learing.  We focus specifically on the problem of choosing how many components to keep for principal components analysis (PCA), but many of the concepts we introduce are more broadly applicable.

Before we can do anything, we need a solid theoretical foundation.  To this end, Chapter~\ref{C:multivariate-background} gives a survey of relevant results from multivariate statistics and random matrix theory.  Then, Chapter~\ref{C:svd-behavior} derives the behavor of the singular value decomposition (SVD) for ``signal-plus-noise'' matrix models.  These two chapters are complemented by Appendices~\ref{A:projections}~and~\ref{A:weighted-sums}, which collect properties of random orthogonal matrices and give some limit theorems for weighted sums of random variables.  Collectively, this work proveds the groundwork for the rest of the thesis.

In Chapter~\ref{C:intrinsic-rank}, we introduce the latent factor model.  This model is a generative model for signal-plus-noise matrix data that expands the setup of PCA to include correlated factor loadings.  We motivate loss functions for estimating the signal part, and then show how the SVD performs with respect to these criteria.  The latent factor model and the associated loss functions are sufficiently general as to be relevant to many different application areas.

The next chapter (Chapter~\ref{C:cvul}) focuses on cross-validation stragegies.  It covers both Wold-style ``speckled'' hold-outs as well as Gabriel-style ``blocked'' hold-outs.  We define model error and prediction error for the latent factor model, and present the two cross-validation methods as estimators of prediction error.  The chapter includes a comparison of CV methods with parametric model-selection procedures, showing through simulation that CV is much more robust to violations in model assumptions.  In situations where parametric assumptions are unreasonable, cross-validation is an attractive method for model selection.

Chapter~\ref{C:optimal-leave-out}, the final chapter, is a theoretical analysis of Gabriel-style cross-validation for the SVD, also known as bi-cross-validation (BCV).  This chapter shows that BCV is in general a biased estimator of model error, with an explicit expression for the bias.  Despite this bias, though, the procedure can still be used successfully for model selection, provied the leave-out sizes are chosen appropriately.  The chapter shows how to choose the leave-out sizes and proves a weak form of consistency.

Cross-validation is a valuable and flexible procedure for model selection.  Through theory and simulation, this thesis demonstrates the applicability and utility of cross-validation as applied to principal components analysis.  Many of the ideas in this thesis generalized to related procedures like clustering and manifold learning.  Cross-validation can be used successfully for model selection in a variety of unsupervised learning contexts.
