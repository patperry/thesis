\chapter{Introduction}

Cross-validation (CV) is a popular method for model selection.  It works by estimating the prediction error of each model under consideration and then choosing the model with the best performance.  In unsupervised contexts, though, there is no clear notion as to what exactly ``prediction error'' is.  Therefore, it is difficult to employ cross-validation for model selection in unsupervised or exploratory contexts.  In this thesis, we take a look at some extensions of cross-validation to unsupervised learning.  We focus specifically on the problem of choosing how many components to keep for principal component analysis (PCA), but many of the concepts we introduce are more broadly applicable.

Before we can do anything, we need a solid theoretical foundation.  To this end, Chapter~\ref{C:multivariate-background} gives a survey of relevant results from multivariate statistics and random matrix theory.  Then, Chapter~\ref{C:svd-behavior} derives the behavior of the singular value decomposition (SVD) for ``signal-plus-noise'' matrix models.  These two chapters are complemented by Appendices~\ref{A:projections}~and~\ref{A:weighted-sums}, which collect properties of random orthogonal matrices and give some limit theorems for weighted sums of random variables.  Collectively, this work provides the groundwork for the rest of the thesis.

In Chapter~\ref{C:intrinsic-rank}, we introduce the latent factor model.  This is a generative model for signal-plus-noise matrix data that expands the setup of PCA to include correlated factor loadings.  We motivate loss functions for estimating the signal part, and then show how the SVD performs with respect to these criteria.  

The next chapter (Chapter~\ref{C:cvul}) focuses on cross-validation strategies.  It covers both Wold-style ``speckled'' hold-outs as well as Gabriel-style ``blocked'' hold-outs.  We define model error and prediction error for the latent factor model, and present the two cross-validation methods as estimators of prediction error.  The chapter includes a comparison of CV methods with parametric model-selection procedures, showing through simulation that CV is much more robust to violations in model assumptions.  In situations where parametric assumptions are unreasonable, cross-validation proves to be an attractive method for model selection.

Chapter~\ref{C:optimal-leave-out}, the final chapter, contains a theoretical analysis of Gabriel-style cross-validation for the SVD, also known as bi-cross-validation (BCV).  This chapter shows that BCV is in general a biased estimator of model error, with an explicit expression for the bias.  Despite this bias, though, the procedure can still be used successfully for model selection, provided the leave-out sizes are chosen appropriately.  The chapter shows how to choose the leave-out sizes and proves a weak form of consistency.

Cross-validation is a valuable and flexible procedure for model selection.  Through theory and simulation, this thesis demonstrates the applicability and utility of cross-validation as applied to principal component analysis.  Many of the ideas in the following chapters generalize to other unsupervised learning procedures.  This thesis shows that cross-validation can successfully be used for model selection in a variety of contexts.
