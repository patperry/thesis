
\chapter{Properties of random projections}

We use this section to present some results about random projection
matrices.  We call a symmetric $p \times p$ matrix $\mP$ a projection 
if its eigenvalues are in the set $\{ 0, 1 \}$.  Any projection matrix of rank
$k \leq p$ can be decomposed as $\mP = \mV \mV^\trans$ for some $\mV$
satisfying $\mV^\trans \mV = \mI_k$.  We call the set 
\[
    \stiefel_{k}\left( \reals^p \right)
        =
        \left\{
            \mV \in \reals^{p \times k}
            :
            \mV^\trans \mV = \mI_k
        \right\}
        \subseteq
        \reals^{p \times k}
\]
the \emph{rank-$k$ Stiefel manifold of $\reals^p$}.  It is the set
of orthonormal $k$-frames in $\reals^p$.  Similarly, we call 
the set
\[
    \grassmann_k\left( \reals^p \right)
        =
        \left\{
            \mV \mV^\trans
            :
            \mV \in \stiefel_k \left( \reals^p \right)
        \right\}
        \subseteq
        \reals^{p \times p}
\]
the \emph{rank-$k$ Grassmannian of $\reals^p$}; this is the set of
rank $k$ projection matrices.  If
\(
    \left(
    \begin{matrix}
        \mV & \mbV
    \end{matrix}
    \right)
\)
is an $p \times p$ Haar-distributed orthogonal matrix and $\mV$ is
$p \times k$, then we say that $\mV$ is uniformly distributed
over $\stiefel_k( \reals^p )$ and that $\mV \mV^\trans$ is uniformly
distributed over $\grassmann_k(\reals^p)$.


\section{Uniformly distributed orthonormal $k$-frames}

We first present some results about a matrix $\mV$ distributed uniformly
over $\stiefel_k(\reals^p)$.  We denote this distribution by
\(
    \mV 
        \sim
            \Unif \big(
                \stiefel_k(\reals^p)
            \big).
\)
In the special case of $k = 1$, the distribution is equivalent to drawing
a random vector uniformly from the unit sphere in $\reals^p$.  We denote 
this distribution by
\(
    \vV
        \sim
            \Unif \big(
                \sphere^{p-1}
            \big).
\)


\subsection{Generating random elements}

The easiest way to generate a random element of $\stiefel_k(\reals^p)$ is
to let $\mZ$ to be an $p \times k$ matrix of \iid $\Normal( 0, \, 1)$
random variables, take the $QR$ decomposition $\mZ = \mQ \mR$, and let
$\mV$ be equal to the first $k$ columns of $\mQ$.  In practice, there is
a bias in the way standard $QR$ implementations choose the signs of the
columns of $\mQ$.  To get around this, we recommend using 
Algorithm~\ref{A:random-steifel}, below.

\begin{algorithm}
    \caption{\label{A:random-steifel}Generate a random orthonormal $k$-frame}
    \begin{enumerate}
        \item Draw $\mZ$, a random $p \times k$ matrix whose elements are 
              \iid $\Normal( 0, \, 1 )$ random variables.
              
        \item Compute $\mZ = \mQ \mR$, the $QR$-decomposition of $\mZ$.
              Set $\mQ_1$ to be the $p \times k$ matrix containing the
              first $k$ columns of $\mQ$.

        \item Draw $\mS$, a random $k \times k$ diagonal matrix with \iid 
              diagonal entries such that 
              \(
                  \Prob\left\{ S_{11} = -1 \right\}
                  =
                  \Prob\left\{ S_{11} = +1 \right\}
                  =
                  \frac{1}{2}.
              \)
        \item Return $\mV = \mQ_1 \mS$.
    \end{enumerate}
\end{algorithm}

\noindent 
This algorithm has a time complexity of $\Oh\left( p k^2 \right)$. Diaconis
and Shahshahani~\cite{diaconis1987sag} present an alternative approach called
the subgroup algorithm which can be used to generate $\mV$ as a product of 
$k$ Householder reflections.  Their algorithm has time complexity 
$\Oh\left( p k \right)$.  Mezzadri~\cite{mezzadri2007grm} gives a readable 
description of the subgroup algorithm.


\subsection{Mixed moments}

Since we can flip the sign of any row or column of $\mV$ and not change
its distribution, the mixed moments of the elements of $\mV$ vanish unless 
the number of elements from any row or column is even (counting multiplicity).  For example, $\E \left[ V_{11}^2 V_{21} \right] = 0$ since we can flip the sign of the second row of $\mV$ to get
\[
    V_{11}^2 V_{21} 
    \eqd
    V_{11}^2 (- V_{21})
    =
    -V_{11}^2 V_{21}.
\]
This argument does not apply to $V_{11} V_{12} V_{22} V_{21}$ or
$V_{11}^2 V_{22}^2$.


In the special case when $k = 1$, the vector
\(
    \left(
        V_{11}^2, V_{21}^2, \ldots, V_{p1}^2
    \right)
\)
is distributed as 
\(
    \Dirichlet\left( \frac{1}{2}, \frac{1}{2}, \ldots, \frac{1}{2} \right).
\)
This follows from the fact that if $Y_1, Y_2, \ldots, Y_p$ are independently distributed Gamma random variables and $Y_i$ has shape
$a_i$ and scale $s$, then with $S = \sum_{i=1}^p Y_i$, the vector
\(
    \left(
        \frac{Y_1}{S},
        \frac{Y_2}{S},
        \ldots
        \frac{Y_p}{S}
    \right)
\)
is distributed $\Dirichlet\left( a_1, a_2, \ldots, a_p \right)$.  Using
the standard formulas for Dirichlet variances and covariances, we get
the mixed moments up to fourth order.  They are summarized in the next
lemma.

\begin{lemma}\label{L:stiefel-1-moments}
    If $\mV \sim \Unif\big( \stiefel_1(\reals^p) \big)$, then
    \begin{subequations}
    \begin{align*}
        \E \left[ V_{11}^2 \right] 
            &= \frac{1}{p}, \\
        \E \left[ V_{11} V_{21} \right] 
            &= 0, \\
        \E \left[ V_{11}^4 \right] 
            &= \frac{3}{p \, (p + 2)}, \\
        \E \left[ V_{11}^2 V_{21}^2 \right] 
            &= \frac{1}{p \, (p + 2)}.
    \end{align*}
    \end{subequations}
    The odd mixed moments are all equal to zero.
\end{lemma}

Using Theorem~4 from Diaconis and Shahshahani~\cite{diaconis1994erm},
which gives the moments of the traces of Haar-distributed orthogonal
matrices, we can compute the mixed moments of $\mV$ for more general $k$.
Meckes~\cite{meckes2006ivs} gives an alternative derivation of these
results.

\begin{lemma}\label{L:stiefel-k-moments}
    If $\mV \sim \Unif\big( \stiefel_k(\reals^p) \big)$ and $k > 1$, then the
    nonzero mixed moments of the elements $\mV$ up to fourth order
    are defined by
    \begin{subequations}
    \begin{align*}
        \E \left[ V_{11}^2 \right] 
            &= 
                \frac{1}{ p }, \\
        \E \left[ V_{11}^4 \right] 
            &=
                \frac{3}{ p \, (p+2) }, \\
        \E \left[ V_{11}^2 V_{21}^2 \right] 
            &= 
                \frac{1}{ p \, (p+2) }, \\
        \E \left[ V_{11}^2 V_{22}^2 \right]
            &= 
                \frac{p+1}{ p \, (p-1) (p+2) }, \\
        \E \left[ V_{11} V_{12} V_{22} V_{21} \right] 
            &= 
                \frac{ -1 }{ p \, (p-1) (p+2) }.
    \end{align*}
    \end{subequations}
\end{lemma}
\begin{proof}
    The first three equations follow directly from the previous lemma.
    We can get the other moments from the moments of $\mO$, a Haar-distibuted
    $p \times p$ orthogonal matrix. For the fourth equation, we use that 
    \[
        \E \left[ \tr( \mO ) \right]^4
        =
        \sum_{r,s,t,u} \E \left[ O_{rr} O_{ss} O_{tt} O_{uu} \right].
    \]
    Only the terms with even powers of $O_{ii}$ are nonzero.  Thus, we have
    \begin{align*}
        \E \left[ \tr( \mO ) \right]^4
        &=   \binom{p}{1} \E \left[ O_{11}^4 \right]
           + \binom{p}{2} \binom{4}{2} \E \left[ O_{11}^2 O_{22}^2 \right] \\
        &=   p \, \E \left[ O_{11}^4 \right]
           + 3p \, (p-1) \, \E \left[ O_{11}^2 O_{22}^2 \right].
    \end{align*}
    Theorem~4 of Diaconis and Shahshahani \cite{diaconis1994erm} gives that 
    $\E \left[ \tr(\mO) \right]^4 = 3$.  Combined with
    Lemma~\ref{L:stiefel-1-moments}, we get that
    \begin{align*}
        \E \left[ O_{11}^2 O_{22}^2 \right]
        &= \frac{1}{3p \, (p-1)} 
            \left\{
                \E \left[ \tr(\mO) \right]^4
                - p \, \E \left[ O_{11}^4 \right]
            \right\} \\
        &= \frac{1}{3p \, (p-1)} 
           \left\{
                3 - p \cdot \frac{3}{ p \, (p+1) }
           \right\} \\
        &= \frac{p+1}{ p \, (p-1) (p+2) }.
    \end{align*}
    
    For the last equation, we use $\E\left[ \tr(\mO^4) \right]$.  We have that 
    \[
        (\mO^4)_{ij} = \sum_{r,s,t} O_{ir} O_{rs} O_{st} O_{tj}.
    \]
    We would like to compute $\E\left[ (\mO^4)_{11} \right]$.  Note that
    unless $r=s=t=1$, there are only three situations when
    $\E \left[ O_{1r} O_{rs} O_{st} O_{t1} \right] \neq 0$.   Two of them are 
    demonstrated visually by the configurations
    \[
        \begin{matrix}
            \phantom{0} &
            \begin{matrix} 
                1 & \phantom{\cdots} & s \phantom{\cdots}
            \end{matrix} \\
            \begin{matrix}
                1 \\
                \phantom{\vdots}  \\
                s \\
                \phantom{\vdots}
            \end{matrix}&
            \left(
            \begin{matrix}
                O_{1r} & \cdots & O_{rs} & \cdots \\
                \vdots  & \ddots & \vdots  & \\
                O_{t1} & \cdots & O_{st} & \cdots \\
                \vdots  &        & \vdots  &
            \end{matrix}
            \right) \\
            \phantom{0} &
            r=1, s=t, s \neq 1
        \end{matrix}
        \quad\quad\text{and}\quad\quad
        \begin{matrix}
            \phantom{0} &
            \begin{matrix} 
                1 & \phantom{\cdots} & r \phantom{\cdots}
            \end{matrix} \\
            \begin{matrix}
                1 \\
                \phantom{\vdots}  \\
                r \\
                \phantom{\vdots}
            \end{matrix}&
            \left(
            \begin{matrix}
                O_{t1} & \cdots & O_{1r} & \cdots \\
                \vdots  & \ddots & \vdots  & \\
                O_{st} & \cdots & O_{rs} & \cdots \\
                \vdots  &        & \vdots  &
            \end{matrix}
            \right) \\
            \phantom{0} &
            t=1, r=s, r \neq 1
        \end{matrix}.
    \]
    The other nonzero term is when $s=1$, $r=t$, and $r\neq1$, so
    that $O_{1r} O_{rs} O_{st} O_{t1} = O_{1r}^2 O_{r1}^2$.
    In all other configurations there is a row or a column that only
    contains one of $\{ O_{1r}, \, O_{rs}, \, O_{st}, \, O_{t1} \}$.  Since 
    we can multiply a row or a column of $\mO$ by $-1$ and not change the
    distribution of $\mO$, for this choice of $r$, $s$, and $t$ we must have
    $O_{1r} O_{rs} O_{st} O_{t1} \overset{d}{=} -O_{1r} O_{rs} O_{st} O_{t1}$.
    This in turn implies that 
    $\E\left[O_{1r} O_{rs} O_{st} O_{t1} \right] = 0$.
    With these combinatorics in mind, we have that
    \begin{align*}
        \E \! \left[ (\mO^4)_{11} \right]
        \!&=\!
            \sum_{s \neq 1} \E \left[ O_{11} O_{1s} O_{ss} O_{s1} \right]
            +
            \sum_{r \neq 1} \E \left[ O_{1r} O_{rr} O_{r1} O_{11} \right]
            +
            \sum_{r \neq 1} \E \left[ O_{1r}^2 O_{r1}^2 \right]
            + 
            \E \left[ O_{11}^4 \right] \\
        &=
            2(p-1) \E \left[ O_{11} O_{12} O_{22} O_{21} \right]
            + (p-1) \E \left[ O_{12}^2 O_{21}^2 \right]
            + \E \left[ O_{11}^4 \right] \\
        &=
            2(p-1) \E \left[ O_{11} O_{12} O_{22} O_{21} \right]
            + (p-1) \E \left[ O_{11}^2 O_{22}^2 \right]
            + \E \left[ O_{11}^4 \right]
    \end{align*}
    Again applying Theorem~4 of \cite{diaconis1994erm}, we have that 
    $\E \left[ \tr (\mO^4) \right] = 1$.  Combined with 
    Lemma~\ref{L:stiefel-1-moments}, we
    have
    \begin{align*}
        \E \left[ O_{11} O_{12} O_{22} O_{21} \right]
        &= 
        \frac{1}{2(p-1)}
        \left\{
            \frac{1}{p} \E \left[ \tr (\mO^4) \right]
            - \E\left[ O_{11}^4 \right]
            - (p-1) \E \left[ O_{11}^2 O_{22}^2 \right]
        \right\} \\
        &=
        \frac{1}{2(p-1)}
        \left\{
            \frac{1}{p}
            - \frac{3}{ p \, (p+2) }
            - (p-1) \frac{p+1}{ p \, (p-1) (p+2) }
        \right\} \\
        &= 
        \frac{-1}{ p \, (p-1) (p+2) }. \qedhere
    \end{align*} 
\end{proof}


\section{Uniformly distributed projections}

When $\mP \in \reals^{p \times p}$ is chosen uniformly over the set of rank-$k$ $p \times p$ projection matrices, we say
\(
    \mP
        \sim
            \Unif \big(
                \grassmann_k(\reals^p)
            \big).
\)
With the results of the previous section, we can derive the moments of
random projection matrices.


\begin{lemma}\label{L:grassman-moments-1}
    If $\mP \sim \Unif \big( \grassmann_k(\reals^p) \big)$, then
    \begin{align*}
        \E \left[ \mP \right] &= \frac{k}{p} \mI_p.
    \end{align*}
\end{lemma}
\begin{proof}
    Write $\mP = \mV \mV^\trans$, where 
    \(
        \mV \sim \Unif \big( \stiefel_k( \reals^p ) \big)
    \).
    For $1 \leq i,j \leq p$ we have
    \[
        P_{ij}
            =
                \sum_{r=1}^k V_{ir} V_{jr},
    \]
    so
    \[
        \E \left[ P_{ij} \right] 
        =
        \begin{cases}
            k \, \E \left[ V_{11}^2 \right], &\text{when $i = j$,} \\
            k \, \E \left[ V_{11} V_{21} \right], &\text{otherwise.}
        \end{cases}
    \]
    The result now follows from Lemma~\ref{L:stiefel-1-moments}.
\end{proof}

\begin{lemma}\label{L:grassman-moments-2}
    Let $\mP \sim \Unif \big( \grassmann_k(\reals^p) \big)$.  If
    $1 \leq i,j,i',j' \leq p$, then
    \begin{equation*}
        \begin{split}
            \cov \big[ P_{ij}, P_{i'j'} \big]
                &=
                    \frac{1}{(p-1)(p+2)}
                    \left(
                        \frac{k}{p}
                    \right)
                    \left(
                        1 - \frac{k}{p}
                    \right) \\
                &\qquad\cdot
                    \Big(
                          p \, \delta_{(i,j ) = (i',j')}
                        + p \, \delta_{(i,j ) = (j',i')}
                        - 2 \, \delta_{(i,i') = (j ,j')}
                    \Big).
        \end{split}
    \end{equation*}
    This gives us that aside from the obvious symmetry ($P_{ij} = P_{ji}$), 
    the off-diagonal elements of $\mP$ are uncorrelated with each other and 
    with the diagonal elements.
\end{lemma}
\begin{proof}
    We need to perform six computations.  As before, we use the representation
    $\mP = \mV \mV^\trans$, where 
    \(
        \mV \sim \Unif\big( \stiefel_k(\reals^p) \big).
    \)
    We have
    \begin{align*}
        \E \left[ P_{11}^2 \right]
        &= \E \bigg[ \Big( 
                         \sum_{i=1}^k V_{1i}^2
                     \Big)^2
              \bigg] \\
        &= \E \bigg[
                  \sum_{i=1}^k V_{1i}^4
                  + \sum_{i\neq j} V_{1i}^2 V_{1j}^2
              \bigg] \\
        &= k \cdot \frac{3}{ p \, (p+2) } 
           + k \, (k-1) \cdot \frac{1}{ n \, (p+2) } \\
        &= \frac{ k \, (k+2) }{ p \,(p+2) } \\
        &= \frac{2}{ p+2}
           \left(\frac{k}{p}\right) \left( 1 - \frac{k}{p} \right)
           + \left( \frac{k}{p} \right)^2,
    \end{align*}
    which gives us that 
    \[
        \var \left[ P_{11} \right]
        = \frac{2}{ p+2 }
           \left(\frac{k}{p}\right) \left( 1 - \frac{k}{p} \right).
    \]
    Next,
    \begin{align*}
        \E \left[ P_{12}^2 \right]
        &= \E \bigg[ \Big( 
                         \sum_{i=1}^k V_{1i} V_{2i}
                     \Big)^2
              \bigg] \\
        &= \E \bigg[ 
                  \sum_{i=1}^k V_{1i}^2 V_{2i}^2
                  + \sum_{i\neq j} 
                        V_{1i} V_{2i} V_{1j} V_{2j}
              \bigg] \\
        &= k \cdot \frac{ 1 }{ p \, (p+2) }
           + k \, (k-1) \cdot \frac{-1}{ p \, (p-1) (p+2) } \\
        &= \frac{p}{ (p-1) (p+2) } 
           \left(\frac{k}{p}\right) \left( 1 - \frac{k}{p} \right),
    \end{align*}
    so that
    \[
        \var \left[ P_{12} \right]
        = \frac{p}{ (p-1) (p+2) } 
           \left(\frac{k}{p}\right) \left( 1 - \frac{k}{p} \right).
    \]
    Also,
    \begin{align*}
        \E \left[ P_{11} P_{22} \right]
        &= \E \bigg[
                \Big( \sum_{i=1}^k V_{1i}^2 \Big)
                \Big( \sum_{j=1}^k V_{2j}^2 \Big)
              \bigg] \\
        &= \E \bigg[
                  \sum_{i=1}^k V_{1i}^2 V_{2i}^2
                  + \sum_{i=1}^k
                    \sum_{j\neq i} V_{1i}^2 V_{2j}^2
              \bigg] \\
        &= k \cdot \frac{1}{ p \, (p+2) }
           + k \, (k-1) \cdot \frac{ p+1 }{ p \, (p-1) (p+2) } \\
        &= \frac{k}{ p \, (p+2) }
           \left( 1 + (p+1) \frac{k-1}{p-1} \right) \\
        &= \frac{-2}{(p-1)(p+2)}
           \left(\frac{k}{p}\right) \left( 1 - \frac{k}{p} \right)
           + \left( \frac{k}{p} \right)^2,
    \end{align*}
    so that
    \[
        \cov \left[ P_{11}, P_{22} \right]
        = \frac{-2}{ (p-1) (p+2) }
           \left(\frac{k}{p}\right) \left( 1 - \frac{k}{p} \right).
    \]
    Since $\mP$ is symmetric, we have
    \[
        \cov \left[ P_{12}, P_{21} \right]
        = \var \left[ P_{12} \right].
    \]
    The other covariances are all zero.  This is because
    \begin{align*}
        \E \left[ P_{11} P_{12} \right]
        &= \E \bigg[ 
                  \sum_{i,j} V_{1i}^2 \, V_{1j}  V_{2j} 
              \bigg], \\
        \E \left[ P_{12} P_{23} \right]
        &= \E \bigg[ 
                \sum_{i,j} V_{1i} V_{2i} V_{2j} V_{3j}
              \bigg], \\
    \intertext{and}
        \E \left[ P_{12} P_{34} \right]
        &= \E \bigg[ 
                \sum_{i,j} V_{1i} V_{2i} V_{3j} V_{4j}
              \bigg].
    \end{align*}
    Each term in these sums has an element that appears only once in a row.
    Thus, the expectations are all $0$.
\end{proof}


\section{Applications}

We now present two applications of the results in this section.

\subsection{Projections of orthonormal $k$-frames}

Suppose we have $\mU \in \reals^{p \times k}$, an orthonormal $k$-frame,
and we randomly project $\mU$ into $\reals^q$, with $q < p$.  If we denote
the projection matrix by $\mV^\trans$ and set $\mtU =\mV^\trans \mU$, it is 
natural to ask how close $\mtU$ is to being an orthonormal $k$-frame.
We can prove the following

\begin{proposition}\label{P:project-kframe}
    Suppose $\mU \in \reals^{p \times k}$ satisfies $\mU^\trans \mU = \mI_k$.
    Let $\mV \sim \Unif\big( \stiefel_q(\reals^p) \big)$ with 
    $k \leq q \leq p$ and set
    \(
        \mtU = \sqrt{p/q} \mV^\trans \mU.
    \)
    Then there exists a decomposition 
    $\mtU = \mtU_0 + \frac{1}{\sqrt{q}} \mtU_1$ such that
    \[
        \mtU_0^\trans \mtU_0 = \mI_k
    \]
    and
    \[
        \E \| \mtU_1 \|_\Frob^2 
            \leq
                \frac{1}{2} \,
                k \, (k + 1)
                \left( \frac{q}{p} \right)^2
                \left( 1 - \frac{q}{p} \right).
    \]
    In particular, this implies that
    \[
        \E \| \mtU_1 \|_\Frob^2 
            \leq \frac{2}{27} \, k \, (k+1).
    \]
\end{proposition}

\noindent
The main ingredients of the proof are a perturbation lemma and a result about $\mtU^\trans \mtU$, stated below.

\begin{lemma}\label{L:project-kframe-dot-products}
    Suppose $\mU \in \reals^{p \times k}$ satisfies $\mU^\trans \mU = \mI_k$.
    Let $\mV \sim \Unif\big( \stiefel_q(\reals^p) \big)$ with 
    $k \leq q \leq p$
    and set
    \(
        \mtU = \sqrt{p/q} \mV^\trans \mU.
    \)
    Then 
    \[
        \E \left[ \mtU^\trans \mtU \right] = \mI_k
    \]
    and
    \[
        \mE \Big\| 
            \sqrt{q} \, \big(  \mtU^\trans \mtU - \mI_k \big) 
        \Big\|_\Frob^2
                \leq
                k \, (k+1)
                \left( \frac{q}{p} \right)^2                
                \left( 1 - \frac{q}{p} \right),
    \]
    with a matching lower bound of
    \[
        \mE \Big\| 
            \sqrt{q} \, \big(  \mtU^\trans \mtU - \mI_k \big) 
        \Big\|_\Frob^2
            \geq
            k \, (k+1)
            \left( \frac{q}{p} \right)^2
            \left( 1 - \frac{q}{p} \right)
            \left(
            \frac{p}{p+2}
            \right).
    \]
\end{lemma}
\begin{proof}
    Set $\mP = \mV \mV^\trans$ so that
    \(
        \mtU^\trans \mtU
        =
        \frac{p}{q} \, 
        \mU^\trans \mP \mU.
    \)
    Now,
    \[
        \mE \left[ \mtU^\trans \mtU \right]
            =
                \frac{p}{q} \,
                \mU^\trans
                \E \left[ \mP \right]
                \mU
            =
            \mI_k.
    \]
    Also, since $\mO^\trans \mP \mO \eqd \mP$ for any $p \times p$ orthogonal
    matrix $\mO$, we must that $\mU^\trans \mP \mU$ has the same distribution
    as the upper $k \times k$ submatrix of $\mP$.  This implies that
    \begin{align*}
        \mE \Big\| 
            \mtU^\trans \mtU - \mI_k
        \Big\|_\Frob^2
            &= \sum_{i=1}^k \sum_{j=1}^k
                     \var \left[ \mP_{ij} \right] \\
            &= \frac{q}{p} \left( 1 - \frac{q}{p} \right)
               \bigg\{ 
                    k \cdot \frac{2}{p + 2} 
                    +
                    k \, (k-1)
                    \cdot
                    \frac{p}{ (p-1) (p+2) }
               \bigg\} \\
            &= 
               \frac{p \, k \, (k+1) }{ (p-1) (p+2) }
               \left( \frac{q}{p} \right) \left( 1 - \frac{q}{p} \right)
               \left(
                    1
                    -
                    \frac{2}{ p \, (k+1) }
               \right).
    \end{align*}
    The lower and upper bounds follow.
\end{proof}

The next ingredient is a perturbation theorem due to Mirsky, which we take 
from Stewart~\cite{stewart1990pts} and state as a lemma.

\begin{lemma}[Mirsky]\label{L:mirsky}
    If $\mA$ and $\mA + \mE$ are in $\reals^{n \times p}$, then
    \[
        \sum_{i=1}^{n \wedge p} 
            \big(
                \sigma_i(\mA + \mE) - \sigma_i( \mA ) 
            \big)^2 
        \leq \| \mE \|_\Frob^2,
    \]
    where $\sigma_i(\cdot)$ denotes the $i$th singular value.
\end{lemma}

We can now proceed to the rest of the proof.
\begin{proof}[Proof of Proposition~\ref{P:project-kframe}]
    We have that $\mtU^\trans \mtU = \mI_k + \mE$, 
    where $\E \| \sqrt{q} \mE \|_\Frob^2 \leq C$ and $C$ is
    given in Lemma~\ref{L:project-kframe-dot-products}.
    We can apply Lemma~\ref{L:mirsky} to get
    \[
        \sum_{i=1}^k 
            \big(
                \sigma_i(\mtU^\trans \mtU) - 1
            \big)^2 
        \leq \| \mE \|_\Frob^2.
    \]
    Setting $\epsilon_i = \sigma_i(\mtU^\trans \mtU) - 1$, we have 
    $\sigma_i( \mtU ) = \sqrt{1 + \epsilon_i}$.  Note that 
    \(
        \E \left[ q \sum_i \epsilon_i^2 \right]
        \leq \E \| \sqrt{q} \, \mE \|_\Frob^2
        \leq C.
    \)
    Let $\mR$ and $\mS$ be $p \times k$ and $k \times k$ matrices
    with orthonormal columns such that
    \[
        \mtU = \mR (\mI_k + \mDelta) \mS^\trans
    \]
    is the SVD of $\mtU$, where 
    \(
        \mDelta
            = 
                \diag( 
                    \Delta_1, \Delta_2, \ldots, \Delta_k
                ),
    \)
    and $\Delta_i = \sqrt{ 1 + \epsilon_i } - 1.$  Set
    \(
        \mtU_0 = \mR \mS^\trans
    \)
    and
    \(
        \mtU_1 = \sqrt{q} \, \mR \mDelta \mS^\trans.
    \)
    Then,
    \[
        \mtU_0^\trans \mtU_0 
                = \mS \mR^\trans \mR \mS^\trans
                = \mS \mS^\trans
                = \mI_k
    \]
    and 
    \[
        \E \| \mtU_1 \|_\Frob^2 
            = \sum_{i=1}^k \E \left[ q \, \Delta_i^2 \right]
            \leq \frac{1}{2} \sum_{i=1}^k \E \left[ q \, \epsilon_i^2 \right]
            \leq \frac{C}{2},
    \]
    where we have used that
    \(
        \sqrt{1 + \epsilon_i} 
            \geq 
                1 + \frac{1}{2} \epsilon_i - \frac{1}{2} \epsilon_i^2.
    \)
\end{proof}


\subsection{A probabilistic interpretation of the Frobenius norm}

As another application of the results in this section, we give a probabilistic
representation of the Frobenius norm of a matrix.  It is commonly known that 
for any 
$n \times p$ matrix $\mA$, 
\[
    \sup_{ \substack{
                \| \vx \|_2 = 1, \\
                \| \vy \|_2 = 1 } }
        \big( \vx^\trans \mA \, \vy \big)^2
        =
        \| \mA \|_2^2,
\]
where $\| \cdot \|_2$ is the spectral norm, equal largest singular value.  The 
function
\(
    f( \vx, \vy ) = \vx^\trans \mA \, \vy
\)
is a general bilinear form on $\reals^n \times \reals^p$.  The square of the
spectral norm of $\mA$ gives the maximum value of $\big( f(\vx, \vy) \big)^2$
when $\vx$ and $\vy$ are both unit vectors.
It turns out that the Frobenius norm of $\mA$ is related to the
\emph{average} value of $\big( f(\vX, \vY) \big)^2$ when $\vX$ and $\vY$ are
random unit vectors.

\begin{proposition}
    If $\mA \in \reals^{n \times p}$, then
    \[
        \int\limits_{ \substack{
                          \| \vx \|_2 = 1, \\
                          \| \vy \|_2 = 1 } }
            \!\!\!\!\!
            \big(
                \vx^\trans \mA \, \vy
            \big)^2
            \, d\vx \, d\vy
        =
        \frac{1}{np} \| \mA \|_F^2.
    \]
\end{proposition}
\begin{proof}
    There are two steps to the proof.  First, we show that if 
    \(
        \vX \sim \Unif\big( \sphere^{n-1} \big)
    \)
    and $\va$ is arbitrary, then
    \[
        \E \left[ \vX^\trans \va \right]^2 = \frac{1}{n} \| \va \|_2^2.
    \]
    Next, we show that if $\vY \sim \Unif\big( \sphere^{p-1} \big)$, then
    \[
        \E \| \mA \, \vY \|_2^2 = \frac{1}{p} \| \mA \|_\Frob^2.
    \]
    The result follows from these two facts.
    To see the first part, since $\vX$ is orthogonally invariant we have
    \[
        \vX^\trans \va
            \eqd 
                \vX^\trans \left( \| \va \|_2 \, \ve_1 \right)
            = \| \va \|_2 \, \vX_1.
    \]
    To see the second part, let $\mA = \mU \mSigma \mV^\trans$ be the
    SVD of $\mA$ and write
    \[
        \| \mA \, \vY \|_2^2
            =
                \| \mSigma \mV^\trans \vY \|_2^2
            \eqd
                \| \mSigma \vY \|_2^2
            =
                \sum_{i=1}^{n \wedge p} \sigma_i^2( \mA ) \, Y_i^2.
        \qedhere
    \]
\end{proof}
