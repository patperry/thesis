
\documentclass{beamer}

\mode<presentation>
{
  \usetheme{boxes}
  \usecolortheme[RGB={128,0,0}]{structure}
  \usefonttheme{structurebold}

  \setbeamertemplate{frametitle}[default][center]
  \setbeamertemplate{navigation symbols}{} 
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{../perry-thesis-macros}

\title{Cross-Validation for Unsupervised~Learning}

\author[P. O. Perry]{Patrick O. Perry}

\institute[Harvard University] 
{
  Statistics and Information Sciences Laboratory\\
  Harvard University
} 

\date[SISL Group Meeting]
{September 30, 2009 / SISL Group Meeting}

\subject{Talks}


\begin{document}

\begin{frame}
  \titlepage
  \hfill\small{(joint work with Art Owen)}
\end{frame}

\section*{Outline}
\begin{frame}{Outline}
  \tableofcontents
\end{frame}


\section{Introduction}

\subsection{low-rank matrix approximations}
\begin{frame}
\frametitle{Data with large $n$ and $p$}
  \textbf{large number of observations, $n$}

  \textbf{measurements on many variables, $p$} 

  \begin{itemize}
  \item \textit{portfolio selection:} prices of hundreds or thousands of stocks measured every minute 
  \item \textit{genomics:} microarray measurements of thousands or tens-of-thousands of gene expression levels for patients
  \item \textit{text processing:} word frequencies for thousands of words in thousands of documents
  \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Low-rank matrix approximations}
  \centerline{$\mX \approx \mL \mF^\trans$}

  \begin{itemize}
  \item $\mX = \left(\begin{matrix} \vx_1^\trans \\ \vx_2^\trans \\ \vdots \\ \vx_n^\trans \end{matrix} \right) \in \reals^{n \times p}$ is the data matrix
  \item $k \ll \min(n,p)$ is the rank of the approximation
  \item $\mL \in \reals^{n \times k}$ contains factor loadings
  \item $\mF \in \reals^{p \times k}$ contains factors
  \end{itemize}

  \vskip1em
  \textit{Examples:} singular value decomposition, non-negative matrix factorization, $CUR$-decomposition, semi-discrete decomposition, archetypal analysis, \ldots 

  \vskip2em
  \uncover<2->{\centerline{\alert{\Large{How should you pick $k$?}}}}
\end{frame}

\begin{frame}{The singular value decomposition (SVD)}
  \begin{gather*}
    \mX = \sum_{i=1}^{n \wedge p} \sqrt{n} \, \hd_i \vhu_i \vhv_i^\trans = \sqrt{n} \, \mhU \mhD \mhV^\trans \\
    \mhX(k) = \sum_{i=1}^k \sqrt{n} \, \hd_i \vhu_i \vhv_i^\trans = \sqrt{n} \, \mhU \mhD(k) \mhV^\trans
  \end{gather*}

  \begin{itemize}
  \item $\mhU = \left(\begin{matrix} \vhu_1 & \vhu_2 & \cdots & \vhu_{n \wedge p} \end{matrix}\right)$ has orthonormal columns
  \item $\mhV = \left(\begin{matrix} \vhv_1 & \vhv_2 & \cdots & \vhv_{n \wedge p} \end{matrix}\right)$ has orthonormal columns
  \item $\mhD = \diag(\hd_1, \hd_2, \ldots, \hd_{n \wedge p})$ has 
    $\hd_1 \geq \hd_2 \geq \cdots \geq \hd_{n \wedge p} \geq 0$
  \item $\mhD(k) = \diag(\hd_1, \hd_2, \ldots, \hd_k, 0, 0, \ldots, 0)$ 
  \end{itemize}
\end{frame}

\begin{frame}
\frametitle{The SVD minimizes Frobenius norm}
  $\mhX(k) = \sqrt{n} \, \mhU \mhD(k) \mhV^\trans$ is the best rank-$k$ approximation of $\mX$ \\
  {\small \textit{(with respect to Frobenius norm)}}

  \begin{itemize}
  \item $\| \mA \|_\Frob^2 = \sum_{i,j} A_{ij}^2$ is the squared Frobenius norm of $\mA$
  \item $\| \mX - \mhX(k) \|_\Frob^2$ is minimized
  \end{itemize}
\end{frame}

\subsection{the latent factor model}

\begin{frame}
\frametitle{When do low-rank approximations make sense?}
  \begin{block}{Data generated by a low-rank process}
    \begin{itemize}
    \item suppose $\vx_i$ is a weighted combination of $k_0$ latent factors corrupted by additive white noise:
      \[ \vx_i = \sum_{j=1}^{k_0} \va_j s_{i,j} + \vepsilon_i = \mA \vs_i + \vepsilon_i \]
    \item $k_0$ is the number of latent factors
    \item $\mA = \left(\begin{matrix} \va_1 & \va_2 & \cdots & \va_{k_0}\end{matrix}\right) \in \reals^{p \times k_0}$ is the matrix of factors
    \item $\vs_i \sim \Normal(0, \mSigma_\text{S}) \in \reals^{k_0}$ is the vector of loadings 
    \item $\vepsilon_i \sim \Normal(0, \sigma^2 \mI) \in \reals^p$ is the noise
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
\frametitle{Spiked covariance}
  in the previous model, the covariance of $\vx_i$ is ``spiked'':
  \[
    \mSigma \equiv \E\left[ \vx_i \vx_i^\trans \right] = \mA \mSigma_\text{S} \mA^\trans + \sigma^2 \mI
  \]
  reparametrize:
  \[
    \mSigma = \mQ \mLambda \mQ^\trans + \sigma^2 \mI,
  \]
  where $\mQ$ has orthonormal columns and $\mLambda = \diag( \lambda_1, \lambda_2, \ldots, \lambda_k)$ has $\lambda_1 > \lambda_2 > \cdots > \lambda_k$
\end{frame}

\begin{frame}
\frametitle{The latent factor model}
  \begin{block}{spiked covariance}
  \[
    \mX \eqd \mZ \mLambda^{1/2} \mQ^\trans + \mE
  \]
  where $Z_{ij} \sim \Normal(0, 1)$, $E_{ij} \sim \Normal(0, \sigma^2)$
  \end{block}
  \begin{block}{general latent factor model}
  \[
    \mX \eqd \sqrt{n} \, \mU \mD \mV^\trans + \mE
  \]
  where
  \begin{itemize}
  \item $\mU = \left(\begin{matrix} \vu_1 & \vu_2 & \cdots & \vu_{k_0}\end{matrix}\right)$
  and $\mV = \left(\begin{matrix} \vv_1 & \vv_2 & \cdots & \vv_{k_0}\end{matrix}\right)$ have orthonormal columns
  \item $\mD = \diag(d_1, d_2, \ldots, d_k)$ has $d_1 > d_2 > \cdots > d_{k_0}$
  \item $E_{ij} \sim \Normal(0, \sigma^2)$
  \end{itemize}
  \end{block}
\end{frame}

\subsection{a loss criterion}

\begin{frame}
  \frametitle{Model error}
  \[
    \ME(k) \equiv \| \sqrt{n} \mU \mD \mV^\trans - \mhX(k) \|_\Frob^2
  \]
  \begin{itemize}
    \item $\mX = \sqrt{n} \mU \mD \mV^\trans + \mE$ is rank-$k_0$ plus noise
    \item $\mX = \sqrt{n} \, \mhU \mhD \mhV^\trans$ is the SVD of $\mX$
    \item $\mhX(k) = \sqrt{n} \, \mhU \mhD(k) \mhV^\trans$ is the rank-$k$ approximation
    \item $\| \cdot \|_\Frob^2$ is squared Frobenius norm (sum of squared elements)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Prediction error}
  \[
    \PE(k) \equiv \E \| \mX' - \mhX(k) \|_\Frob^2
  \]
  \begin{itemize}
    \item $\mX = \sqrt{n} \mU \mD \mV^\trans + \mE$ is rank-$k_0$ plus noise
    \item $\mX = \sqrt{n} \, \mhU \mhD \mhV^\trans$ is the SVD of $\mX$
    \item $\mhX(k) = \sqrt{n} \, \mhU \mhD(k) \mhV^\trans$ is the rank-$k$ approximation
    \item $\mE' \eqd \mE$ is another realization of the noise
    \item $\mX' = \sqrt{n} \mU \mD \mV^\trans + \mE'$ is another realization of $\mX$
  \end{itemize}
  \vskip1em
  \[
    \PE(k) = \E\left[ \ME(k) \right] + n p \, \sigma^2
  \]
\end{frame}

\section{Behavior of the SVD}
\subsection{sample singular values and vectors}
\subsection{loss behavior}
\subsection{simulations}

\section{Cross-validation}
\subsection{Wold-style holdouts}
\subsection{Gabriel-style holdouts}
\subsection{theory}
\subsection{examples}


\begin{frame}
\end{frame}

\end{document}


