\chapter[Optimal leave-out size for BCV]{Optimal leave-out size for bi-cross-validation}

In this chapter we will determine the optimal leave out-size for bi-cross-validation (BCV), along with proving a weak form of consistency.  We start with a lengthy discussion of what exactly BCV is trying to estimate.  Once we have a well-defined criterion, we show how to choose BCV parameters to get optimal performance.  In our context, ``optimal'' will mean that the rank chosen by BCV agrees with a specified risk.

\subsection{An asymptotic framework}

From a theoretical standpoint, it is more natural to work with a sequence
of data matrices $\mX_1, \mX_2, \ldots, \mX_n$.  
The matrix $\mX_n$ is of size $n \times p$, with $p = p(n)$, $n \to \infty$,
and $\frac{n}{p} = \gamma + \oh\left( \frac{1}{\sqrt{n}} \right)$ for fixed
constant $\gamma \in (0, \infty)$.   We have
\begin{equation}
    \mX_n = \sqrt{n} \mU_n \mD_n \mV_n^\trans + \mE_n,
\end{equation}
where $\mU_n^\trans \mU_n = \mV_n^\trans \mV_n^\trans = \mI_k$ and
$\mD_n = \diag( d_{n,1}, d_{n,2}, \ldots, d_{n,k})$.  We label the columns
of $\mU_n$ and $\mV_n$ as $\vu_{n,1}, \vu_{n,2}, \ldots, \vu_{n,k}$ and
$\vv_{n,1}, \vv_{n,2}, \ldots, \vv_{n,k}$, respectively.


\section{Choosing a loss function}

