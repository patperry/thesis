\chapter[A theoretical analysis of BCV]{A theoretical analysis of bi-cross-validation}
\label{C:optimal-leave-out}

In this chapter we will determine the optimal leave out-size for Gabriel-style
cross-validation of an SVD, also known as bi-cross-validation (BCV), along
with proving a weak form of consistency.  In Chapter~\ref{C:intrinsic-rank},
we rigorously defined the rank estimation problem, and in 
Chapter~\ref{C:cvul} we introduced Gabriel-style cross validation.  Here, we 
provide theoretic justification for Gabriel-style CV.

First, a quick review of the problem.  We are given $\mX$, an $n \times p$ matrix generating by a ``signal-plus-noise'' process, 
\[
    \mX = \sqrt{n} \, \mU \mD \mV^\trans + \mE.
\]
Here, $\mU \in \reals^{n \times k_0}$, $\mD \in \reals^{k_0 \times k_0}$,
$\mV \in \reals^{p \times k_0}$, and $\mE \in \reals^{n \times p}$.  The first
term, $\sqrt{n} \, \mU \mD \mV^\trans$, is the low-rank ``signal'' part.  We call 
$\mU$ and $\mV$ the matrices of left and right factors, respectively.  They 
are normalized so that $\mU^\trans \mU = \mV^\trans \mV = \mI_{k_0}$.  The 
factor ``strengths'' are given in $\mD$, a diagonal matrix of the form
\(
    \mD =
        \diag( d_1, d_2, \ldots, d_{k_0}),
\)
with $d_1 \geq d_2 \geq \cdots \geq d_{k_0} \geq 0$.  Also, typically $k_0$ is 
much smaller than $n$ and $p$.  Lastly, $\mE$ consists of ``noise''.  Although more general types of noise are possible, for simplicity we will assume that $\mE$ is independent of $\mU$, $\mD$, and $\mV$.  We think 
of the signal part as the important part of $\mX$, and the noise part is 
inherently uninteresting.

The rank estimation problem is find the optimal number of terms of the SVD to keep to estimate the signal part.  We let 
$\mX = \sqrt{n} \, \mhU \mhD \mhV^\trans$ be the SVD of $\mX$, where 
$\mhU \in \reals^{n \times n \wedge p}$ and 
$\mhV \in \reals^{p \times n \wedge p}$ have orthonormal columns, and
\(
    \mhD = \diag(
        \hd_1, \hd_2, \ldots, \hd_{n \wedge p}
    )
\)
with $\hd_1 \geq \hd_2 \geq \cdots \geq \hd_{n \wedge p}$.  For $0 \leq k \leq n \wedge p$, we define
\(
    \mhD(k) = \diag(
        \hd_1, \hd_2, \ldots, \hd_k, 0, 0, \ldots, 0
    )
\)
so that $\mhX(k) \equiv \sqrt{n} \, \mhU \mhD(k) \mhV^\trans$ is the SVD of $\mX$ truncated to $k$ terms.  The model error with respect to Frobenius loss is given by
\[
    \ME(k) = \frac{1}{n \, p}
             \| \sqrt{n} \, \mU \mD \mV^\trans - \mhX(k) \|_\Frob^2
\]
The optimal rank is defined with respect to this criterion is
\[
    k^\ast = \argmin_k \ME(k).
\]
The problem we consider is how to estimate $\ME(k)$ or $k^\ast$.

Closely related to model error is the \emph{prediction error}.  For prediction error, we conjure up a noise matrix $\mE'$ with the same distribution as $\mE$ and let $\mX' = \sqrt{n} \mU \mD \mV^\trans + \mE'$.  The prediction error is defined as
\[
    \PE(k) \equiv \frac{1}{n \, p} \E[ \mX' - \mhX(k) ],
\]
which can be expressed as
\[
    \PE(k) = \E[ \ME(k) ] + \frac{1}{n \, p} \E \| \mE \|_\Frob^2.
\]
The minimizer of $\PE$ is the same as the minimizer of $\E[ \ME(k) ]$,
and one can get an estimate of $\ME$ from an estimate of $\PE$ by subtracting an estimate of the noise level.

The previous chapter suggests using Gabriel-style cross-validation for estimating the optimal rank.  Owen \& Perry~\cite{owen2009bi} call this procedure bi-cross-validation (BCV).  For fold $(i,j)$ of BCV, we permute
the rows of $\mX$ with matrices $\mP^{(i)}$ and $\mQ^{(j)}$, then partition the result into four blocks as
\[
    \mP^{(i)\trans} \mX \mQ^{(j)}
        =
        \left(
        \begin{matrix}
            \mX_{11} & \mX_{12} \\
            \mX_{21} & \mX_{22}
        \end{matrix}
        \right).
\]
We take the SVD of the upper-left block and evaluate its predictive 
performance on the lower-right block.  If 
$\mX_{11} = \sqrt{n} \, \mhU_1 \mhD_1 \mhV^\trans$ is the SVD of $\mX_{11}$ and
$\mhX_{11}(k) = \sqrt{n} \, \mhU_1 \mhD_1(k) \mhV^\trans$ is its truncation to 
$k$ terms (with $\mhD_1(k)$ defined analogously to $\mhD(k)$), then the BCV estimate of prediction error from this fold is given 
by
\[
    \widehat{\PE}\big(k ; i, j \big)
        =
            \frac{1}{n_2 \, p_2}
            \| \mX_{22} - \mX_{21} \mhX_{11}(k)^+ \mX_{12} \|_\Frob^2,
\]
Here, $^+$ denotes pseudo-inverse and $\mX_{22}$ has dimensions $n_2 \times p_2$.  For $(K,L)$-fold BCV, the final estimate is the average over all folds:
\[
    \widehat{\PE}(k)
        =
        \frac{1}{K L}
        \sum_{i=1}^K
        \sum_{j=1}^L
            \widehat{\PE}\big(k ; i, j \big).
\]
From $\widehat{\PE}(k)$ we can get an estimate of the optimal rank as
$\hat k = \argmin_k \widehat{\PE}(k)$.

In this chapter, we give a theoretical analysis of $\widehat{\PE}(k)$. This
allows us to determine the bias inherent in $\widehat{\PE}(k)$ and its
consistency properties for estimating $k^\ast$, along with guidance for
choosing the number of folds ($K$ and $L$).
Section~\ref{S:bcv-theory-assumptions} sets out our assumptions and notation.
Section~\ref{S:bcv-theory-results} gives our main results.  Then, Sections~\ref{S:bcv-theory-holdin}~and~\ref{S:bcv-pe-estimate} are devoted to proofs, followed by a discussion in Section~\ref{S:bcv-theory-summary}.


\section{Assumptions and notation}\label{S:bcv-theory-assumptions}

The theory becomes easier if we work in an asymptotic framework.  For that,
we introduce a sequence of data matrices indexed by $n$:
\[
    \mX_{n} = \sqrt{n} \, \mU_n \mD_n \mV_n^\trans + \mE_n.
\]
Here, $\mX_n \in \reals^{n \times p}$ with $p = p(n)$ and 
$\frac{n}{p} \to \gamma \in (0,\infty)$.  Even though the dimensions of $\mX_n$ grow, we assume that the number of factors is fixed at $k_0$.  The first set of assumptions is as follows:

\begin{assumption}\label{A:optimal-leave-out-shapes}
    We have a sequence of random matrices $\mX_n \in \reals^{n \times p}$ 
    with $n \to \infty$ and $p = p(n)$ also going to infinity.  Their ratio
    converges to a fixed constant $\gamma \in (0, \infty)$ as 
    $\frac{n}{p} = \gamma + \oh\left( \frac{1}{\sqrt{n}} \right)$.
\end{assumption}

\begin{assumption}
    The matrix $\mX_n$ is generated as 
    $\mX_n = \sqrt{n} \, \mU_n \mD_n \mV_n^\trans + \mE_n$.  Here,
    $\mU_n \in \reals^{n \times k_0}$, 
    $\mD_n \in \reals^{k_0 \times k_0}$,
    $\mV_n \in \reals^{p \times k_0}$, and $\mE_n \in \reals^{n \times p}$.
    The number of factors, $k_0$, is fixed.
\end{assumption}

\begin{assumption}
    The matrices of left and right factors, $\mU_n$ and $\mV_n$, have 
    orthonormal columns, i.e. 
    $\mU_n^\trans \mU_n = \mV_n^\trans \mV_n = \mI_{k_0}$.
    Their columns are denoted by
    $\vu_{n,1}, \vu_{n,2}, \ldots, \vu_{n,k_0}$ and
    $\vv_{n,1}, \vu_{n,2}, \ldots, \vv_{n,k_0}$, respectively.
\end{assumption}

\begin{assumption}
    The matrix of factor strengths is diagonal:
    \[
        \mD_n = \diag( d_{n,1}, d_{n,2}, \ldots, d_{n,k_0}).  
    \]
    The strengths
    converge as $d_{n,i}^2 \toas \mu_i$ and 
    $d_{n,i}^2 - \mu_i = \OhP\left( \frac{1}{\sqrt{n}} \right)$, strictly
    ordered as
    \(
        \mu_1 > \mu_2 > \cdots > \mu_{k_0} > 0.
    \)
\end{assumption}

\begin{assumption}\label{A:optimal-leave-out-noise}
    The noise matrix $\mE_n$ is independent of $\mU_n$, $\mD_n$, and $\mV_n$.
    Its elements are \iid with 
    \(
        E_{n,11} \sim \Normal( 0, \, \sigma^2 ).
    \)
\end{assumption}

\noindent
These assumptions are standard for latent factor models.

We can apply the work of
Chapter~\ref{C:intrinsic-rank} to get the behavior of the model error.
    We
let $\mX_n = \sqrt{n} \, \mhU_n \mhD_n \mhV_n^\trans$ be the SVD of $\mX_n$ and let $\mhX_n(k) = \sqrt{n} \, \mhU_n \mhD_n(k) \mhV_n^\trans$ be its truncation to $k$ terms.  Then the model error is
\[
    {\ME}_n (k)
        = 
            \frac{1}{n \, p} 
            \big\| 
                \sqrt{n} \, \mU_n \mD_n \mV_n^\trans 
                - 
                \mhX_n(k)
            \big\|_\Frob^2.
\]
With Assumtions~\ref{A:optimal-leave-out-shapes}--\ref{A:optimal-leave-out-noise}, we can apply Proposition~\ref{P:frob-loss-behavior} to get that for fixed
$k$ as $n \to \infty$,
\[
    p \cdot {\ME}_n (k)
        \toas
        \sum_{i=1}^{k}
            \alpha_i \mu_i
        +
        \sum_{i=k+1}^{k_0}
            \mu_i
        +
        \sigma^2
        \left(
            1 + \frac{1}{\sqrt{\gamma}}
        \right)^2
        \cdot
        (k - k_0)_+,
\]
where
\[
    \alpha_i 
    =
    \begin{cases}
        \frac{\sigma^2}{\gamma \mu_i^2}
                \big(
                    3 \sigma^2 + (\gamma+1) \mu_i
                \big)
            &\text{if $\mu_i > \frac{\sigma^2}{\sqrt{\gamma}}$,} \\
        1 
        + 
        \frac{\sigma^2}{\mu_i}
        \left(
            1
            +
            \frac{1}{\sqrt{\gamma}}
        \right)^2
            &\text{otherwise.}
    \end{cases}
\]
Defining $k_n^\ast$ as the minimizer of ${\ME}_n(k)$, we also get that
\[
    k_n^\ast
        \toas
            \max \left\{ i : \mu_i > \mu_\text{crit} \right\},
\]
where
\[
    \mu_\text{crit}
    \equiv
    \sigma^2
    \left(
    \frac{1 + \gamma^{-1}}{2}
    +
    \sqrt{ 
        \left( \frac{1 + \gamma^{-1} }{2} \right)^2
        +
        \frac{3}{\gamma}
    }
    \right),
\]
provided no $\mu_i$ is exactly eqaul to $\mu_\text{crit}$.  We therefore know
how ${\ME}_n(k)$ and its minimizer behave.

To study the bi-cross-validation estimate of prediction error, we need to 
introduce some more assumptions and notation.  As we are only analyzing first-order 
behavior, we can restrict our analysis to the prediction error estimate from a 
single fold.  We let $\mP_n \in \reals^{n \times n}$ and 
$\mQ_n \in \reals^{p \times p}$ be permutation matrices for the fold, partitioned as
\(
    \mP_n 
        = 
        \left( 
        \begin{matrix}
            \mP_{n,1} & \mP_{n,2}
        \end{matrix}
        \right)
\)
and
\(
    \mQ_n
        =
        \left(
        \begin{matrix}
            \mQ_{n,1} & \mQ_{n,2}
        \end{matrix}
        \right),
\)
with $\mP_{n,1} \in \reals^{n \times n_1}$, $\mP_{n,2} \in \reals^{n \times n_2}$, $\mQ_{n,1} \in \reals^{p \times p_1}$, and $\mQ_{n,2} \in \reals^{p \times p_2}$.  Note that $n = n_1 + n_2$ and that $p = p_1 + p_2$.  We define
$\mX_{n,ij} = \mP_{n,i}^\trans \mX \mQ_{n,j}$,
$\mE_{n,ij} = \mP_{n,i}^\trans \mE \mQ_{n,j}$,
$\mU_{n,i} = \mP_{n,i}^\trans \mU_n$, and
$\mV_{n,j} = \mQ_{n,j}^\trans \mV_n$.  Then in block form,
\begin{align*}
    \mP_n^\trans \mX_n \mQ_n
        &=
            \left(
            \begin{matrix}
                \mX_{n,11} & \mX_{n,12} \\
                \mX_{n,21} & \mX_{n,22}
            \end{matrix}
            \right) \\
        &=
            \sqrt{n}
            \left(
            \begin{matrix}
                \mU_{n,1} \mD_n \mV_{n,1}^\trans &
                    \mU_{n,1} \mD_n \mV_{n,2}^\trans \\
                \mU_{n,2} \mD_n \mV_{n,1}^\trans &
                    \mU_{n,2} \mD_n \mV_{n,2}^\trans
            \end{matrix}
            \right)
            +
            \left(
            \begin{matrix}
                \mE_{n,11} & \mE_{n,12} \\
                \mE_{n,21} & \mE_{n,22}
            \end{matrix}
            \right)
\end{align*}
This is the starting point of our analysis.

Now we look at the estimate of prediction error.  We let
$\mX_{n,11} = \sqrt{n} \, \mhU_{n,1} \mhD_{n,1} \mhV_{n,1}^\trans$ be the
SVD of $\mX_{n,11}$.  Here,
\begin{align*}
    \mhU_{n,1}
        &=
            \left(
            \begin{matrix}
                \vhu_{n,1}^{(1)} & 
                \vhu_{n,2}^{(1)} & 
                \cdots & 
                \vhu_{n,n_1 \wedge p_1}^{(1)}
            \end{matrix}
            \right), \\
    \mhV_{n,1}
        &=
            \left(
            \begin{matrix}
                \vhv_{n,1}^{(1)} & 
                \vhv_{n,2}^{(1)} & 
                \cdots & 
                \vhv_{n,n_1 \wedge p_1}^{(1)}
            \end{matrix}
            \right),
\intertext{and}
    \mhD_{n,1}
        &=
            \diag \left(
                \hd_{n,1}^{(1)},
                \hd_{n,2}^{(1)},
                \ldots,
                \hd_{n,n_1 \wedge p_1}^{(1)}
            \right).
\end{align*}
For convenience, we define $\hmu_{n,i}^{(1)} = \big(\hd_{n,i}^{(1)}\big)^2$.
For $0 \leq k \leq n_1 \wedge p_1$, we let
\[
    \mhD_{n,1}(k)
        =
            \diag \left(
                \hd_{n,1}^{(1)},
                \hd_{n,2}^{(1)},
                \ldots,
                \hd_{n,k}^{(1)},                
                0,
                0,
                \ldots,
                0
            \right)
\]
so that 
\(
    \mhX_{n,11} (k)
        \equiv \sqrt{n} \, \mhU_{n,1} \mhD_{n,1}(k) \mhV_{n,1}^\trans
\)
is the SVD of $\mX_{n,11}$ truncated to $k$ terms.  This matrix has pseudo-inverse
\(
    \mhX_{n,11}(k)^+
        =
            \frac{1}{\sqrt{n}} \, 
            \mhV_{n,1} \mhD_{n,1}(k)^+ \mhU_{n,1}^\trans.
\)
Therefore, the BCV rank-$k$ prediction of $\mX_{22}$ is
\begin{align*}
    \mhX_{n,22}(k)
        &=
            \mX_{n,21} \mhX_{n,11}^{+}(k) \mX_{n,12} \\
        &=
            \big(
                \sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,1}^\trans 
                + \mE_{n,21}
            \big)
            \big(
                \mhX_{n,11}(k)^+
            \big)
            \big(
                \sqrt{n} \, \mU_{n,1} \mD_n \mV_{n,2}^\trans 
                + \mE_{n,12}
            \big) \\
    \begin{split}
        &= 
            \sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,1}^\trans
                \mhV_{n,1} \mhD_{n,1}(k)^+ \mhU_{n,1}^\trans
                \mU_{n,1} \mD_n \mV_{n,2}^\trans \\
        &\quad+
            \mU_{n,2} \mD_n \mV_{n,1}^\trans
                \mhV_{n,1} \mhD_{n,1}(k)^+ \mhU_{n,1}^\trans
                \mE_{n,12} \\
        &\quad+
            \mE_{n,21}
                \mhV_{n,1} \mhD_{n,1}(k)^+ \mhU_{n,1}^\trans
                \mU_{n,1} \mD_n \mV_{n,2}^\trans \\
        &\quad+
            \frac{1}{\sqrt{n}} \, \mE_{n,21}
                \mhV_{n,1} \mhD_{n,1}(k)^+ \mhU_{n,1}^\trans
                \mE_{n,12}
    \end{split} \\
    \begin{split}
        &=
            \sqrt{n} \, 
                \mU_{n,2} \mD_n 
                \mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}^\trans
                \mD_n \mV_{n,2}^\trans \\
        &\quad+
            \mU_{n,2} \mD_n \mTheta_{n,1} \mhD_{n,1}(k)^+ \mtE_{n,12} \\
        &\quad+
            \mtE_{n,21} 
                \mhD_{n,1}(k)^+ \mPhi_{n,1}^\trans \mD_n \mV_{n,2}^\trans \\
        &\quad+
            \frac{1}{\sqrt{n}} \, \mtE_{n,21} \mhD_{n,1}(k)^+ \mtE_{n,12},
    \end{split}
\end{align*}
where 
$\mTheta_{n,1} = \mV_{n,1}^\trans \mhV_{n,1}$,
$\mPhi_{n,1}   = \mU_{n,1}^\trans \mhU_{n,1}$,
$\mtE_{n,12} = \mhU_{n,1}^\trans \mE_{n,12}$, and
$\mtE_{n,21} = \mE_{n,21} \mhV_{n,1}$.  Note that $\mtE_{n,12}$ and $\mtE_{n,21}$ have \iid $\Normal( 0 ,\, \sigma^2 )$ entries, also independent of the other terms that make up $\mhX_{n,22}(k)$ and $\mX_{n,22}$.  By conditioning on $\mtE_{n,12}$ and $\mtE_{n,21}$, we can see that $\mhX_{22}(k)$ is in general a biased estimate of $\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans$.

To analyze $\mhX_{22}(k)$, we need to impose some additional assumptions.
The first assumption is fairly banal and involves the leave-out sizes.

\begin{assumption}
    There exist fixed $K, L \in (0, \infty)$ (not necessarily integers),
    such that
    $\frac{n}{n_2} = K + \oh\left(\frac{1}{\sqrt{n}}\right)$ and 
	$\frac{p}{p_2} = L + \oh\left(\frac{1}{\sqrt{n}}\right)$.
\end{assumption}

\noindent
The next assumption is not quite so innocent and involves the distribution of the factors.

\begin{assumption}\label{A:held-in-orthog}
    The departure from orthogonality for the held-in factors $\mU_{n,1}$
    and $\mV_{n,1}$ is of order $\frac{1}{\sqrt{n}}$.  Specifically,
    \begin{gather*}
        \sup_n 
        \E \, \Big\| 
            \sqrt{n_1} 
            \Big( 
                \mU_{n,1}^\trans \mU_{n,1} 
                -
                \frac{n_1}{n} \mI_{k_0}
            \Big)
        \Big\|_\Frob^2
        <
        \infty, \quad\text{and} \\
        \sup_n
        \E \, \Big\| 
            \sqrt{p_1} 
            \Big( 
                \mV_{n,1}^\trans \mV_{n,1} 
                -
                \frac{p_1}{p} \mI_{k_0}
            \Big)
        \Big\|_\Frob^2
        <
        \infty.
    \end{gather*}
\end{assumption}

\noindent
This assumption is there so that we can apply the theory in Chapter~\ref{C:svd-behavior} to get at the behavior of the SVD of $\mX_{n,11}$.  It is satisfied, for example, if we are performing rotated cross-validation (see subsection~\ref{SS:rotated-cv}) or if the factors are generated by certain stationary processes.  It is likely that the theory presented below holds under a weaker condition, but a detailed analysis is beyond the scope of this chapter.


\section{Main results}\label{S:bcv-theory-results}

The BCV estimate of prediction error from a single replicate is given
by
\[
	\widehat{\PE}_n(k)
		=
			\frac{1}{n_2 \, p_2}
			\| \mX_{n,22} - \mhX_{n,22}(k) \|_\Frob^2.
\]
It turns out that
\(
	\E\left[
		\widehat{\PE}_n(k)
	\right]
\)
is dominated by the irreducible error.  However, if we have a $\sqrt{n p}$-consistent estimate of $\sigma^2$, then we can get an expression for
the scaled limit of the estimated model error.  This expression is the main result of the paper.

\begin{theorem}\label{T:bcv-expected-me}
	Suppose that $\hat \sigma_n^2$ is a sequence of 
	$\sqrt{n p}$-consistent estimators of $\sigma^2$ statisfying
	\(
		\E\left[
			\sqrt{n p}
				\left(
				\hat \sigma_n^2
				-
				\sigma^2
			\right)
		\right]
		\to
		0.
	\)
	Define the BCV estimate of model error from a single replicate as
	\[
		\widehat{\ME}_n(k)
			=
				\widehat{\PE}_n(k)
				-
				\hat \sigma_n^2.
	\]
	Then, for fixed $k$ as $n\to\infty$,
	\[
		\E \left[ p \cdot \widehat{\ME}_n(k) \right]
			\to
				\sum_{i=1}^{k \wedge k_0}
					\beta_i \, \mu_i
				+
				\sum_{i=k+1}^{k_0}
					\mu_i
				+
				\eta
				\cdot
				(k - k_0)_+,
	\]
	where
	\begin{align*}
		\beta_i
			&=
				\begin{cases}
					\frac{
			            \frac{\sigma^2}{\gamma \mu_{i}^2}
			            \left(
			                3 \sigma^2
			                +
							\left(
								\gamma
								\frac{K-1}{K}
								+
								\frac{L-1}{L}
							\right)
			                \mu_{i}
			            \right)
			        }
			        {
			            \rho
			            +
						\left(
							\gamma
							\frac{K-1}{K}
							+
							\frac{L-1}{L}
						\right)
			            \frac{\sigma^2}{\gamma \mu_{i}}
			            +
			            \frac{\sigma^4}{\gamma \mu_{i}^2}
			        }&\text{when $\mu_{i} > 
								  \frac{\sigma^2}{\sqrt{\rho \, \gamma}}$,} \\
					1 + \frac{\eta}{\mu_i}&\text{otherwise,}
				\end{cases}
	\end{align*}
	and
	\begin{gather*}
		\rho
			=
				\frac{K-1}{K}
				\cdot
				\frac{L-1}{L}, \\
		\eta
			=
				\frac{\sigma^2}
				     {\left(
					      \sqrt{\gamma}
						  +
						  \sqrt{\frac{K}{K-1}} 
						  \cdot 
						  \sqrt{\frac{L-1}{L}}
					  \right)^{2}
					 }.
	\end{gather*}
\end{theorem}

It is interesting to compare $\beta_i$ with the expression for $\alpha_i$
in the limit of $p \cdot \ME_n(k)$, the true model error.  Although the BCV 
estimator of model error is biased, this bias is small for large 
$\mu_i$, $K$, and $L$.

A corollary gives the behavior of the minimizer of the expected model 
error estimate. We let $\hat k_n$ be the rank that minimizes 
$\E\left[ \widehat{\ME}_n(k) \right]$.  The next two results follow from
Theorem~\ref{T:bcv-expected-me}.

\begin{corollary}\label{C:bcv-expected-me-cutoff}
	As $n \to \infty$,
	\[
		\hat k_n
			\to
				\max\left\{
					i :
						\mu_i 
						> 
						\frac{\sigma^2}{\sqrt{\gamma}}
						\cdot
						\sqrt{\frac{2}{\rho}}
				\right\},
	\]
	(provided no $\mu_i$ is exactly equal to
	\(
		\frac{\sigma^2}{\sqrt{\gamma}}
		\cdot
		\sqrt{\frac{2}{\rho}},
	\)
	in which case the limit is ambiguous).
\end{corollary}

We can use Corollary~\ref{C:bcv-expected-me-cutoff} to guide our choice
of $K$ and $L$.  If we choose them carefully, then $\hat k_n$ and
$k^\ast_n$ will converge to the same value.

\begin{corollary}
	If
	\[
		\sqrt{\rho}
			=
				\frac{\sqrt{2}}
					 {\sqrt{\bar \gamma} + \sqrt{\bar \gamma + 3}},
	\]
	where
	\[
		\bar \gamma
			=
				\left(
					\frac{\gamma^{1/2} + \gamma^{-1/2}}{2}
				\right)^2,
	\]
	then $\hat k_n$ and $k^\ast_n$ converge to the same value
	(provided no $\mu_i$ is exactly equal to
		\(
			\frac{\sigma^2}{\sqrt{\gamma}}
			\cdot
			\sqrt{\frac{2}{\rho}}).
		\)
\end{corollary}

Interestingly, the first-order-optimal choices of $K$ and $L$ do not
depend on the aspect ratio of the original matrix.  All that matters
is the ratio of the number of elements in $\mX_{n,11}$ to the number
of elements in $\mX_{n}$.  

For a square matrix, $\gamma = \bar \gamma = 1$, and the optimal 
$\rho$ is $\frac{2}{9}$.  If we choose $K = L$, then this requires
\[
	\frac{K-1}{K}
		=
			\frac{\sqrt{2}}{3},
\]
so that
\[
	K
		=
			\left(
				1
				-
				\frac{\sqrt{2}}{3}
			\right)^{-1}
	\approx
			1.89.
\]
For general aspect ratios ($\gamma \neq 1$), this requires
\begin{align*}
	K
		&=
			\frac{3}{3
					 -
					 2
					 \left(
						 \sqrt{\bar \gamma + 3}
						 -
					     \sqrt{\bar \gamma}
					 \right)}.
\end{align*}
For very large or very small aspect ratios ($\gamma \to 0$ or $\gamma \to \infty$), $K \to 1$.  In these situations one should leave out almost all of the matrix when performing
bi-cross-validation.

The remainder of the chapter is devoted to proving Theorem~\ref{T:bcv-expected-me}.


\section{The SVD of the held-in block}\label{S:bcv-theory-holdin}

The first step in analyzing the BCV estimate of prediction error is to see how the SVD of $\mX_{n,11}$ behaves.  With Assumptions~\ref{A:optimal-leave-out-shapes}--\ref{A:held-in-orthog}, we can start this analysis.  Our strategy is to use Assumption~\ref{A:held-in-orthog} to apply a matrix perturbation argument in combination with Theorems~\ref{T:spiked-eigenvalue-limits} and~\ref{T:spiked-eigenvector-limits}.

We show that we can apply Theorem~\ref{T:spiked-eigenvector-limits} to 
$\sqrt{n} \, \mU_{n,1} \mD_n \mV_{n,1}^\trans + \mE_{n,11}$ even though $\mU_{n,1}$ and $\mV_{n,1}$ do not have orthogonal columns.  First we set
\[
	\gamma_1
		= 
		\frac{K-1}{K}
		\cdot
		\frac{L}{L-1}
		\cdot
		\gamma
\]
and note that 
\(
	\frac{n_1}{p_1} 
		= 
		\frac{n_1}{n} \cdot \frac{p}{p_1} \cdot \frac{n}{p}
		=
		\gamma_1 + \oh\left( \frac{1}{\sqrt{n_1}} \right).
\)
Next, for $1 \leq i \leq k_0$, we define
\begin{gather*}
	\mu_{1,i}
		= 
			\frac{L-1}{L} \cdot \mu_{i}, \\
	\bmu_{1,i}
		=
		\begin{cases}
			\frac{K-1}{K}
			\cdot
            \left( \mu_{1,i} + \sigma^2 \right)
            \left( 1 + \frac{\sigma^2}{\gamma_1 \mu_{1,i}} \right)
                &\text{when $\mu_{1,i} 
							 > 
							 \frac{\sigma^2}{\sqrt{\gamma_1}}$}, \\
			\frac{K-1}{K}
			\cdot
            \sigma^2 \left( 1 + \frac{1}{\sqrt{\gamma_1}} \right)^2
                &\text{otherwise.}
        \end{cases}		
			\\
	\theta_{1,i}
		=
	        \begin{cases}
				\sqrt{\frac{L-1}{L}}
				\cdot
				\sqrt{
		            \left( 
						1 
						- 
						\frac{\sigma^4}{ \gamma_1 \mu_{1,i}^2}
					\right)
		            \left(
						1 
						+ 
						\frac{\sigma^2}{ \gamma_1 \mu_{1,i} } 
					\right)^{-1}
				}
	            &\text{when $\mu_{1,i} 
							 > 
							 \frac{\sigma^2}{\sqrt{\gamma_1}}$,} \\
	            0
	            &\text{otherwise,}
	        \end{cases} \\
    \varphi_{1,i}
        =
            \begin{cases}
				\sqrt{\frac{K-1}{K}}
				\cdot
				\sqrt{
	                \left( 
						1 
						- 
						\frac{\sigma^4}{ \gamma_1 \mu_{1,i}^2} 
					\right)
	                \left(
						1 
						+ 
						\frac{\sigma^2}{ \mu_{1,i} } 
					\right)^{-1}
				}
                &\text{when $\mu_{1,i} 
							 > 
							\frac{\sigma^2}{\sqrt{\gamma_1}}$,} \\
                0
                &\text{otherwise.}
            \end{cases}
\end{gather*}
For $i > k_0$, we put 
\(
    \bmu_{1,i} 
        =
            \frac{K-1}{K} 
            \cdot
            \sigma^2
            \left(
                1
                +
                \frac{1}{\sqrt{\gamma_1}}
            \right)^2.
\)
Now, for $k \geq 1$ we let $\mTheta_{1}(k)$ and $\mPhi_1(k)$ be $k_0 \times k$ matrices with entries
\[
    \theta_{1,ij}^{(k)}
        =
            \begin{cases}
                \theta_{1,i}&\text{if $i = j$,} \\
                0&\text{otherwise,}
            \end{cases}
    \qquad
    \text{and}
    \qquad
    \varphi_{1,ij}^{(k)}
        =
            \begin{cases}
                \varphi_{1,i}&\text{if $i = j$,} \\
                0&\text{otherwise,}
            \end{cases}
\]
respectively.  In this section, we prove the following:

\begin{proposition}\label{P:holdin-svd}
    For fixed $k$ as $n\to\infty$, the first $k$ columns of $\mTheta_{1,n}$
    and $\mPhi_{1,n}$ converge in probability to $\mTheta_{1}(k)$ and 
    $\mPhi_{1}(k)$, respectively.  Likewise, for $1 \leq i \leq k$,
    $\hd_{n,i}^{(1)} \toP \bmu_{1,i}^{1/2}$.
\end{proposition}

We prove the proposition by leveraging the work of 
Chapter~\ref{C:svd-behavior}.  We have that 
\(
    \mX_{n,11} = \sqrt{n} \, \mU_{n,1} \mD_n \mV_{n,1}^\trans + \mE_{n,11}.
\)
The first term does not satisfy the conditions of Theorems~\ref{T:spiked-eigenvalue-limits}~and~\ref{T:spiked-eigenvector-limits} since $\mU_{n,1}$ and $\mV_{n,1}$ do not have orthonormal columns.  Moreover, the scaling is $\sqrt{n}$ instead of $\sqrt{n_1}$.  We introduce scaling constants and group the terms as
\[
    \mX_{n,11}
        =
            \sqrt{n_1}
            \Big(
                \sqrt{\frac{n}{n_1}} \mU_{n,1}
            \Big)
            \Big(
                \sqrt{\frac{p_1}{p}} \mD_n
            \Big)
            \Big(
                \sqrt{\frac{p}{p_1}}
                \mV_{n,1}
            \Big)^\trans
            +
            \mE_{n,11}.
\]
With this scaling,
\[
    \mE \bigg[
        \Big( \sqrt{\frac{n}{n_1}} \mU_{n,1} \Big)^\trans
        \Big( \sqrt{\frac{n}{n_1}} \mU_{n,1} \Big)
    \bigg]
    =
    \mE \bigg[
        \Big( \sqrt{\frac{p}{p_1}} \mV_{n,1} \Big)^\trans
        \Big( \sqrt{\frac{p}{p_1}} \mV_{n,1} \Big)
    \bigg]
    =
    \mI_{k_0},
\]
and the diagonal elements of
\(
    \Big(
          \sqrt{\frac{p_1}{p}} \mD_n
    \Big)
\)
converge to 
\(
    \mu_{1,1}^{1/2},
    \mu_{1,2}^{1/2},
    \ldots,
    \mu_{1,k_0}^{1/2}.
\)
So, Proposition~\ref{P:holdin-svd} should at least be plausible.  

We prove the result by showing that
\(
    \Big(
        \sqrt{\frac{n}{n_1}} \mU_{n,1}
    \Big)
    \Big(
        \sqrt{\frac{p_1}{p}} \mD_n
    \Big)
    \Big(
        \sqrt{\frac{p}{p_1}}
        \mV_{n,1}
    \Big)^\trans
\)
is \emph{almost} an SVD.  We denote its $k_0$-term SVD by 
$\mtU_{n,1} \mtD_{n,1} \mtV_{n,1}^\trans$, and demonstrate the following:
\begin{lemma}\label{L:svd-random-perturb}
    Three properties hold:
    \begin{enumerate}[(1)]
        \item For $1 \leq i \leq k_0$, 
            \(
                |\frac{p_1}{p} d_{n,i}^2 - \tilde d_{n,i}^2| 
                    = 
                        \OhP\left(\frac{1}{\sqrt{n}}\right).
            \)
        \item For any sequence of vectors $\vx_1, \vx_2, \ldots, \vx_n$
            with $\vx_n \in \reals^n$,
            \[
                \left\|
                    \Big(
                        \sqrt{\frac{n}{n_1}} \mU_{n,1}
                    \Big)^\trans
                    \vx_n
                    -
                    \mtU_{n,1}^\trans \,
                    \vx_n
                \right\|_2
                =
                \OhP\left(\frac{\| \vx_n \|_2}{\sqrt{n}}\right).
            \]
        \item For any sequence of vectors $\vy_1, \vy_2, \ldots, \vy_n$
            with $\vy_n \in \reals^p$,
            \[
                \left\|
                    \Big(
                        \sqrt{\frac{p}{p_1}} \mV_{n,1}
                    \Big)^\trans
                    \vy_n
                    -
                    \mtV_{n,1}^\trans \,
                    \vy_n
                \right\|_2
                =
                \OhP\left(\frac{\| \vy_n \|_2}{\sqrt{n}}\right).
            \]
    \end{enumerate}
    Above, $\tilde d_{n,i}$ is the $i$th diagonal entry of $\mtD_{n}$,
    which are assumed to be sorted in descending order.
\end{lemma}
\noindent
Proposition~\ref{P:holdin-svd} is then a direct consequence of
Lemma~\ref{L:svd-random-perturb} and
Theorems~\ref{T:spiked-eigenvalue-limits}~and~\ref{T:spiked-eigenvector-limits}.

\begin{proof}[Proof of Lemma~\ref{L:svd-random-perturb}]
    Let $\sqrt{\frac{n}{n_1}} \mU_{n,1} = \mbU_{n,1} \mR_n$ be a 
    $QR$-decomposition so that $\mbU_{n,1} \in \reals^{n_1 \times k_0}$ has
    orthonormal columns and $\mR_n$ is an upper-triangular matrix.  Define
    $\mR_{n,1} = \sqrt{n} ( \mR_{n,1} - \mI_{k_0})$ so that
    $\mR_n = \mI_{k_0} + \frac{1}{\sqrt{n}} \mR_{n,1}$.  We can write
    \[
        \frac{n}{n_1}
        \mU_{n,1}^\trans \mU_{n,1}
            =
                \mI_{k_0}
                +
                \frac{1}{\sqrt{n}}
                \left(
                    \mR_{n,1}
                    +
                    \mR_{n,1}^\trans
                \right)
                +
                \frac{1}{n}
                \mR_{n,1}^\trans \mR_{n,1}.
    \]
    By Assumption~\ref{A:held-in-orthog},
    \(
        \frac{n}{n_1} \mU_{n,1}^\trans \mU_{n,1}
        -
        I_{k_0}
        =
        \OhP\left( \frac{1}{\sqrt{n}} \right).
    \)
    Therefore, $\mR_{n,1} = \OhP\left( 1 \right)$.
    
    The same argument applies to show that there exits a
    $\mbV_{n,1} \in \reals^{p_1 \times k_0}$ with orthonormal columns
    such that
    \[
        \sqrt{\frac{p}{p_1}} \mV_{n,1} 
            =
            \mtV_{n,1}
            \left(
                \mI_{k_0}
                +
                \frac{1}{\sqrt{n}}
                \mS_{n,1}
            \right),
    \]
    and $\mS_{n,1}$ is upper-triangular with $\mS_{n,1} = \OhP(1)$.
    
    We now look at
    \begin{multline*}
        \Big(
            \mI_{k_0}
            +
            \frac{1}{\sqrt{n}}
            \mR_{n,1}
        \Big)
        \Big(
            \mD_n
        \Big)
        \Big(
            \mI_{k_0}
            +
            \frac{1}{\sqrt{n}}
            \mS_{n,1}
        \Big)^\trans \\
            =
                \mD_n
                +
                \frac{1}{\sqrt{n}}
                \left(
                    \mR_{n,1} \mD_n
                    +
                    \mD_n \mS_{n,1}^\trans
                \right)
                +
                \frac{1}{n}
                \mR_{n,1} \mD_n \mS_{n,1}^\trans.
    \end{multline*}
    Since the diagonal elements of $\mD_n$ are distinct, we can apply
    Lemma~\ref{L:eigen-perturb} twice to get that there exist
    $k_0 \times k_0$ matrices $\mbR_{n,1}$, $\mbS_{n,1}$ and $\mDelta_n$
    of size $\OhP(1)$ such that
    \begin{multline*}
        \Big(
            \mI_{k_0}
            +
            \frac{1}{\sqrt{n}}
            \mR_{n,1}
        \Big)
        \Big(
            \mD_n
        \Big)
        \Big(
            \mI_{k_0}
            +
            \frac{1}{\sqrt{n}}
            \mS_{n,1}
        \Big)^\trans \\
            =
                \Big(
                    \mI_{k_0}
                    +
                    \frac{1}{\sqrt{n}}
                    \mbR_{n,1}
                \Big)
                \Big(
                    \mD_n
                    +
                    \frac{1}{\sqrt{n}}
                    \mDelta_n
                \Big)
                \Big(
                    \mI_{k_0}
                    +
                    \frac{1}{\sqrt{n}}
                    \mbS_{n,1}
                \Big)^\trans,
    \end{multline*}
    with 
    \(
        \mI_{k_0}
        +
        \frac{1}{\sqrt{n}}
        \mbR_{n,1}
    \)
    and
    \(
        \mI_{k_0}
        +
        \frac{1}{\sqrt{n}}
        \mbS_{n,1}
    \)
    both orthogonal matrices, and $\mDelta_n$ diagonal.
    
    We define
    \(
        \mtU_{n,1} 
            = 
            \mbU_{n,1}
            \Big(
                \mI_{k_0}
                +
                \frac{1}{\sqrt{n}}
                \mbR_{n,1}
            \Big),
    \)
    \(
        \mtV_{n,1} 
            = 
            \mbV_{n,1}
            \Big(
                \mI_{k_0}
                +
                \frac{1}{\sqrt{n}}
                \mbS_{n,1}
            \Big),
    \)
    and
    \(
        \mtD_n
            =
            \sqrt{\frac{p_1}{p}}
            \Big(
                \mD_n
                +
                \frac{1}{\sqrt{n}}
                \mDelta
            \Big).
    \)
    Now, as promised, $\mtU_{n,1} \mtD_n \mtV_{n,1}^\trans$ is the
    SVD of
    \[
        \Big(
            \sqrt{\frac{n}{n_1}} \mU_{n,1}
        \Big)
        \Big(
            \sqrt{\frac{p_1}{p}} \mD_n
        \Big)
        \Big(
            \sqrt{\frac{p}{p_1}}
            \mV_{n,1}
        \Big)^\trans.
    \]
    We can see immediately the property (1) holds.  For property (2), note
    that
    \begin{align*}
        \mtU_{n,1}
            &=
                \mbU_{n,1}
                \Big(
                    \mI_{k_0}
                    +
                    \frac{1}{\sqrt{n}}
                    \mbR_{n,1}
                \Big) \\
            &= 
                \sqrt{\frac{n}{n_1}}                
                \mU_{n,1}
                \Big(
                    \mI_{k_0}
                    +
                    \frac{1}{\sqrt{n}}
                    \mR_{n,1}
                \Big)^{-1}
                \Big(
                    \mI_{k_0}
                    +
                    \frac{1}{\sqrt{n}}
                    \mbR_{n,1}
                \Big) \\
            &= 
                \sqrt{\frac{n}{n_1}}
                \mU_{n,1}
                \Big(
                    \mI_{k_0}
                    +
                    \frac{1}{\sqrt{n}}
                    \mtR_{n,1}
                \Big)
    \end{align*}
    for some $\mtR_{n,1} = \OhP(1)$.  Therefore,
    \[
        \mtU_{n,1}^\trans \vx_n - \mU_{n,1}^\trans \vx_n
            =
                \frac{1}{\sqrt{n}}
                \mtR_{n,1}^\trans \mU_{n,1}^\trans \vx_n
    \]
    so that
    \begin{align*}
        \| \mtU_{n,1}^\trans \vx_n - \mU_{n,1}^\trans \vx_n \|_2
            &\leq 
                \frac{1}{\sqrt{n}}
                \| \mtR_{n,1} \|_\Frob
                \cdot
                \| \mU_{n,1} \|_\Frob
                \cdot
                \| \vx_n \|_2 \\
            &=
                \OhP \left( \frac{\| \vx_n \|_2 }{\sqrt{n}} \right).
    \end{align*}
    A similar argument applies to show that property (3) holds.
\end{proof}

\section{The prediction error estimate}\label{S:bcv-pe-estimate}

In this section, we study the estimate of prediction error
\[
	\widehat{\PE}_n(k)
		=
			\frac{1}{n_2 \, p_2}
			\| \mX_{n,22} - \mhX_{n,22}(k) \|_\Frob^2
\]
We can expand this as
\begin{align*}
	\widehat{\PE}_n(k)
		&=
			\frac{1}{n_2 \, p_2}
			\tr \Big(
				\big(
					\mX_{n,22} - \mhX_{n,22}(k)
				\big)
				\big(
					\mX_{n,22} - \mhX_{n,22}(k)
				\big)^\trans
			\Big) \\
	\begin{split}
		&=
			\frac{1}{n_2 \, p_2}
			\tr \Big(
				\big(
					\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans 
					+
					\mE_{n,22}
					- 
					\mhX_{n,22}(k)
				\big) \\
				&\qquad\qquad\qquad\cdot
				\big(
					\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans 
					+
					\mE_{n,22}
					- 
					\mhX_{n,22}(k)
				\big)^\trans
			\Big)
	\end{split} \\
	\begin{split}
		&=
			\frac{1}{n_2 \, p_2}
			\bigg(
				\big\| 
					\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans
					-
					\mhX_{n,22}(k)
				\big\|_\Frob^2 \\
				&\qquad\qquad\quad+
				2 \, 
				\tr \Big(
					\mE_{n,22}
					\big(
						\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans 
						- 
						\mhX_{n,22}(k)
					\big)^\trans
				\Big) \\
				&\qquad\qquad\quad+
				\|
					\mE_{n,22}
				\|_\Frob^2
			\bigg).
	\end{split}
\end{align*}
It has expectation
\[
	\E \left[ \widehat{\PE}_n(k) \right]
		=
			\E \left[
				\frac{1}{n_2 \, p_2}
				\big\| 
					\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans
					-
					\mhX_{n,22}(k)
				\big\|
			\right]
			+
			\sigma^2.
\]
The first term is the expected model approximation error and the second term is the irreducible error.

The expected model approximation error expands into four terms.  We have
\begin{multline*}
	\E \big\|
		\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans
		-
		\mhX_{n,22}(k)
	\big\|_\Frob^2 \\
		=
			\E\Big[ 
				\tr \Big(
					\big(
						\sqrt{n} \,
						\mU_{n,2} 
						( 
							\mD_n 
							- 
							\mD_n 
							\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
							\mD_n
						)
						\mV_{n,2}^\trans \\
						-
						\mU_{n,2} 
						\mD_n \mTheta_{n,1} \mhD_{n,1}(k)^+ 
						\mtE_{n,12} \\
						- 
						\mtE_{n,21}
						\mhD_{n,1}(k)^+ \mPhi_{n,1}^\trans \mD_n 
						\mV_{n,2}^\trans \\
						-
						\frac{1}{\sqrt{n}}
						\mtE_{n,21}
						\mhD_{n,1}(k)^+
						\mtE_{n,12}
					\big) \\
					\cdot
					\big(
						\sqrt{n} \,
						\mU_{n,2} 
						( 
							\mD_n 
							- 
							\mD_n 
							\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
							\mD_n
						)
						\mV_{n,2}^\trans \\
						-
						\mU_{n,2} 
						\mD_n \mTheta_{n,1} \mhD_{n,1}(k)^+ 
						\mtE_{n,12} \\
						- 
						\mtE_{n,21}
						\mhD_{n,1}(k)^+ \mPhi_{n,1}^\trans \mD_n 
						\mV_{n,2}^\trans \\
						-
						\frac{1}{\sqrt{n}}
						\mtE_{n,21}
						\mhD_{n,1}(k)^+
						\mtE_{n,12}
					\big)^\trans
				\Big)
			\Big].
\end{multline*}
By first conditioning on everything but $\mtE_{n,12}$ and $\mtE_{n,21}$, the
cross-terms cancel and we get
\begin{multline*}
	\E \big\|
		\sqrt{n} \, \mU_{n,2} \mD_n \mV_{n,2}^\trans
		-
		\mhX_{n,22}(k)
	\big\|_\Frob^2 \\
		=
			\E\big\|
				\sqrt{n} \,
				\mU_{n,2} 
				( 
					\mD_n 
					- 
					\mD_n 
					\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
					\mD_n
				)
				\mV_{n,2}^\trans 
			\big\|_\Frob^2 \\
			+
			\E\big\|
				\mU_{n,2} 
				\mD_n \mTheta_{n,1} \mhD_{n,1}(k)^+ 
				\mtE_{n,12}
			\big\|_\Frob^2 \\
			+
			\E\big\|
				\mtE_{n,21}
				\mhD_{n,1}(k)^+ \mPhi_{n,1}^\trans \mD_n 
				\mV_{n,2}^\trans
			\big\|_\Frob^2 \\
			+
			\E\big\|
				\frac{1}{\sqrt{n}}
				\mtE_{n,21}
				\mhD_{n,1}(k)^+
				\mtE_{n,12}
			\big\|_\Frob^2.
\end{multline*}
Since $\mtE_{n,12}$ and $\mtE_{n,21}$ are made of \iid $\Normal( 0, \, \sigma^2)$ random variables, the last three terms are fairly easy to analyze.
We can use the following lemma:

\begin{lemma}\label{L:frob-random-product}
	Let $\mZ \in \reals^{m \times n}$ be a random matrix with uncorrelated
	elements, all having having mean $0$ and variance $1$ (but not necessarily 
	from the same distribution).  If $\mA \in \reals^{n \times p}$ is 
	independent of $\mZ$, then
	\[
		\E \left\|
			\mZ \mA
		\right\|_\Frob^2
			=
				m
				\cdot
				\mE \left\| \mA \right\|_\Frob^2.
	\]
\end{lemma}
\begin{proof}
	The square of the $ij$ element of the product is given by
	\begin{align*}
		\left( \mZ \mA \right)_{ij}^2
			&=
				\Big(
					\sum_{\alpha=1}^n
						Z_{i \alpha} A_{\alpha j}
				\Big)^2 \\
			&=
				\sum_{\alpha=1}^n
					Z_{i \alpha}^2 A_{\alpha j}^2
				+
				\sum_{\alpha \neq \beta}
					Z_{i \alpha} A_{\alpha j}
					Z_{i \beta}  A_{\beta j}
	\end{align*}
	This has expectation
	\[
		\E \left[
			\left( \mZ \mA \right)_{ij}^2
		\right]
			=
				\sum_{\alpha=1}^n
					\E\left[ A_{\alpha j}^2 \right],
	\]
	so that
	\begin{align*}
		\E \left\|
			\mZ \mA
		\right\|_\Frob^2
			&=
				\sum_{i=1}^m
				\sum_{j=1}^p
					\E \left[
						\left( \mZ \mA \right)_{ij}^2
					\right] \\
			&=
				\sum_{i=1}^m
				\sum_{j=1}^p
					\sum_{\alpha=1}^n
						\E\left[ A_{\alpha j}^2 \right] \\
			&= m \cdot \E \left\| \mA \right\|_\Frob^2. \qedhere
	\end{align*}
\end{proof}

We also need a technical result to ensure that after appropriate scaling, the
expectations are finite.

\begin{lemma}\label{L:d1-bounded-away-from-0}
	For fixed $k$ as $n \to \infty$, 
	$\hd_{n,k}^{(1)} $ is almost surely bounded away from zero.
\end{lemma}
\begin{proof}
	We have that $\hd_{n,k}^{(1)}$ is the $k$th singular value of
	\(
		\frac{1}{\sqrt{n}} \mX_{n,11}
			=
				\mU_{n,1} \mD_n \mV_{n,1}^\trans
				+
				\frac{1}{\sqrt{n}}
				\mE_{n,11}.
	\)
	This is the same as the $k$th eigenvalue of
	\begin{multline*}
		\frac{1}{n} \mX_{n,11} \mX_{n,11}^\trans
			=
				\mU_{n,1} \mD_n \mV_{n,1}^\trans 
					\mV_{n,1} \mD_n \mU_{n,1}^\trans
				+
				\frac{1}{\sqrt{n}}
				\mU_{n,1} \mD_n \mV_{n,1}^\trans \mE_{n,11}^\trans \\
				+
				\frac{1}{\sqrt{n}}
				\mE_{n,11}
				\mV_{n,1} \mD_n \mU_{n,1}^\trans
				+
				\frac{1}{n}
				\mE_{n,11} \mE_{n,11}^\trans.
	\end{multline*}
	For each $n$, we choose 
	\(
		\mO_n
			=
			\left(
			\begin{matrix}
				\mO_{n,1} & \mO_{n,2}
			\end{matrix}
			\right)
		\in \reals^{n \times n}
	\)
	to be an orthogonal matrix with $\mO_{n,2} \in \reals^{n \times n-k_0}$ 
	and $\mO_{n,2}^\trans \mU_{n,1} = 0$.
	Then, with $\lambda_k(\cdot)$ denoting the $k$th eigenvalue, we have
	\begin{align*}
		\hd_{n,k}^{(1)}
			&=
				\lambda_k \left(
					\frac{1}{n} \mX_{n,11} \mX_{n,11}^\trans
				\right) \\
			&=
				\lambda_k \left(
					\mO_{n}^\trans
					\Big(
						\frac{1}{n} \mX_{n,11} \mX_{n,11}^\trans
					\Big)
					\mO_n
				\right) \\
			&=
				\lambda_k \left(
					\left(
					\begin{matrix}
						\mA_{n,11} & \mA_{n,12} \\
						\mA_{n,21} & \mA_{n,22}
					\end{matrix}
					\right)
				\right),
	\end{align*}
	for
	\(
		\mA_{n,ij} 
			= 
				\frac{1}{n}
				\mO_{n,i}^\trans 
				\mX_{n,11}^\trans \mX_{n,11}
				\mO_{n,j}.
	\)
	Note that
	\(
		\mA_{n,22} 
			= 
				\frac{1}{n} 
				\mO_{n,2}^\trans \mE_{n,11} \mE_{n,11}^\trans \mO_{n,2}
	\)
	is an $(n-k) \times (n-k)$ matrix with \iid $\Normal( 0, \, \sigma^2)$
	entries.
	
	Define $G_n = \lambda_k ( \mA_{n,22} )$
	By the eigenvalue interlacing inequality \cite{golub1996mc}, we have
	that
	\(
		\hd_{n,k}^{(1)}
			\geq
				G_n.
	\)
	Moreover, Theorems~\ref{T:mp-limit}~and~\ref{T:max-wishart-eig-limit}
	give us that 
	\(
		G_n
			\toas 
				\sigma^2
				\left(
					1 + \frac{1}{\sqrt{\gamma_1}}
				\right)^2.
	\)
	Hence, almost surely for $n$ large enough 
	$\hd_{n,k}^{(1)} \geq G_n \geq \sigma^2$.
\end{proof}

With these lemmas, we can prove the following:

\begin{lemma}
	The terms in the expected model approximation error converge as:
	\begin{gather*}
		\begin{split}
		\E\bigg[ 
			\frac{p}{n_2 \, p_2} 
			\Big\|
				\sqrt{n} \,
				\mU_{n,2} 
				( 
					\mD_n 
					&- 
					\mD_n 
					\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
					\mD_n
				)
				\mV_{n,2}^\trans 
			\Big\|_\Frob^2
		\bigg] \\
			&\to 
			 	\sum_{i=1}^{k \wedge k_0}
					\mu_i 
					\left( 
						1 
						- 
						\sqrt{ \frac{ \mu_i }{ \bmu_{1,i} } }
						\theta_{1,i}
						\varphi_{1,i}
					\right)^2
				+
				\sum_{i=k+1}^{k_0}
					\mu_i,
		\end{split}\\
		\E\left[ 
			\frac{p}{n_2 \, p_2} 
			\Big\|
				\mU_{n,2} 
				\mD_n \mTheta_{n,1} \mhD_{n,1}(k)^+ 
				\mtE_{n,12}
			\Big\|_\Frob^2
		\right]
			\to
				\frac{\sigma^2}{\gamma}
				\cdot 
				\sum_{i=1}^{k \wedge k_0}
					\frac{\mu_i}{\bmu_{1,i}}
					\theta_{1,i}^2,
				\\
		\E\left[ 
			\frac{p}{n_2 \, p_2} 
			\Big\|
				\mtE_{n,21}
				\mhD_{n,1}(k)^+ \mPhi_{n,1}^\trans \mD_n 
				\mV_{n,2}^\trans
			\Big\|_\Frob^2
		\right]
			\to
				\sigma^2
				\cdot
				\sum_{i=1}^{k \wedge k_0} 
					\frac{\mu_i}{\bmu_{1,i}}
					\varphi_{1,i}^2,
				\\
\intertext{and}
		\E\left[ 
			\frac{p}{n_2 \, p_2} 
			\Big\|
				\frac{1}{\sqrt{n}}
				\mtE_{n,21}
				\mhD_{n,1}(k)^+
				\mtE_{n,12}
			\Big\|_\Frob^2
		\right]
			\to
				\frac{\sigma^2}{\gamma}
				\cdot
				\sum_{i=1}^k
					\frac{\sigma^2}{\bmu_{1,i}}.
	\end{gather*}
\end{lemma}
\begin{proof}
	The squared Frobenius norm in the first term is equal to
	\begin{align*}
		\tr\Big(
			&\big(
				\sqrt{n} \,
				\mU_{n,2} 
				( 
					\mD_n 
					- 
					\mD_n 
					\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
					\mD_n
				)
				\mV_{n,2}^\trans 
			\big) \\
			&\qquad\cdot
			\big(
				\sqrt{n} \,
				\mU_{n,2} 
				( 
					\mD_n 
					- 
					\mD_n 
					\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
					\mD_n
				)
				\mV_{n,2}^\trans 
			\big)^\trans
		\Big) \\
		&=
			n
			\cdot
			\tr\Big(
				\big( 
					\mD_n 
					- 
					\mD_n 
					\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
					\mD_n
				\big)
				\cdot
				\big(
					\mV_{n,2}^\trans 
					\mV_{n,2}
				\big) \\
				&\qquad\qquad\cdot
				\big( 
					\mD_n 
					- 
					\mD_n 
					\mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1}
					\mD_n
				\big)
				\cdot
				\big(
					\mU_{n,2}^\trans
					\mU_{n,2} 
				\big) 
			\Big).
	\end{align*}
	Now, 
		$\mU_{n,2}^\trans \mU_{n,2} \toP \frac{1}{K} \mI_{k_0}$,
		$\mV_{n,2}^\trans \mV_{n,2} \toP \frac{1}{L} \mI_{k_0}$,
	and
	\begin{align*}
		\mD_n &- \mD_n \mTheta_{n,1} \mhD_{n,1}(k)^+ \mPhi_{n,1} \mD_n \\
			&\toP
				\diag\Big(
					\mu_1^{1/2}
					-
					\mu_1^{1/2}
					\theta_{1,1}
					\bmu_{1,1}^{-1/2}(k) \,
					\varphi_{1,1}
					\mu_1^{1/2},
					\quad
					\mu_2^{1/2}
					-
					\mu_2^{1/2}
					\theta_{1,2}
					\bmu_{1,2}^{-1/2}(k) \,
					\varphi_{1,2}
					\mu_2^{1/2},
					\quad
					\ldots, \\
					&\qquad\qquad\qquad
					\mu_{k_0}^{1/2}
					-
					\mu_{k_0}^{1/2}
					\theta_{1,k_0}
					\bmu_{1,k_0}^{-1/2}(k) \,
					\varphi_{1,k_0}
					\mu_{k_0}^{1/2} 
				\Big),
	\end{align*}
	where
	\[
		\bmu_{1,i}(k) 
			=
			\begin{cases}
				\bmu_{1,i} &\text{if $i \leq k$,} \\
				0          &\text{otherwise.}
			\end{cases}
	\]
	We can apply the Bounded Convergence to get the result for the first
	term since the elements of $\mU_{n,2}$, $\mV_{n,2}$, $\mTheta_{n,1}$ 
	and $\mPhi_{n,1}$ are bounded by $1$ and since
	Lemma~\ref{L:d1-bounded-away-from-0} ensures that the elements of
	$\mhD_{n,1}(k)^+$ are bounded as well.
	
	The last three terms can be gotten similarly by applying
	Lemmas~\ref{L:frob-random-product}~and~\ref{L:d1-bounded-away-from-0}.
\end{proof}

We can now get an expression for the limit of the estimated model
approximation error.  Specifically,
\begin{align*}
	\begin{split}
	\E \bigg[  
		\frac{p}{n_2 \, p_2}
		\Big\|
			\sqrt{n} \, &\mU_{n,2} \mD_n \mV_{n,2}^\trans
			-
			\mhX_{n,22}(k)
		\Big\|_\Frob^2
	\bigg] \\
		&=
			\sum_{i=1}^{k \wedge k_0}
			\Bigg\{
				\mu_i 
				\left( 
					1
					-
					\sqrt{\frac{\mu_i}{\bmu_{1,i}}}
					\theta_{1,i}
					\varphi_{1,i}
				\right)^2
				+
				\sigma^2
				\frac{\mu_i}{\bmu_i}
				\left(
					\gamma^{-1} \theta_{1,i}^2
					+
					\varphi_{1,i}^2
				\right)
				+
				\frac{\sigma^4}{\gamma \bmu_{1,i}}
			\Bigg\} \\
			&\qquad\qquad+
			\sum_{i=k+1}^{k_0}
				\mu_i
			+
			\frac{\sigma^2}{\gamma}
			\cdot\!\!\!
			\sum_{i=k_0+1}^k
				\frac{\sigma^2}{\bmu_{1,i}}
	\end{split} \\
		&=
			\sum_{i=1}^{k \wedge k_0}
				\beta_i \, \mu_i
			+
			\sum_{i=k+1}^{k_0}
				\mu_i
			+
			\frac{\sigma^2}{\gamma}
			\left(
				1
				+
				\frac{1}{\sqrt{\gamma_1}}
			\right)^{-2}\!\!\!\!\!
			\cdot
			(k - k_0)_+,
\end{align*}
where
\begin{align*}
	\beta_i
		&=
			\left( 
				1
				-
				\sqrt{\frac{\mu_i}{\bmu_{1,i}}}
				\theta_{1,i}
				\varphi_{1,i}
			\right)^2
			+
			\frac{\sigma^2}{\bmu_{1,i}}
			\left(
				\gamma^{-1} \theta_{1,i}^2
				+
				\varphi_{1,i}^2
			\right)
			+
			\frac{1}{\mu_i}
			\frac{\sigma^4}{\gamma \bmu_{1,i}} \\
		&=
			1
			-
			2
			\sqrt{\frac{\mu_i}{\bmu_{1,i}}}
			\theta_{1,i}
			\varphi_{1,i}
			+
			\frac{\mu_i}{\bmu_{1,i}}
			\theta_{1,i}^2
			\varphi_{1,i}^2
			+
			\frac{\sigma^2}{\bmu_{1,i}}
			\left(
				\gamma^{-1} \theta_{1,i}^2
				+
				\varphi_{1,i}^2
			\right)
			+
			\frac{1}{\mu_i}
			\frac{\sigma^4}{\gamma \bmu_{1,i}}.
\end{align*}
When $\mu_{1,i} \leq \frac{\sigma^2}{\sqrt{\gamma_1}}$,
$\theta_{1,i} = \varphi_{1,i} = 0$ and
\(
	\bmu_{1,i} 
		= 
			\frac{K-1}{K} 
			\cdot 
			\sigma^2
			\left(
				1
				+
				\frac{1}{\sqrt{\gamma_1}}
			\right)^2,
\)
so that
\[
	\beta_i
		=
			1
			+
			\frac{1}{\gamma}
			\cdot
			\frac{\sigma^2}{\mu_i}
			\cdot
			\left(
				1
				+
				\frac{1}{\sqrt{\gamma_1}}
			\right)^{-2}.
\]
In the opposite situation
($\mu_{1,i} > \frac{\sigma^2}{\sqrt{\gamma_1}}$), we define
\[
    \rho
        =
            \frac{L-1}{L}
            \cdot
            \frac{K-1}{K}
\]
and get the simplifications
\begin{gather*}
    \frac{\bmu_{1,i}}{\mu_i}
        =
            \rho
            \cdot
            \left(
                1
                +
                (1 + \gamma_1^{-1})
                \frac{\sigma^2}{\mu_{1,i}}
                +
                \frac{\sigma^4}{\gamma_1 \mu_i^2}
            \right), \\
    \theta_{1,i} \varphi_{1,i}
        =
            \rho
            \cdot
            \sqrt{\frac{\mu_i}{\bmu_{1,i}}}
            \cdot
            \left(
                1
                -
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right),
\end{gather*}
so that
\begin{multline*}
    -2 \sqrt{\frac{\mu_i}{\bmu_{1,i}}} \theta_{1,i} \varphi_{1,i}
    +
    \frac{\mu_i}{\bmu_i} \theta_{1,i}^2 \varphi_{1,i}^2 \\
        =
            -
            \rho^2
            \left(
                \frac{\mu_i}{\bmu_{1,i}}
            \right)^2
            \left(
                1
                -
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right)
            \left(
                1
                +
                2( 1 + \gamma_1^{-1}) \frac{\sigma^2}{\mu_{1,i}}
                +
                3
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right),
\end{multline*}
\[
    \frac{\sigma^2}{\bmu_{1,i}}
    \left(
        \gamma^{-1} \theta_{1,i}^2
        +
        \varphi_{1,i}^2
    \right)
        =
            \rho^2
            \left(
                \frac{\mu_i}{\bmu_{1,i}}
            \right)^2
            \left(
                1
                -
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right)
            \left(
                (1 + \gamma_1^{-1})
                \frac{\sigma^2}{\mu_{1,i}}
                +
                2
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right),
\]
and
\[
    \frac{1}{\mu_i}
    \frac{\sigma^4}{\gamma \bmu_{1,i}}
        =
            \rho^2
            \left(
                \frac{\mu_i}{\bmu_{1,i}}
            \right)^2
            \left(
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right)
            \left(
                1
                +
                (1 + \gamma_1^{-1})
                \frac{\sigma^2}{\mu_{1,i}}
                +
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            \right).
\]
Putting it all together, we get
\begin{align*}
    \begin{split}
    \beta_i
        &=
            1
            +
            \rho^2
            \left(
                \frac{\mu_i}{\bmu_{1,i}}
            \right)^2 
            \cdot
            \bigg\{
                \left(
                    1
                    -
                    \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
                \right)
                \left(
                    -
                    1
                    -
                    (1 + \gamma_1^{-1})
                    \frac{\sigma^2}{\mu_{1,i}}
                    -
                    \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
                \right) \\
            &\qquad\qquad\qquad\qquad\qquad+
                \left(
                    \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
                \right)
                \left(
                    1
                    +
                    (1 + \gamma_1^{-1})
                    \frac{\sigma^2}{\mu_{1,i}}
                    +
                    \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
                \right)
            \bigg\}
    \end{split} \\
    &=
        1
        +
        \rho^2
        \left(
            \frac{\mu_i}{\bmu_{1,i}}
        \right)^2
        \left(
            2
            \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
            -
            1
        \right)
        \left(
            1
            +
            (1 + \gamma_1^{-1})
            \frac{\sigma^2}{\mu_{1,i}}
            +
            \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
        \right) \\
    &=
        1
        +
        \frac{2 \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2} - 1}
             {
                1
                +
                (1 + \gamma_1^{-1})
                \frac{\sigma^2}{\mu_{1,i}}
                +
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
             } \\
    &=
        \frac{ 3 \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2} 
               +
               (1 + \gamma_1^{-1})
               \frac{\sigma^2}{\mu_{1,i}}
             }
             {
                1
                +
                (1 + \gamma_1^{-1})
                \frac{\sigma^2}{\mu_{1,i}}
                +
                \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
             } \\
    &=
        \frac{
            \frac{\sigma^2}{\gamma_1 \mu_{1,i}^2}
            \left(
                3 \sigma^2
                +
                (\gamma_1 + 1)
                \mu_{1,i}
            \right)
        }
        {
            1
            +
            (1 + \gamma_1^{-1})
            \frac{\sigma^2}{\mu_{1,i}}
            +
            \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2}
        }.
\end{align*}
In particular, note that $\beta_i < 1$ when
\(
    2 \frac{\sigma^4}{\gamma_1 \mu_{1,i}^2} - 1 < 0,
\)
or equivalently
\(
    \mu_i > \frac{\sigma^2}{\sqrt{\gamma}} \cdot \sqrt{\frac{2}{\rho}}.
\)
Getting the final expressions for $\beta_i$ and $\eta$ in 
Theorem~\ref{T:bcv-expected-me} is a matter of routine algebra.

\section{Summary and future work}\label{S:bcv-theory-summary}

We have provided an analysis of the first-order behavior of bi-cross-validation.  This analysis has shown that BCV gives a biased estimate of prediction error, with an explicit expression for the bias.  Fortunately, the bias is not too bad when the signal strength is large and the leave-out sizes are small.  Importantly, our analysis gives guidance as to how the leave-out sizes should be chosen.  Our theoretical analysis agrees with the simulations done by Owen \& Perry~\cite{owen2009bi}, who observed that larger hold-out sizes tend to perform better.

The form of consistency we give is rather weak since we did not analyze the variance of the BCV prediction error estimate.  We will address this limitation in future work.  For now, we can get some comfort from empirical observations in \cite{owen2009bi} that the variance of the estimator is not too large.  Indeed, based on these simulations, it is entirely possible that the variance becomes negligible for large $n$ and $p$.
