
\chapter[Behavior of the SVD]{Behavior of the SVD in low-rank plus noise models}\label{C:svd-behavior}

\section{Introduction}

Many modern data sets involve simultaneous measurements of a large number of
variables. Some financial applications, such as portfolio selection, involve
looking at the market prices of hundreds or thousands of stocks and their
evolution over time~\cite{markowitz1952ps}. Microarray studies
involve measuring the expression levels of thousands of
genes simultaneously~\cite{schena1995qmg}.  Text processing involves counting
the appearances of thousands of words on thousands of documents~\cite{manning1999fsn}.  In all of these
applications, it is natural to suppose that even though the data are
high-dimensional, their dynamics are driven by a relatively small number of
latent factors.

Under the hypothesis that one ore more latent factors explain the behavior
of a data set, principal component analysis (PCA) \cite{jolliffe2002pca} is a
popular method for estimating these latent factors.  When the dimensionality
of the data is small relative to the sample size, Anderson's 1963 paper
\cite{anderson1963atp} gives a complete treatment of how the procedure
behaves.  Unfortunately, his results do not apply when the sample size is
comparable to the dimensionality.

A further complication with many modern data sets is that it is no longer
appropriate to assume the observations are iid. Also, sometimes it is
difficult to distinguish between ``observation'' and ``variable''. We call such
data ``transposable''. A microarray study involving measurements of $p$ genes
for $n$ different patients can be considered transposable: we can either treat
each gene as a measurement of the patient, or we can treat each patient as a
measurement of the gene. There are correlations both between the genes
\emph{and} between the patients, so in fact both interpretations are relevant~\cite{efron2008smi}.

One can study latent factor models in a transpose-agnostic way by considering
generative models of the form $\mX = \mU \mD \mV^\trans + \mE$. Here, $\mX$ is
the $n \times p$ observed data matrix. The unobserved row and column factors
are given by $\mU$ and $\mV$, respectively, matrices with $k$ orthonormal
columns each, and $k \ll \min(n,p)$. The strengths of the factors are given
in the $k\times k$ diagonal matrix $\mD$, and $\mE$ is a matrix of noise. A
natural estimator for the latent factor term $\mU \mD \mV^\trans$ can be
constructed by truncating the singular value decomposition (SVD)
\cite{golub1996mc} of $\mX$. The goal of this paper is to study the behavior
of the SVD when $n$ and $p$ both tend to infinity, with their ratio tending to
a nonzero constant.

In an upcoming paper, Onatski~\cite{onatski2009} gives a thorough
treatment of latent factor models. He assumes that the elements of $\mE$ are iid
Gaussians, that $\sqrt{n} ( \mU^\trans \mU - \mI_k) $ tends to a multivariate
Gaussian distribution, and that $\mV$ and $\mD$ are both nonrandom. The
contributions of this chapter are twofold. First, we work under a
transpose-agnostic generative model that allows randomness in all three of
$\mU$, $\mD$, and $\mV$. Second, we give a more complete picture of the
almost-sure limits of the sample singular vectors, taking into account the
signs of the dot products between the population and sample vectors.

We describe the main results in Section~\ref{S:lowrank-assumptions-results}.
Sections~\ref{S:lowrank-preliminaries}--\ref{S:lowrank-gamma-lt-1} are dedicated to proving the two main theorems.  Finaly, we discuss related work and extensions in Section~\ref{S:lowrank-related-extensions}.  We owe a substantial debt to Onatski's work.  Although most of the details below are different, the general outline and the main points of the argument are the same.

\section{Assumptions, notation, and main results}\label{S:lowrank-assumptions-results}

Here we make explicit what the model and assumptions are, and we present our
main results.

\subsection{Assumptions and notation}

We will work sequences of matrices indexed by $n$, with
\begin{equation}
    \mX_n = \sqrt{n} \, \mU_n \mD_n \mV_n^\trans + \mE_n.
\end{equation}
We denote by $\sqrt{n} \, \mU_n \mD_n \mV_n^\trans$ the ``signal'' part of the
matrix $\mX_n$ and $\mE_n$ the ``noise'' part. We will often refer to $\mU_n$
and $\mV_n$ as the left and right factors of $\mX_n$, and the matrix $\mD_n$
will be called the matrix of normalized factor strengths. The first two
assumptions concern the signal part:

\begin{assumption}\label{A:factors}
    The factors $\mU_n$ and $\mV_n$ are random matrices of dimensions
    $n \times k$ and $p \times k$, respectively, normalized so that
    $\mU_n^\trans \mU_n =  \mV_n^\trans \mV_n = \mI_k$. 
    The number of factors, $k$, is a fixed constant.  The aspect ratio
    satisfies $\frac{n}{p} = \gamma + \oh\left( \frac{1}{\sqrt{n}} \right)$
    for a fixed nonzero constant $\gamma \in (0,\infty)$.
\end{assumption}
\noindent

\begin{assumption}\label{A:sizes}
    The matrix of factor strengths, $\mD_n$, is of size $k\times k$ and 
    diagonal, with 
    \(
        \mD_n = \diag \left( d_{n,1}, d_{n,2}, \ldots, d_{n,k} \right)
    \)
    and
    $d_{n,1} > d_{n,2} > \cdots > d_{n,k} > 0$.  The matrix $\mD_n$ converges
    almost surely to a deterministic matrix 
    $\mD = \diag( \mu_1^{1/2}, \mu_2^{1/2}, \ldots, \mu_k^{1/2})$ with
    $\mu_1 > \mu_2 > \cdots > \mu_k > 0$. Moreover, the vector
    \(
        \sqrt{n} ( d_{n,1}^2 - \mu_1, d_{n,1}^2 - \mu_2, 
                   \ldots, 
                   d_{n,K}^2 - \mu_k )
    \)
    converges in distribution to a mean-zero multivariate normal with 
    covariance matrix $\mSigma$ having entries $\Sigma_{ij} = \sigma_{ij}$
    (possibly degenerate).
\end{assumption}
\noindent
The next assumption concerns the noise part:

\begin{assumption}\label{A:noise}
    The noise matrix $\mE_n$ is an $n\times p$ matrix with entries 
    $E_{n,ij}$ independent $\Normal(0,\, \sigma^2)$ random variables, also
    independent of $\mU_n$, $\mD_n$, and $\mV_n$.
\end{assumption}
\noindent

For analyzing the SVD of $\mX_n$, we need to introduce some more notation. We denote the columns of $\mU_n$ and
$\mV_n$ by $\vu_{n,1}, \vu_{n,2}, \ldots, \vu_{n,k}$ and $\vv_{n,1},
\vv_{n,2}, \ldots, \vv_{n,k}$, respectively. We let 
\(
    \sqrt{n} \, \mhU_n \mhD_n \mhV_n^\trans
\)
be the singular value decomposition of
$\mX_n$ truncated to $k$ terms, where $\mhD_n = \diag( \hat \mu_{n,1}^{1/2},
\hat \mu_{n,2}^{1/2}, \ldots, \hat \mu_{n,k}^{1/2})$ and the columns of $\mhU$ and $\mhV$ are given
by $\vhu_{n,1}, \vhu_{n,2} \ldots, \vhu_{n,k}$ and $\vhv_{n,1}, \vhv_{n,2},
\ldots, \vhv_{n,k}$, respectively.


\subsection{Main results}

We are now in a position to say what the results are.  There are two main
theorems, one concerning the sample singular values and the other concerning
the sample singular vectors.  First we give the result about the singular
values.
\begin{theorem}\label{T:spiked-eigenvalue-limits}
    Under Assumptions~\ref{A:factors}~--~\ref{A:noise},
    the vector $\hat \vmu_n = (\hmu_{n,1}, \hmu_{n,2}, \ldots, \hmu_{n,k})$
    converges almost surely to 
    $\vbmu = (\bmu_1, \bmu_2, \ldots, \bmu_k)$,
    where
    \begin{equation}
        \bmu_i
        =
        \begin{cases}
            \left( \mu_i + \sigma^2 \right)
            \left( 1 + \frac{\sigma^2}{\gamma \mu_i} \right)
                &\text{when $\mu_i > \frac{\sigma^2}{\sqrt{\gamma}}$}, \\
            \sigma^2 \left( 1 + \frac{1}{\sqrt{\gamma}} \right)^2
                &\text{otherwise.}
        \end{cases}
    \end{equation}
    Moveover, 
    \(
        \sqrt{n} (\vhmu - \vbmu)
    \)
    converges in distribution to a (possibly degenerate) multivariate normal
    with covariance matrix $\mbSigma$ whose $ij$ element is given by
    \begin{equation}
        \bsigma_{ij}
        \equiv
        \begin{cases}
            \begin{aligned}
                \sigma_{ij}
                &\left(
                    1 - \frac{\sigma^4}{\gamma \mu_i^2}
                \right)
                \left(
                    1 - \frac{\sigma^4}{\gamma \mu_j^2}
                \right)
                \\
                &+ \delta_{ij}
                2
                \sigma^2 \,
                \bigg(
                    2 \mu_i + (1 + \gamma^{-1}) \, \sigma^2
                \bigg)
                \bigg(
                    1 - \frac{ \sigma^4 }{ \gamma  \mu_i^2 }
                \bigg)
            \end{aligned}
                &\text{when $\mu_i, \, \mu_j >
                           \frac{\sigma^2}{\sqrt{\gamma}}$,} \\
            0
                &\text{otherwise.}
        \end{cases}
    \end{equation}
    When $\sigma_{ii} = 2 \mu_i^2$, 
    and $\mu_i > \frac{\sigma^2}{\sqrt{\gamma}}$, the variance of the $i$th
    component simplifies to
    \(
        \bsigma_{ii} 
        = 
        2 (\mu_i + \sigma^2)^2 
        \left( 1 - \frac{\sigma^4}{\gamma \mu_i^2} \right).
    \)
\end{theorem}

Next, we give the result for the singular vectors:
\begin{theorem}\label{T:spiked-eigenvector-limits}
    Suppose Assumptions~\ref{A:factors}~--~\ref{A:noise} hold.  Then the
    $k\times k$ matrix $\mTheta_n \equiv \mV_n^\trans \mhV_n$ converges almost 
    surely to a matrix 
    $\mTheta = \diag(\theta_1, \theta_2, \ldots, \theta_k)$, where
    \begin{equation}
        \theta_i^2
        =
        \begin{cases}
            \left( 1 - \frac{\sigma^4}{ \gamma \mu_i^2} \right)
            \left( 1 + \frac{\sigma^2}{ \gamma \mu_i  } \right)^{-1}
            &\text{when $\mu_i > \frac{\sigma^2}{\sqrt{\gamma}}$,} \\
            0
            &\text{otherwise.}
        \end{cases}
    \end{equation}
    Also, the $k\times k$ matrix $\mPhi_n \equiv \mU_n^\trans \mhU_n$
    converges almost surely to a matrix
    $\mPhi = \diag(\varphi_1, \varphi_2, \ldots, \varphi_k)$, where    
    \begin{equation}
        \varphi_i^2
        =
        \begin{cases}
            \left( 1 - \frac{\sigma^4}{ \gamma \mu_i^2} \right)
            \left( 1 + \frac{\sigma^2}{ \mu_i  } \right)^{-1}
            &\text{when $\mu_i > \frac{\sigma^2}{\sqrt{\gamma}}$,} \\
            0
            &\text{otherwise.}
        \end{cases}
    \end{equation}
    Moreover, $\theta_i$ and $\varphi_i$ almost surely have the same
    sign.
\end{theorem}


\subsection{Notes}

Some discussion of the assumptions and the results are in order:

\begin{enumerate}
    
\item
Assumptions~\ref{A:factors}~and~\ref{A:sizes} are simpler than the assumptions given in many other papers while still being quite general.  For example, Paul's ``spiked'' covariance model has data of the form
\[
    \mX = \mZ \mXi^\trans + \mE,
\]
where $\mXi$ is an $p \times k$ matrix of factors and $\mZ$ is an $n\times k$ matrix of factor loadings whose rows are iid multivariate $\Normal(0,\, \mC)$ random variables for covariance matrx $\mC$ having eigen-decomposition $\mC = \mQ \mLambda \mQ^\trans$.  Letting $\mZ = \sqrt{n} \mhP \mhLambda^{1/2} \mhQ^\trans$ be the SVD of $\mZ$, Anderson's results \cite{anderson1963atp} give us that $\mhLambda$ and $\mhQ$ converge almost surely to $\mLambda$ and $\mQ$, respectively, and that the diagonal elements of $\sqrt{n} (\mhLambda - \mLambda)$ converge to a mean-zero multivariate normal whenever $\mC$ has no repeated eigenvalues.  If we define $\mU = \mhP$, $\mD = \mhLambda^{1/2}$, and $\mV = \mXi \mhQ$, then $\mX = \sqrt{n} \, \mU \mD \mV^\trans + \mE$, where the factors satisfy Assumptions~\ref{A:factors}~and~\ref{A:sizes}.  Dropping the normality assumption on the rows of $\mZ$ poses no problem.  Moreover, we can suppose instead of iid that the rows of $\mZ$ are a martingale difference array with well-behaved low-order moments and still perform a transformation of the variables to get factors of the form we need for Theorems~\ref{T:spiked-eigenvalue-limits}~and~\ref{T:spiked-eigenvector-limits}.

\item
There is a sign-indeterminancy in the sample and population singular vectors. We choose them arbitrarily.

\item
If instead of almost-sure convergence, $\mD_n$ converges in probability to $\mD$,
then the theorems still hold with $\vhmu_n$, $\mTheta_n$, and $\mPhi_n$ converging
in probability.

\item
The assumption that
\(
    \sqrt{n}
    \left(
        d_1^2 - \mu_1,
        d_2^2 - \mu_2,
        \ldots
        d_k^2 - \mu_k
    \right)
\)
converges weakly is only necessary for determining second-order
behavior; the first order results still hold without this assumption.  If
the limiting distribution of the vector of factor strengths
\(
    \sqrt{n}
    \left(
        d_1^2 - \mu_1,
        d_2^2 - \mu_2,
        \ldots
        d_k^2 - \mu_k
    \right)
\)
is non-normal, one can still get at the second-order behavior of the SVD 
through a small adaptation of the proof.

\item 
Most of the results in Theorems~\ref{T:spiked-eigenvalue-limits}~and~\ref{T:spiked-eigenvector-limits} can be gotten from Onatski's results \cite{onatski2009}.   However, Onatski does not show that $\sqrt{n} (\hmu_i - \bmu_i) \toP 0$ when $\mu_i$ is below the critical threshold.  Furthermore, Onatski proves convergence in probability, not almost sure convergence.  Lastly, Onatski does not get at the joint behavior between $\mTheta_n$ and $\mPhi_n$.

\end{enumerate}

\section{Preliminaries}\label{S:lowrank-preliminaries}

Without loss of generality we will assume that $\sigma^2 = 1$.  Until Section~\ref{S:lowrank-gamma-lt-1} , we will also assume that $\gamma \geq 1$.

\subsection{Change of basis}

A convenient choice of basis will make it easier to study the SVD of $\mX_n$.  Define $\mU_{n,1} = \mU_n$, and choose $\mU_{n,2}$ so that
\(
    \left( 
    \begin{matrix}
        \mU_{n,1} & \mU_{n,2}
    \end{matrix}
    \right)
\) 
is an orthogonal matrix. Similarly, put $\mV_{n,1} = \mV_n$ and choose
$\mV_{n,2}$ so that
\(
    \left(
    \begin{matrix}
        \mV_{n,1} & \mV_{n,2}
    \end{matrix}
    \right)
\)
is orthogonal.  If we define
\(
    \mtE_{n,ij} = \mU_{n,i}^\trans \mE_n \mV_{n,j}
\)
and
\(
    \mX_{n,ij} = \mU_{n,i}^\trans \mX_n \mV_{n,j},
\)
then in block form,
\[
    \left( 
    \begin{matrix}
        \mU_{n,1}^\trans \\
        \mU_{n,2}^\trans
    \end{matrix}
    \right)
    \mX_n
    \left(
    \begin{matrix}
        \mV_{n,1} & \mV_{n,2}
    \end{matrix}
    \right)
    =
    \left(
    \begin{matrix}
        \sqrt{n} \mD_n + \mtE_{n,11} & \mtE_{n,12} \\
        \mtE_{n,21}                  & \mtE_{n,22}
    \end{matrix}
    \right).
\]
Because Gaussian white noise is orthogonally invariant, the
matrices $\mtE_{n,ij}$ are all independent with iid $\Normal(0, \, 1)$ elements.  Let
\begin{equation}
    \mtE_{n,22}
    =
    \sqrt{n}
    \left(
    \begin{matrix}
        \mO_{n,1} & \mO_{n,2}
    \end{matrix}
    \right)
    \left(
    \begin{matrix}
        \mLambda_n^{1/2} \\
        0
    \end{matrix}
    \right)
    \mP_{n}^\trans
\end{equation}
be the SVD of $\mtE_{n,22}$, with
\(
    \mLambda_n
    =
    \diag \left(
        \lambda_{n,1}, 
        \lambda_{n,2}, 
        \ldots
        \lambda_{n,p-k}
    \right)
\)
.  Note that 
\(
    \mtE_{n,22}^\trans \mtE_{n,22}
    \sim
    \Wishart_{p-k} \left( n-k, \mI_{p-k} \right).
\)
Define
\begin{align}
    \mtX_n
        &=
            \left(
            \begin{matrix}
                \mI_k & 0 \\
                0  & \mO_{n,1}^\trans \\
                0  & \mO_{n,2}^\trans
            \end{matrix}
            \right)
            \left(
            \begin{matrix}
                \sqrt{n} \mD_n + \mtE_{n,11} & \mtE_{n,12} \\
                \mtE_{n,21}                  & \mtE_{n,22}
            \end{matrix}
            \right)
            \left(
            \begin{matrix}
                \mI_k & 0 \\
                0     & \mP_n
            \end{matrix}
            \right) \notag \\
        &=
            \left(
            \begin{matrix}
                \sqrt{n} \mD_n + \mE_{n,11} & \mE_{n,12} \\
                \mE_{n,21}                  & \sqrt{n} \mLambda^{1/2}_n \\
                \mE_{n,31}                  & 0
            \end{matrix}
            \right),
\end{align}
where 
$\mE_{n,11} = \mtE_{n,11}$, 
$\mE_{n,12} = \mtE_{n,12} \mP_n$,
$\mE_{n,21} = \mO_{n,1}^\trans \mtE_{n,21}$, and
$\mE_{n,31} = \mO_{n,2}^\trans \mtE_{n,31}$.
Let
\(
    \mtU_n \mtD_n \mtV_n
\)
be the SVD of $\mtX_n$, truncated to $k$ terms.  Lastly, put the left and right singular vectors in block form as
\begin{equation}
    \mtU_n
    =
    \left(
    \begin{matrix}
        \mtU_{n,1} \\
        \mtU_{n,2}
    \end{matrix}
    \right)
\end{equation}
and
\begin{equation}
    \mtV_n
    =
    \left(
    \begin{matrix}
        \mtV_{n,1} \\
        \mtV_{n,2}
    \end{matrix}
    \right),
\end{equation}
where $\mtU_{n,1}$ and $\mtV_{n,1}$ both $k\times k$ matrices.

We have gotten to $\mtX_n$ via an orthogonal change of basis applied to $\mX_n$.  By carefully choosing this basis, we have assured that:
\begin{enumerate}
    \item The blocks of $\mtX_n$ are all independent.
    \item The elements of the matrices $\mE_{n,ij}$ are 
        iid $\Normal\left( 0, 1 \right)$.
    \item The elements 
        $\{ n \lambda_{n,1}, n \lambda_{n,2}, \ldots, 
            n \lambda_{n,p-k} \}$ are eigenvalues from
        a white Wishart matrix with $n-k$ degrees of freedom.
    \item $\mtX_n$ and $\mX_n$ have the same singular values.  This implies
        that $\mhD_n = \mtD_n$.
    \item The left singular vectors of $\mX_n$ can be recovered from the
        left singular vectors of $\mtX_n$ via multiplication by an orthogonal
        matrix.  The same holds for the right singular vectors.
    \item The dot product matrix $\mU_n^\trans \mhU_n$ is equal to
        $\mtU_{n,1}$.
        Similarly, $\mV_n^\trans \mhV_n = \mtV_{n,1}$.
\end{enumerate}
This simplified form of the problem makes it much easier to analyze the 
SVD of $\mX_n$.


\section{The secular equation}\label{S:secular-equation}

We set $\mS_n = \frac{1}{n} = \mtX_n^\trans \mtX_n$.  The eigenvalues and eigenvectors of $\mS_n$ are the squares of the singular values of $\frac{1}{\sqrt{n}} \mtX_n$ and its right singular vectors, respectively.  In block form, we have
\begin{equation}
    \mS_n
    =
    \left(
    \begin{matrix}
        \mS_{n,11} & \mS_{n,12} \\
        \mS_{n,21} & \mS_{n,22}
    \end{matrix}
    \right),
\end{equation}
where
\begin{subequations}
\begin{align}
    \begin{split}
    \mS_{n,11}
        &=
            \mD_n^2 
            + 
            \frac{1}{\sqrt{n}} 
            \left( 
                \mD_n \mE_{n,11} + \mE_{n,11}^\trans \mD_n
            \right) \\
            &\phantom{= \mD_n^2} +
            \frac{1}{n}
            \left(
                \mE_{n,11}^\trans \mE_{n,11}
                +
                \mE_{n,21}^\trans \mE_{n,21}
                +
                \mE_{n,31}^\trans \mE_{n,31}
            \right),
    \end{split}
\end{align}
and
\begin{align}
    \mS_{n,12}
        &=
            \frac{1}{\sqrt{n}}
            \left(
                \mD_n \mE_{n,12} + \mE_{n,21}^\trans \mLambda_n^{1/2}
            \right)
            +
            \frac{1}{n}
            \mE_{n,11}^\trans \mE_{n,12}, \\
    \mS_{n,21}
        &=
            \frac{1}{\sqrt{n}}
            \left(
                \mE_{n,12}^\trans \mD_n
                +
                \mLambda_n^{1/2} \mE_{n,21}
            \right)
            +
            \frac{1}{n}
            \mE_{n,12}^\trans \mE_{n,11}, \\
    \mS_{n,22}
        &=
            \mLambda_n
            +
            \frac{1}{n}
            \mE_{n,12}^\trans \mE_{n,12}.
\end{align}
\end{subequations}

Now we study the eigendecomposition of $\mS_n$.  If 
\(
    \vv
    = 
    \left( 
    \begin{matrix}
        \vv_{1} \\
        \vv_{2}
    \end{matrix}
    \right)
\)
is an eigenvector of $\mS_n$ with eigenvalue $\mu$, then
\[
    \left(
    \begin{matrix}
        \mS_{n,11} & \mS_{n,12} \\
        \mS_{n,21} & \mS_{n,22}
    \end{matrix}
    \right)
    \left( 
    \begin{matrix}
        \vv_{1} \\
        \vv_{2}
    \end{matrix}
    \right)
    =
    \mu
    \left( 
    \begin{matrix}
        \vv_{1} \\
        \vv_{2}
    \end{matrix}
    \right).
\]
If $\mu$ is not an eigenvalue of $\mS_{n,22}$, then we get
\begin{subequations}
\begin{gather}\label{E:v2-from-v1}
    \vv_2
    =
    -
    \left(
        \mS_{n,22}
        -
        \mu
        \mI_{p-k}
    \right)^{-1}
    \mS_{n,21} \,
    \vv_1, \qquad\text{and} \\
    \label{E:secular-nof}
    \left(
        \mS_{n,11}
        -
        \mu
        \mI_k
        -
        \mS_{n,12}
        \left(
            \mS_{n,22}
            -
            \mu
            \mI_{p-k}
        \right)^{-1}
        \mS_{n,21}
    \right)
    \vv_1
    =
    0.
\end{gather}
\end{subequations}
Conversely, if $(\mu, \vv_1)$ is a pair that solves~\eqref{E:secular-nof}
and $\vv_1 \neq 0$, then
\begin{equation}\label{E:v-from-v1}
    \vv
    =
    \left(
    \begin{matrix}
        \vv_1 \\
        -
        \left(
            \mS_{n,22}
            -
            \mu
            \mI_{p-k}
        \right)^{-1}
        \mS_{n,21} \,
        \vv_1
    \end{matrix}
    \right)
\end{equation}
is an eigenvector of $\mS_n$ with eigenvalue $\mu$.  

We define
\begin{gather}
    \mT_n(z)
        =
            \mS_{n,11}
            -
            z
            \mI_k
            -
            \mS_{n,12}
            \left(
                \mS_{n,22}
                -
                z
                \mI_{p-k}
            \right)^{-1}
            \mS_{n,21}
            \label{E:secular-T} \\
    f_n ( z, \vx )
        =
            \mT_n(z) \,
            \vx, \label{E:secular-f}
\end{gather}
and refer to the equation $f_n(z, \vx) = 0$ as the \emph{secular equation}. 
This terminology comes from numerical linear algebra, where a secular equation is analogous to a characteristic equation; it is an equation whose
roots are eigenvalues of a matrix.   Typically, secular equations arise in eigenvalue perturbation problems.  The name comes from the fact that they originally arose from studying secular perturbations of planetary orbits.  A more standard use of the term ``secular equation'' would involve the equation $\det \mT_n(z) = 0$.  However, for our purposes it is more convenient to work with $f_n$.

\subsection{Outline of the proof}

With probability one, $\mS_n$ and $\mS_{n,22}$ have no eigenvalues in common, so every eigenvalue-eigenvector pair of $\mS_n$ is a solution to the secular equation.  To study these solutions, we first focus on $\mT_n(z)$.  

It turns out that when $z > (1 + \gamma^{-1/2})^2$, we can find a perturbation expansion for $\mT_n(z)$. In Section~\ref{S:anal-sec-eqn}, we show that for $z$ above this threshold, we can expand
\[
    \mT_n(z)
        =
            \mT_0(z)
            +
            \frac{1}{\sqrt{n}}
            \mT_{n,1}(z),
\]
where $\mT_0(z)$ is deterministic and $\mT_{n,1}(z)$ converges in distribution
to a matrix-valued Gaussian process with known covariance.  With this expansion, in Section~\ref{S:solutions-to-secular-eq} we study sequences of solutions to the equation $f_n(z_n, \vx_n) = 0$.  Using a Taylor series expansion for $z_n > (1 + \gamma^{-1/2})^2$, we write
\begin{align*}
    z_n
        &= 
            z_0 
            + 
            \frac{1}{\sqrt{n}}
            z_{n,1} 
            + 
            \ohP\left( \frac{1}{\sqrt{n}} \right), \\
    \vx_n
        &=
            \vx_0
            +
            \frac{1}{\sqrt{n}}
            \vx_{n,1}
            + 
            \ohP\left( \frac{1}{\sqrt{n}} \right),
\end{align*}
where $(z^0, \vx^0)$ is the limit of $(z_n, \vx_n)$ as $n\to \infty$ and
$(z_{n,1}, \vx_{n,1})$ is the order-$\frac{1}{\sqrt{n}}$ approximation error.

In Section~\ref{S:singular-values-and-vectors} we get the singular values and singular vectors of $\frac{1}{\sqrt{n}} \mtX_n$.  From every solution pair $(z_n, \vx_n)$, to the equation $f_n(z_n, \vx_n) = 0$, we can construct an eigenvalue and an eigenvector of $\mS_n$ using equation~\eqref{E:v-from-v1}.  Then, we can get the corresponding left singular vectors through multiplying by $\mtX_n$.  For singular values above the critical threshold, we use the perturbation results of the previous two sections.  Below the threshold, we use a more direct approach involving the fluctuations of the top eigenvalues of $\mLambda_n$.

Finally, in Section~\ref{S:lowrank-gamma-lt-1} we show that the results still hold when $\gamma < 1$.  For parts of the proof, we will need some limit theorems for weighted sums from 
Appendix~\ref{A:weighted-sums}.

\section{Analysis of the secular equation}\label{S:anal-sec-eqn}

We devote this section to finding a simplified formula for $\mT_n(z)$ for certain values of $z$.  By a bit of algebra and analysis, we find the first- and second-order behavior of the secular equation.

First, we employ the Sherman-Morrison-Woodbury formula~\cite{golub1996mc} to  get an expression for $\left(\mS_{n,22} - z \mI_{p-k}\right)^{-1}$.  As a reminder, the Sherman-Morrison-Woodbury formula states that for matrices $\mA$, $\mB$, and $\mC$ with compatible dimensions, the following formula for the inverse holds:
\[
    \left(
        \mA
        +
        \mB
        \mC
    \right)^{-1}
        =
        \mA^{-1}
        -
        \mA^{-1}
        \mB
        \left(
            \mI
            +
            \mC
            \mA^{-1}
            \mB
        \right)^{-1}
        \mC
        \mA^{-1}.
\]
Using this, we can write
\begin{align}\label{E:lowrank-smw-expansion}
    \left(
        \mS_{n,22} - z \mI_{p-k}
    \right)^{-1}
        &=
            \left(
                \left(
                    \mLambda_n - z \mI_{p-k}
                \right)
                +
                \frac{1}{n}
                \mE_{n,12}^\trans \mE_{n12}
            \right)^{-1} \notag \\
        \begin{split}
        &=
            \left(
                \mLambda_n
                -
                z
                \mI_{p-k}
            \right)^{-1} \\
            &\qquad-
            \frac{1}{n}
            \left(
                \mLambda_n
                -
                z
                \mI_{p-k}
            \right)^{-1} \\
            &\qquad\qquad\cdot
            \mE_{n,12}^\trans
            \left(
                \mI_k
                +
                \frac{1}{n}
                \mE_{n,12}
                \left(
                    \mLambda_n
                    -
                    z
                    \mI_{p-k}
                \right)^{-1}
                \mE_{n,12}^\trans
            \right)^{-1}
            \mE_{n,12} \\
            &\qquad\qquad\cdot
            \left(
                \mLambda_n
                -
                z
                \mI_{p-k}
            \right)^{-1}.
    \end{split}
\end{align}
Next, we define $\mtD_n = \mD_n + \frac{1}{\sqrt{n}} \mE_{n,11}$, so that
\begin{align}\label{E:lowrank-S-schur}
    \begin{split}
    \mS_{n,12} 
    \Big(
        \mS_{n,22} &- z \mI_{p-k}
    \Big)^{-1}
    \mS_{n,21} \\
        &=
            \frac{1}{n}
            \left(
                \mtD_n^\trans \mE_{n,12}
                +
                \mE_{n,21}^\trans \mLambda_n^{1/2}
            \right) \\
            &\qquad\cdot
            \left(
                \mS_{n,22} - z \mI_{p-k}
            \right)^{-1} \\
            &\qquad\cdot
            \left(
                \mE_{n,12}^\trans \mtD_n
                +
                \mLambda_n^{1/2} \mE_{n,21}
            \right).
    \end{split}
\end{align}

There are three important terms coming out of equations~\eqref{E:lowrank-smw-expansion}~and~\eqref{E:lowrank-S-schur} that involve $\mLambda_n$.  These are
\(
    \mE_{n,12}
    \left(
        \mLambda_n - z \mI_{p-k}
    \right)^{-1}
    \mE_{n,12}^\trans,
\)
\(
    \mE_{n,21}^\trans
    \left(
        \mLambda_n - z \mI_{p-k}
    \right)^{-1}
    \mE_{n,21},
\)
and
\(
    \mE_{n,12}
    \left(
        \mLambda_n - z \mI_{p-k}
    \right)^{-1}
    \mLambda_n^{1/2}
    \mE_{n,21}.
\)
Each term can we written as a weighted sum of outer products.  There is
dependence between the weights, but the outer products are iid.  For example,
with
\(
    \mE_{n,12}
    =
    \left(
    \begin{matrix}
        \vE_{n,12,1} &
        \vE_{n,12,2} &
        \cdots &
        \vE_{n,12,p-k}
    \end{matrix}
    \right),
\) 
we have
\[
    \mE_{n,12}
    \left(
        \mLambda_n - z \mI_{p-k}
    \right)^{-1}
    \mE_{n,12}^\trans,
        =
            \sum_{\alpha=1}^{p-k}
                \frac{1}{\lambda_{n,\alpha} - z}
                \cdot
                \vE_{n,12,\alpha} \,
                \vE_{n,12,\alpha}^\trans.
\]
From the Central Limit Theorem and the Strong Law of Large Numbers
we know that
\(
    \frac{1}{p-k}
    \sum_{\alpha=1}^{p-k}
        \vE_{n,12,\alpha} \,
        \vE_{n,12,\alpha}^\trans
        \toas
            \mI_{k}
\)
and that
\(
    \sqrt{p-k}
    \left(
        \frac{1}{p-k}
        \sum_{\alpha=1}^{p-k}
            \vE_{n,12,\alpha} \,
            \vE_{n,12,\alpha}^\trans
        -
        \mI_{k}
    \right)
\)
converges in distribution to mean-zero symmetric matrix whose elements are jointly multivariate normal.  In the limiting distribution, the elements have variance $2$ along the diagonal and variance $1$ off of it; aside from the matrix being symmetric, the unique elements are all uncorrelated.  The
difficulty in analyzing these terms comes from the dependence between 
the weights.

When $z$ is in the support of $\FMP_\gamma$, the weights behave
erratically, but otherwise they have some nice properties.
First of all, $\mLambda_n$ is independent of $\mE_{n,12}$ and $\mE_{n,21}$.
Secondly, the Wishart LLN~(Corollary~\ref{C:wishart-lln}) and
Theorem~\ref{T:max-wishart-eig-limit} ensure that for
$z~>~(1+\gamma^{-1/2})^2$,
\(
    \frac{1}{p-k}
    \sum_{\alpha=1}^{p-k}
        \frac{1}{\lambda_{n,\alpha} - z}
    \toas
    \int
        \frac{1}{t - z}
        d\FMP_\gamma(t).
\)
Moreover, the Wishart CLT~(Theorem~\ref{T:wishart-clt}) guarantees that
the error is of size $\OhP(\frac{1}{n})$.  These properties in
combination with the limit theorems for weighted sums in 
Appendix~\ref{A:weighted-sums} allow us to get the behavior of
\(
    \mE_{n,12}
    \left(
        \mLambda_n
        -
        z
        \mI_k
    \right)^{-1}
    \mE_{n,12}^\trans
\)
and its cousins.

The function
\begin{align}
    m(z)
        &\define
            \int
                \frac{1}{t - z} d\FMP_\gamma(t) \notag \\
        &=
            \gamma
            \cdot
            \frac{ -(z - 1 + \gamma^{-1})
                   +\sqrt{ \big( z - b_\gamma \big) 
                           \big( z - a_\gamma \big) } }
                 { 2 z }
\end{align}
is the Stieltjes transform of $\FMP_\gamma$, 
where
\(
    a_\gamma = \left( 1 - \gamma^{-1/2} \right)^2
\)
and
\(
    b_\gamma = \left( 1 + \gamma^{-1/2} \right)^2.
\)
When restricted to the complement of the support of $\FMP_\gamma$, $m$ has a well-defined inverse
\begin{equation}\label{E:stieltjes-inverse}
    z(m)
        = 
            -
            \frac{1}{m}
            +
            \frac{1}{1 + \gamma^{-1} \, m}.
\end{equation}
Also, $m$ is strictly increasing and convex outside the support of
$\FMP_\gamma$.  This function appears frequently in the remainder of the chapter.

\begin{lemma}\label{L:eij-product-limits}
    If $z > b_\gamma$, then
    \begin{subequations}
    \begin{align}
        \frac{1}{n}
        \mE_{n,12}
        \left(
            \mLambda_n
            -
            z \mI_{p-k}
        \right)^{-1}
        \mE_{n,12}^\trans
            &\toas
                \gamma^{-1}
                m(z)
                \cdot
                \mI_k, \\
        \frac{1}{n}
        \mE_{n,21}^\trans
        \left(
            \mLambda_n
            -
            z \mI_{p-k}
        \right)^{-1}
        \mE_{n,21}
            &\toas
                \gamma^{-1}
                m(z)
                \cdot
                \mI_k,
    \end{align}
    and
    \begin{equation}
        \frac{1}{n}
        \mE_{n,12}
        \left(
            \mLambda_n
            -
            z \mI_{p-k}
        \right)^{-1}
        \mLambda_n^{1/2}
        \mE_{n,21}
            \toas
                0.
    \end{equation}
    \end{subequations}
\end{lemma}
\begin{proof}
We prove the result for the first quantity and the other derivations are analogous.  For each $1 \leq i, j \leq k$, we have that
\begin{align*}
    \left(
        \frac{1}{n}
        \mE_{n,12}
        \left(
            \mLambda_n
            -
            z \mI_{p-k}
        \right)^{-1}
        \mE_{n,12}^\trans    
    \right)_{ij}
    =
    \frac{p-k}{n}
    \cdot
    \frac{1}{p-k}
    \sum_{\alpha=1}^{p-k}
        \frac{ \left( \mE_{n,12} \right)_{i\alpha}
               \left( \mE_{n,12} \right)_{j\alpha}
             }
             { \lambda_{n,\alpha} - z }.
\end{align*}
Let $N = p-k$, define weights 
\(
    W_{N,\alpha} = (\lambda_{n,\alpha} - z)^{-1},
\)
and let
\(
    Y_{N,\alpha} = \left( \mE_{n,12} \right)_{i\alpha}
                   \left( \mE_{n,12} \right)_{j\alpha}.
\)
The function
\[
    g(t) 
    = 
    \begin{cases}
        \frac{1}{t - z} 
            &\text{if $t \leq b_\gamma$,} \\
        \frac{1}{b_\gamma - z}
            &\text{otherwise,}
    \end{cases}
\]
is bounded and continuous.  Moreover, since $\lambda_{n,1} \toas b_\gamma$, with probability one $g(\lambda_{n,\alpha})$ is eventually equal to $W_{N,\alpha}$ for all $\alpha$.  The Wishart LLN (Corollary~\ref{C:wishart-lln}) gives us that
\(
    \frac{1}{N}
    \sum_{\alpha=1}^N W_{N,\alpha}
        \toas
            \int
                \frac{1}{t - z} d\FMP_\gamma(t) = m(z).
\)
Since $| W_{N,\alpha} | \leq W_{N,1} \overset{\text{a.s.}}{\leq} b_\gamma$, the fourth moments of the weights are uniformly bounded in $N$.  
The $Y_{N,\alpha}$ are all iid with $\E Y_{N,\alpha} = \delta_{ij}$ and $\E Y_{N,\alpha}^4 < \infty$.  
Applying these results, the weighed SLLN (Proposition~\ref{P:weighted-slln}) gives us that
\(
    \frac{1}{N}
    \sum_{\alpha=1}^N
        W_{N,\alpha}
        Y_{N,\alpha}
    \toas
        m(z)
        \delta_{ij}.
\)
Since $\frac{p-k}{n} \to \gamma^{-1}$, this completes the proof.
\end{proof}


\begin{lemma}\label{L:eij-product-limits-uniform}
    Considered as functions of $z$, the quantities in 
    Lemma~\ref{L:eij-product-limits} and their derivatives
    converge uniformly over any closed
    interval $[u,v] \subset \big( b_\gamma,\infty \big)$.
\end{lemma}
\begin{proof}
    We show this for the first quantity and the other proofs are similar.
    Defining
    \[
        e_{n,ij}(z)
        =
        \left(
            \frac{1}{n}
            \mE_{n,12}
            \left(
                \mLambda_n - z \mI_{p-k}
            \right)^{-1}
            \mE_{n,12}^\trans
        \right)_{ij},
    \]
    we will show that for all $(i,j)$ with $1 \leq i,j \leq k$, 
    \(
        \sup_{z \in [u,v]} 
            \left|
                e_{n,ij}(z)
                    -
                    \gamma^{-1}
                    m(z)
                    \delta_{ij}
            \right|
            \toas 0.
    \)
    
    Let $\epsilon > 0$ be arbitrary.  Note that for 
    $z > b_\gamma$, $m'(z) > 0$ and $m''(z) < 0$.  We let $d = m'(u)$ and
    choose a grid
    \(
        u = z_1 < z_2 < \cdots < z_{M-1} < z_M = v,
    \)
    with $|z_l - z_{l+1}| = \frac{\gamma \epsilon}{2 d}$.  
    Then $|\gamma^{-1} m(z_l) - \gamma^{-1} m(z_{l+1})| < \frac{\epsilon}{2}$.  
    From 
    Lemma~\ref{L:eij-product-limits}, we can find $N$ large enough such
    that for $n > N$, 
    \(
        \max_{l\in\{1, \ldots, M\}}
            \left|
                e_{n,ij}(z_l)
                    -
                    \gamma^{-1}
                    m(z_l)
                    \delta_{ij}
            \right|
            <
            \frac{\epsilon}{2}.
    \)
    Also guarantee that $N$ is large enough so that $\lambda_{n,1} < u$ 
    (this is possible since $\lambda_{n,1} \toas b_\gamma < u$).
    Let $z \in [u,v]$ be arbitrary and find $l$ such that 
    $z_l \leq z \leq z_{l+1}$.
    Observe that $e_{n,ij}(z)$ is monotone for $z > \lambda_{n,1}$.  Thus,
    either
    \(
        e_{n,ij}(z_l) \leq e_{n,ij}(z) \leq e_{n,ij}(z_{l+1})
    \)
    or
    \(
        e_{n,ij}(z_{l+1}) \leq e_{n,ij}(z) \leq e_{n,ij}(z_{l}).
    \)
    
    If $i \neq j$, we have for $n > N$,
    \(
        -\frac{\epsilon}{2} < e_{n,ij}(z) < \frac{\epsilon}{2}.
    \)
    Otherwise, when $i = j$, $e_{n,ij}(z)$ is 
    monotonically increasing and
    \(
        \gamma^{-1} m(z_l) - \frac{\epsilon}{2}
            <
                e_{n,ij}(z)
                    <
                        \gamma^{-1} m(z_{l+1}) + \frac{\epsilon}{2},
    \)
    so that
    \(
        \gamma^{-1} m(z) - \epsilon
            <
                e_{n,ij}(z)
                    <
                        \gamma^{-1} m(z) + \epsilon.
    \)
    In either case, 
    $|e_{n,ij}(z) - \gamma^{-1} m(z) \delta_{ij}| < \epsilon$.
    Since 
    \(
        \frac{d}{dz} \left[
            \frac{1}{\lambda - z}
        \right]
            =
            -\frac{1}{(\lambda - z)^{2}},
    \)
    which is monotone for $z > \lambda$, the same argument applies to show
    that the derivatives converge uniformly.
\end{proof}


\begin{lemma}\label{L:eij-product-scaled-limits}
    If $z_1, z_2, \ldots, z_l > b_\gamma$, 
    then jointly for $z \in \{ z_1, z_2, \ldots, z_l \}$
    \begin{subequations}
    \begin{align}
        \sqrt{n}
        \left(
            \frac{1}{n}
            \mE_{n,12}
            \left(
                \mLambda_n
                -
                z
                \mI_{p-k}
            \right)^{-1}
            \mE_{n,12}^\trans
            -
            \gamma^{-1}
            m(z)
            \mI_k
        \right) 
            &\define \mF_n(z)
            \tod \mF(z), \\
        \sqrt{n}
        \left(
            \frac{1}{n}
            \mE_{n,12}
            \left(
                \mLambda_n
                -
                z
                \mI_{p-k}
            \right)^{-1}
            \mLambda_{n}^{1/2}
            \mE_{n,21}
        \right) 
            &\define \mG_n(z)
            \tod \mG(z), \\
        \sqrt{n}
        \left(
            \frac{1}{n}
            \mE_{n,21}^\trans
            \left(
                \mLambda_n
                -
                z
                \mI_{p-k}
            \right)^{-1}
            \mE_{n,21}
            -
            \gamma^{-1}
            m(z)
            \mI_k
        \right) 
            &\define \mH_n(z)
            \tod \mH(z),
    \end{align}
    \end{subequations}
    where the elements of $\mF(z)$, $\mG(z)$, and $\mH(z)$
    jointly define a multivariate Gaussian process indexed by $z$,
    with each matrix independent of the others. 
    The other covariances are defined by
    \begin{subequations}
    \begin{align}
        \cov\Big( F_{ij}(z_1), \, F_{ij}(z_2) \Big)
            &= \gamma^{-1}
               \left( 1 + \delta_{ij} \right)
               \frac{ m(z_1) - m(z_2) }
                    { z_1 - z_2 }, \\
        \cov\Big( G_{ij}(z_1), \, G_{ij}(z_2) \Big)
            &= \gamma^{-1} \,
               \frac{ z_1 \, m(z_1) - z_2 \, m(z_2) }
                    { z_1 - z_2 }, \\
        \cov\Big( H_{ij}(z_1), \, H_{ij}(z_2) \Big)
            &= \gamma^{-1}
               \left( 1 + \delta_{ij} \right)
               \frac{ m(z_1) - m(z_2) }
                    { z_1 - z_2 },
    \end{align}
    \end{subequations}
    with the interpretation when $z_1 = z_2$ that
    \[
        \frac{ m(z_1) - m(z_2) }
             { z_1 - z_2 }
            =
                m'(z_1),
        \quad\text{and}\quad
        \frac{ z_1 \, m(z_1) - z_2 \, m(z_2) }
             { z_1 - z_2 }
            = m( z_1 ) + z_1 \, m'(z_1).
    \]
\end{lemma}
\begin{proof}
    We will need the Wishart CLT~(Theorem~\ref{T:wishart-clt}) and the strong 
    weighted multivariate CLT~(Corollary~\ref{C:strong-weighted-clt}).  To 
    save space we only
    give the argument for the joint distribution of $\mF(z_1)$ and 
    $\mF(z_2)$.
    
    Put $N = p-k$ and consider the $2 k^2$-dimensional vector
    \(
        \vY_{N,\alpha}
        =
        \left(
        \begin{matrix}
            \vtY_{N,\alpha} \\
            \vtY_{N,\alpha}
        \end{matrix}
        \right),
    \)
    where
    \(
        \vtY_{N,\alpha}
        =
        \vecm \left(
            \vE_{n,12,\alpha} \, \vE_{n,12,\alpha}^\trans
        \right)
    \)
    and
    \(
        \mE_{n,12}
        =
        \left(
        \begin{matrix}
            \vE_{n,12,1} &
            \vE_{n,12,2} &
            \cdots &
            \vE_{n,12,N} &            
        \end{matrix}
        \right).
    \)
    Define the $2 k^2$-dimensional weight vector
    \(
        \vW_{N,\alpha}
        =
        \left(
        \begin{matrix}
            \vW_{N,\alpha,1} \\
            \vW_{N,\alpha,2}
        \end{matrix}
        \right),
    \)
    where
    \(
        \vW_{N,\alpha,i}
        =
        \frac{1}{\lambda_\alpha - z_i}
        \vone.
    \)
    We have that for $\alpha = 1, 2, \ldots, N$, the $\vY_{N,\alpha}$ are
    iid with
    \[
        \E \left[ \vY_{N,1} \right]
        =
        \vmu^Y
        =
        \left(
        \begin{matrix}
            \vtmu^Y \\
            \vtmu^Y
        \end{matrix}
        \right)
    \]
    and $\vtmu^Y = \vecm\left( \mI_k \right)$.  Also, we have
    \[
        \E \left[ 
            \left(
                \vY_{N,1}
                -
                \vmu^Y
            \right)
            \left(
                \vY_{N,1}
                -
                \vmu^Y
            \right)^\trans
        \right]
        =
        \mSigma^Y
        =
        \left(
        \begin{matrix}
            \mtSigma^Y & \mtSigma^Y \\
            \mtSigma^Y & \mtSigma^Y
        \end{matrix}
        \right),
    \]
    where
    \(
        \mtSigma^Y
        =
        \E \Big[
            \Big( \!
                \vecm\left( 
                    \vE_{n,12,1} \vE_{n,12,1}^\trans - \mI_k
                \right) \! \!
            \Big)
            \Big( \!
                \vecm\left( 
                    \vE_{n,12,1} \vE_{n,12,1}^\trans - \mI_k
                \right) \! \!
            \Big)^{\! \trans}
        \Big]
    \)
    is a $k^2 \times k^2$ matrix defined by the relation
    \[
        \E \left[
            \left( 
                \vE_{n,12,1} \vE_{n,12,1}^\trans - \mI_k
            \right)_{ij}
            \left( 
                \vE_{n,12,1} \vE_{n,12,1}^\trans - \mI_k
            \right)_{i'j'}
        \right]
        =
        \delta_{(i,j) = (i',j')}
        +
        \delta_{(i,j) = (j',i')}.
    \]
    That is, the diagonal elements of 
    $\vE_{n,12,1} \vE_{n,12,1}^\trans - \mI_k$ have variance $2$ and
    the off-diagonal elements have variance $1$.  Aside from the matrix
    being symmetric, the unique elements are all uncorrelated.
    
    Letting
    \begin{align*}
        \mu^W_i
            &=
                \int \frac{ d\FMP_\gamma(t) }
                          { t - z_i }
            =
                m( z_i ), \qquad\text{and} \\
        \sigma^W_{ij}
            &=
                \int \frac{ d\FMP_\gamma(t) }
                          { (t - z_i)(t - z_j) }
            =
                \frac{ m( z_i ) - m( z_j ) }
                     { z_i - z_j },
    \end{align*}
    the Wishart LLN combined with the truncation argument of
    Lemma~\ref{L:eij-product-limits} gives us that
    \(
        \frac{1}{N}
        \sum_{\alpha=1}^N
            \frac{1}{  \lambda_{n,\alpha} - z_i  }
            \toas
                \mu^W_{i}
    \)
    and
    \(
        \frac{1}{N}
        \sum_{\alpha=1}^N
            \frac{1}{ ( \lambda_{n,\alpha} - z_i )
                      ( \lambda_{n,\alpha} - z_j ) }
            \toas
                \sigma^W_{ij}.
    \)
    Thus,
    \begin{align*}
        \frac{1}{N}
        \sum_{\alpha=1}^N
            \vW_{N,\alpha}
            &\toas
                \vmu^W =
                \left(
                \begin{matrix}
                    \mu^W_1 \vone \\
                    \mu^W_2 \vone
                \end{matrix}
                \right), \qquad\text{and} \\
        \frac{1}{N}
        \sum_{\alpha=1}^N
            \vW_{N,\alpha} \,
            \vW_{N,\alpha}^\trans
            &\toas
                \mSigma^W
            =
                \left(
                \begin{matrix}
                    \sigma^W_{11} \vone \, \vone^\trans &
                        \sigma^W_{12} \vone \, \vone^\trans \\
                    \sigma^W_{21} \vone \, \vone^\trans &
                        \sigma^W_{11} \vone \, \vone^\trans
                \end{matrix}
                \right).
    \end{align*}
    Moreover, the Wishart CLT tells us that the error in each of the
    sums is of size $\OhP\left( \frac{1}{N} \right)$.  
        
    As in Lemma~\ref{L:eij-product-limits}, the fourth moments of
    $\vW_{N,\alpha}$ and $\vY_{N,\alpha}$ are all well-behaved.  Finally,
    we can invoke the strong weighted CLT 
    (Corollary~\ref{C:strong-weighted-clt}) to get that the weighted sum
    \(
        \sqrt{N} \left(
            \frac{1}{N}
            \sum_{\alpha=1}^N
                \vW_{N,\alpha} \bullet \vY_{N,\alpha}
            -
                \vmu^W \bullet \vmu^T
        \right)
    \)
    converges in distribution to a mean-zero multivariate normal with
    covariance
    \[
        \mSigma^W \bullet \mSigma^Y
            =
                \left(
                \begin{matrix}
                    \sigma^W_{11} \mtSigma &
                        \sigma^W_{12} \mtSigma \\
                    \sigma^W_{21} \mtSigma &
                        \sigma^W_{22} \mtSigma 
                \end{matrix}
                \right).
    \]
    This completes the proof since
    \begin{multline*}
        \sqrt{n}
        \vecm \left(
            \frac{1}{n}
            \mE_{n,12}
            \left(
                \mLambda_n
                -
                z_i
                \mI_{p-k}
            \right)^{-1}
            \mE_{n,12}^\trans
            -
            \gamma^{-1}
            m( \mu )
            \mI_k
        \right) \\
            =
                \gamma^{-1}
                \sqrt{ \frac{n}{p-k} }
                \cdot
                \sqrt{ N }
                \Bigg( 
                    \frac{1}{N}
                    \sum_{\alpha=1}^N
                        \vW_{N,\alpha,i} \bullet \vtY_{N,\alpha}
                    -
                        \vmu_i^W \bullet \vtmu_i^Y
                \Bigg)
    \end{multline*}
    (with $\vmu_i^W = \mu_i^W \vone$),
    and
    \(
        \gamma^{-1}
        \sqrt{ \frac{n}{p-k} }
        \to
        \gamma^{-1/2}.
    \)
\end{proof}

\begin{remark}
    It is possible to show that the sequences in Lemma~\ref{L:eij-product-scaled-limits} are tight by an argument similar to the one used in \cite{onatski2009}.  This implies that the convergence is uniform in $z$.  For our purposes, we only need that the finite-dimensional distributions converge.
\end{remark}

With Lemmas~\ref{L:eij-product-limits}--\ref{L:eij-product-scaled-limits},
we can get a perturbation expansion of $\mT_{n}(z)$ for $z > b_\gamma$.

\begin{lemma}
    If $[u,v] \subset \big( b_\gamma, \, \infty \big)$, then
    \(
        \mT_n( z )
            \toas
                \mT_0( z )
    \)
    uniformly on $[u,v]$, where
    \begin{equation}
        \mT_0( z )
            =
                \frac{1}{ 1 + \gamma^{-1} m(z) }
                \mD^2 
                +
                \frac{1}{m(z)}
                \mI_k.
    \end{equation}
\end{lemma}

\begin{lemma}
    If $z > b_\gamma$, then
    \begin{equation}
        \mT_n( z ) 
            = 
                \mT_0( z )
                +
                \frac{1}{\sqrt{n}}
                \mT_{n,1}( z ),
    \end{equation}
    where
    \begin{align}
            \mT_{n,1}( z )
                = 
                &-
                \big( 1 + \gamma^{-1} m(z) \big)^{-2}
                    \cdot
                    \mD \mF_n(z) \mD \notag \\
                &+
                \big( 1 + \gamma^{-1} m(z) \big)^{-1} \notag \\
                    &\quad\quad\quad\cdot
                    \Big\{
                        \sqrt{n} \left( \mD_n^2 - \mD^2 \right)
                        + 
                        \mD \big( \mE_{n,11} - \mG_n(z) \big)
                        + 
                        \big( \mE_{n,11} - \mG_n(z) \big)^\trans \mD
                    \Big\}  \notag \\
                &-
                z \, \mH_n(z)
                +
                \sqrt{n}
                \left(
                    \frac{1}{n} \mE_{n,31}^\trans \mE_{n,31}
                    -
                    (1 - \gamma^{-1})
                    \mI_k
                \right)
                +
                \ohP\left( 1 \right).
    \end{align}
\end{lemma}
\begin{proof}
    First, we have
    \begin{multline*}
        \mS_{n,11}
            = 
                \mD_n^2 + \mI_k
                +
                \frac{1}{\sqrt{n}}
                \left(
                    \mD \mE_{n,11}
                    +
                    \mE_{n,11}^\trans \mD
                \right) \\
                +
                \left(
                    \frac{1}{n}
                    \big(
                        \mE_{n,11}^\trans \mE_{n,11}
                        +
                        \mE_{n,21}^\trans \mE_{n,21}
                        +
                        \mE_{n,31}^\trans \mE_{n,31}
                    \big)
                    -
                    \mI_k
                \right).
    \end{multline*}
    Next, we compute
    \begin{align*}
        \begin{split}
        \Bigg(
            \mI_k
            +
            \frac{1}{n}
            \mE_{n,12}
            &\Big(
                \mLambda_n - z \mI_{p-k}
            \Big)^{-1}
            \mE_{n,12}^\trans
        \Bigg)^{-1} \\
            &=
                \left(
                    \mI_k
                    +
                    \gamma^{-1}
                    m( z )
                    \mI_k
                    +
                    \frac{1}{\sqrt{n}}
                    \mF_n(z)
                \right)^{-1} 
        \end{split} \\
%            &=
%                \big(1 + \gamma^{-1} m(z)\big)^{-1}
%                \left(
%                    \mI_k
%                    +
%                    \frac{1}{\sqrt{n}}
%                    \big( 1 + \gamma^{-1} m(z) \big)^{-1}
%                    \mF_{n}(z)
%                \right)^{-1} \\
            &=
                \big( 1 + \gamma^{-1} m(z) \big)^{-1} \mI_k
                -
                \frac{1}{\sqrt{n}}
                \big( 1 + \gamma^{-1} m(z) \big)^{-2}
                \mF_{n}(z)
                +
                \ohP\left( \frac{1}{\sqrt{n}} \right).              
    \end{align*}
    Using this, after a substantial amount of algebra we get
    \[
        \begin{split}
            \mS_{n,12} 
            &\Big( \mS_{n,22} - z \mI_{p-k} \Big)^{-1} 
            \mS_{n,21} \\
                &=
                z \, m(z) \, \mI_k
                +
                \frac{\gamma^{-1} m(z)}
                     { 1 + \gamma^{-1} m(z)} \mD_n^2 \\
                &\quad+
                \frac{1}{\sqrt{n}} \Bigg\{
                    \frac{1}{1 + \gamma^{-1} m(z)}
                        \left( 
                            \mD \mG_n(z) 
                            + \mG_n^\trans(z) \mD 
                        \right) \\
                    &\quad\qquad\qquad+
                    \frac{\gamma^{-1} m(z)}{1 + \gamma^{-1} m(z)}
                    \left( \mD \mE_{n,11} + \mE_{n,11}^\trans \mD \right) \\
                    &\quad\qquad\qquad+
                    \left(
                        \frac{1}{1 + \gamma^{-1} m(z)}
                    \right)^2
                    \mD \mF_n(z) \mD
                    +
                    z \, \mH_n( z ) \Bigg\} \\
                &\quad+
                \frac{1}{n} \mE_{n,21}^\trans \mE_{n,21}
                +
                \ohP\left( \frac{1}{\sqrt{n}} \right).
        \end{split}
    \]
    The equations for $\mT_0$ and $\mT_{n,1}$ follow.  
    To simplify the form of $\mT_0$, we use the identity
    \(
        z
        \cdot
        \Big(
            1
            +
            \gamma^{-1} m(z)
        \Big)
        =
        -
        \frac{1}{m(z)}
        +
        (1 - \gamma^{-1}).
    \)
\end{proof}


\section{Solutions to the secular equation}\label{S:solutions-to-secular-eq}

We will now study the solutions to $f_{n}(z, \vx) = 0$, defined in 
equation~\eqref{E:secular-f}.  If $(z, \vx)$
is a solution, then so is $(z, \alpha \, \vx)$ for any scalar $\alpha$.  We restrict our attention to solutions with $\|\vx \|_2 = 1$.  We also impose a restriction on the sign of $\vx$, namely we require that the component with the largest magnitude is positive, i.e.
\begin{equation}\label{E:x-identifiability}
    \max_i x_i = \max_i | x_i |.
\end{equation}

\subsection{Almost sure limits}

First we look at the solutions of $f_0(z, \vx) \define \mT_0(z) \vx$, the limit of $f_n(z, \vx)$ for $z > b_\gamma$.  

\begin{lemma}
    Letting $\bar k = \max \{ i : \mu_i > \gamma^{-1/2} \}$, if $\mu_1, \mu_2, 
    \ldots, \mu_k$ are all distinct, then there are exactly
    $\bar k$ solutions to the equation $f_0( z, \vx ) = 0$.  They are given
    by
    \[
        ( \bmu_i, \ve_i ), \quad \text{$i = 1, \ldots, \bar k$},
    \]
    where $\bmu_i$ is the unique solution
    \[
        m( \bmu_i ) = \frac{1}{\mu_i + \gamma^{-1}},
    \]
    and $\ve_i$ is the $i$th standard basis vector.
\end{lemma}
\begin{proof}
    We have that 
    \[
        \mT_0(z) 
        = 
        \frac{1}{1 + \gamma^{-1} m(z)} \mD^2 
        + 
        \frac{1}{m(z)} \mI_k.
    \]
    Since this is diagonal, the equation $f_0(z, \vx) = 0$ holds iff the 
    $i$th diagonal element of $\mT_0(z)$ is zero and $\vx = \ve_i$.  The 
    $i$th diagonal is zero when 
    \[
        \frac{\mu_i}{1 + \gamma^{-1} m(z)} + \frac{1}{m(z)} = 0,
    \]
    equivalently
    \[
        m(z) = - \frac{1}{\mu_i + \gamma^{-1}}.
    \]
    Note that $m(z) > - \left( \gamma^{-1/2} + \gamma^{-1} \right)^{-1}$ and
    $m'(z) > 0$ on $\big( b_\gamma, \infty \big)$.  Hence, a unique solution
    exists exactly when $\mu_i > \gamma^{-1/2}$.
\end{proof}

Given a solution of $f_0(z, \vx) = 0$, it is not hard to believe that there is a sequence of solutions $(z_n, \vx_n)$ such that $f_n(z_n, \vx_n) = 0$, with $z_n$ and $\vx_n$ converging to $z$ and $\vx$, respectively.  We dedicate the rest of this section to making this statement precise.  

\begin{lemma}\label{L:zn-sequence-exists}
    If $\mu > \gamma^{-1/2}$ occurs $l$ times on the diagonal of $\mD^2$, then
    with probability one there exist sequences $z_{n,1}, 
    \ldots, z_{n,l}$ such that for $n$ large enough:
    \begin{enumerate}
        \item $z_{n,i} \neq z_{n,j}$ for $i \neq j$
        \item $\det \mT_n(z_{n,i}) = 0$ for $i = 1, \ldots, l$.
        \item $z_{n,i} \to 
                z_0 
                = 
                m^{-1} \left( \frac{1}{\mu + \gamma^{-1}} \right)$.
    \end{enumerate}
\end{lemma}
\noindent
The proof involves a technical lemma, which we state and prove now.

\begin{lemma}\label{L:uniform-converge-same-roots}
    Let $g_n(z)$ be a sequence of continuous real-valued functions that 
    converge uniformly on $(u,v)$ to $g_0(z)$.  If $g_0(z)$ is analytic
    on $(u,v)$, then for $n$ large enough, $g_n(z)$ and $g_0(z)$ have the
    same number of zeros in $(u,v)$ (counting multiplicity).
\end{lemma}
\begin{proof}
    Since $|g_0(z)|$ is bounded away from zero outside the neighborhoods
    of its zeros, we can assume without loss of generality that $g_0(z)$ has
    a single zero $z_0 \in (u,v)$ of multiplicity $l$.  Define
    \(
        \tg_n(z) = \frac{g_n(z)}{|z - z_0|^{l-1}}.
    \)
    The function $\tg_0(z)$ is bounded and continuous, and has a 
    simple zero at $z_0$.  For $r$ small enough, $\tg_0(z_0 + r)$ and
    $\tg_0(z_0 - r)$ have differing signs.  Without loss of generality,
    say that the first is positive.  
    
    The sequence $\tg_n(z)$ converges uniformly to $\tg_0(z)$ outside of
    a neighborhood of $z_0$.  Thus, for $n$ large enough, $\tg_n(z-r) < 0$
    and $\tg_n(z+r) > 0$.  Also, either $g_n(z)$ has a zero at $z_0$, or else 
    for a small enough neighborhood around $z_0$, $\sgn \tg_n(z)$ is constant.
    Since $\tg_n(z)$ is continuous outside of a neighborhood of $z_0$, 
    $\tg_n(z)$ and $g_n(z)$ must have a zero
    in $(z_0-r,z_0+r) \subset (u,v)$.  Call this zero $z_{n,1}$.  Since
    $r$ is arbitrary, $z_{n,1} \to z_0$, and
    $\frac{g_n(z)}{z - z_{n,1}} \to \frac{g_0(z)}{z - z_0}$ uniformly on
    $(u,v)$.  We can now proceed inductively, since $\frac{g_0(z)}{z - z_0}$
    has a zero of multiplicity $l-1$ at $z_0$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{L:zn-sequence-exists}]
    Let $z_0$ be the unique solution of $m(z_0) = (\mu + \gamma^{-1})^{-1}$.
    Define $g_n(z) = \det \mT_n(z)$.  Since $\det$ is a continuous function, 
    $g_n(z)$ converges uniformly to $g_0(z)$ in 
    any neighborhood of $z_0$.  Noting that $g_0(z)$ has a zero of 
    multiplicity 
    $l$ at $z_0$, by Lemma~\ref{L:uniform-converge-same-roots} we get that for large enough $n$, $g_n(z)$ has $l$ zeros
    in a neighborhood of $z_0$.  By a lemma of Okamoto~\cite{okamoto1973deq}, 
    the zeros of $g_n(z)$ are almost surely simple.
\end{proof}

The last thing we need to do is show that for each sequence
$z_{n,i}$ solving the equation $\det \mT_n(z_{n,i}) = 0$, there is a corresponding sequence
of vectors $\vx_{n,i}$ with $f_n(z_{n,i}, \vx_{n,i}) = 0$.  Since $\det \mT_n (z_{n,i}) = 0$, there exists an $\vx_{n,i}$ with 
$\mT_{n} (z_{n,i}) \, \vx_{n,i} = 0$.  We need to show that the sequence of vectors has a limit.

Every solution pair $(z_{n,i}, \vx_{n,i})$ determines a unique
eigenvalue-eigenvector pair through equation~\eqref{E:v-from-v1}.  Since the eigenvalues of $\mS_n$ are almost surely unique, with
the identifiability restriction of \eqref{E:x-identifiability} we must have that $z_{n,i}$ uniquely determines $\vx_{n,i}$.  Suppose that $z_{n,i}$ is the $i$th largest solution of $\det \mT_n(z) = 0$, and that $f_n( z_{n,i}, \vx_{n,i}) = 0$.  We will now show that $\vx_{n,i} \toas \ve_i$.

\begin{lemma}\label{L:xn-converges}
    Suppose that $f_n(z_{n,i}, \vx_{n,i}) = 0$, that $\vx_{n,i}$ satisfies the 
    identifiability restriction \eqref{E:x-identifiability}, and that
    $z_{n,i} \toas \bmu_i$.
    If $\mu_i \neq \mu_j$ for all $j \neq i$, then $\vx_{n,i} \toas \ve_i$.
\end{lemma}

We will use a perturbation lemma, which follows from the $\sin \Theta$ theorem (see Stewart and Sun~\cite{stewart1990mpt}[p. 258]).

\begin{lemma}\label{L:approx-eigenpair}
    Let $(z,\vx)$ be an approximate eigenpair of the $k\times k$
    matrix $\mA$ (in the sense that $\mA \vx \approx z \vx$), with 
    $\| \vx \|_2 = 1$.  Let $\vr = \mA \vx - z \vx$.
    Suppose that there is a set $\mathcal{L}$ of $k-1$ eigenvalues of $\mA$ 
    such that
    \[
        \delta = \min_{l \in \mathcal{L}} | l - z | > 0.
    \]
    Then there is an eigenpair $(z_0, \vx_0)$ of $\mA$ with 
    $\| \vx_0 \|_2 = 1$ satisfying
    \[
        \vx^\trans \vx_0
            \geq
                \sqrt{ 1 - \frac{\|\vr\|_2^2}{ \delta^2 } }
    \]
    and
    \[
        | z - z_0 |
            \leq \| \vr \|_2.
    \]
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{L:xn-converges}]
    We have that $z_{n,i} \toas \bmu_i$ and 
    $\mT_n(z_{n,i}) \vx_{n,i} = 0$.  Since
    $\mT_n(z_{n,i}) \toas \mT_0(\bmu_i)$, we get
    \begin{align*}
        \| \mT_0(\bmu_i) \vx_{n,i} \|_2
            &= \| \big(\mT_n(z_{n,i} - \mT_0(\bmu_i) \big) \vx_{n,i} \|_2 \\
            &\leq \| \mT_n(z_{n,i} - \mT_0(\bmu_i) \|_F \\
            &\toas 0.
    \end{align*}
    Since $\mu_i$ is distinct, $0$ is a simple eigenvalue of $\mT_0(\bmu_i)$.
    Thus, all other eigenvalues have magnitude at least $\delta > 0$ for some 
    $\delta$.
    Define $\vr_n = \mT_0(\bmu_i) \vx_{n,i}$.  By 
    Lemma~\ref{L:approx-eigenpair}, there exists an eigenpair $(z_0, \vx_0)$
    of $\mT_0(\bmu_i)$ with $| z_0 | \leq \| \vr_n \|_2$ and
    $ \vx_{n,i}^\trans \, \vx_0 \geq
        \sqrt{ 1 - \frac{\|\vr\|_2^2}{ \delta^2 } }$.  Since 
    $\|\vr_n\| \toas 0$, for $n$ large enough we must have $z_0 = 0$ and
    $\vx_0 = \ve_i$ or $-\ve_i$.  Lastly, noting that $\vx_0$ and $\vx_{n,i}$
    are both unit vectors, we get
    \begin{align*}
        \| \vx_{n,i} - \vx_0 \|_2^2
            &= 2 - 2 \vx_{n,i}^\trans \, \vx_0 \\
            &\toas 0.
    \end{align*}
    With the identifiability restriction, this forces $\vx_{n,i} \toas \ve_i$.
\end{proof}

Finally, we show that eventually the points described in Lemma~\ref{L:zn-sequence-exists} are the only zeros of $f_n(z, \vx)$ having
$z > b_\gamma$.  Since $f_n(z, \vx) = 0$ implies 
$\det \mT_n(z) = 0$, it suffices to show that $\mT_n(z)$ has no other
zeros.

\begin{lemma}\label{L:detT-solution-upper-bound}
    For $n$ large enough, almost surely
    the equation $\det \mT_n(z) = 0$ has exactly 
    $\bar k$ solutions in  $\big( b_\gamma, \infty \big)$
    (namely, the $\bar k$  points described in 
    Lemma~\ref{L:zn-sequence-exists}).
\end{lemma}
\begin{proof}
    By Lemma~\ref{L:uniform-converge-same-roots}, for $n$ large enough
    $\det \mT_n(z)$ and $\det \mT_0(z)$ have the same number of solutions in 
    $(u,v) \subset \big( b_\gamma, \infty \big)$.  Thus, we only need to
    show that the solutions of $\det \mT_n(z)$ are bounded.  Since every
    solution is an eigenvalue of $\mS_n$, this amounts to showing that the
    eigenvalues of $\mS_n$ are bounded.  Using the Courant-Fischer 
    min-max characterization of eigenvalues \cite{golub1996mc}, we have
    \begin{align*}
        \| \mS_n \|_2
            &= \frac{1}{\sqrt{n}} \| \mX_n \|_2^2 \\
            &\leq 
                \left(
                    \| \mU_n \mD_n \mV_n^\trans \|_2
                    +
                    \frac{1}{\sqrt{n}}
                    \| \mE_n \|_2
                \right)^2 \\
            &\toas
                \left( \sqrt{\mu_1} + \sqrt{b_\gamma} \right)^2.
    \end{align*}
    Thus, the solutions of $\det \mT_n(z) = 0$ are almost surely bounded.
\end{proof}


\subsection{Second-order behavior}

To find the second-order behavior of the solutions to the secular equation, we use a Taylor-series expansion of $f_n(z,\vx)$ around the limit points.  That is, if $(z_n, \vx_n) \to (z_0, \vx_0)$, we let $D f_n$ be the derivative of $f_n$ and write
\[
    0
    =
    f_n (z_n, \vx_n)
        \approx
        f_n( z_0, \vx_0 )
        + 
        D f_n(z_0, \vx_0)
        \left(
        \begin{matrix}
            z_n   - z_0 \\
            \vx_n - \vx_0
        \end{matrix}
        \right).
\]
We now want to solve for $z_n - z_0$ and $\vx_n - \vx_0$.  Without the identifiability constraint, there are $k$ equations and $k+1$ unknowns, but as soon as we impose the condition $\| \vx_n \|_2 = \| \vx_0 \|_2 = 1$, the
system becomes well-determined.

To make this precise, we first compute
\begin{equation}
    D f_n(z, \vx)
    =
    \left(
    \begin{matrix}
        \mT_0'(z) \vx & \mT_0(z)
    \end{matrix}
    \right)
    +
    \OhP\left( \frac{1}{\sqrt{n}} \right),
\end{equation}
with pointwise convergence in $z$.  Then, we write
\[
    f_n( z_n, \vx_n)
        = 
        f_n (z_0, \vx_0)
        +
        D f_n( z_0, \vx_0 )
        \left(
        \begin{matrix}
            z_n - z_0 \\
            \vx_n - \vx_0
        \end{matrix}
        \right)
        +
        \OhP \left(
            (z_n - z_0)^2
            +
            \| \vx_n - \vx_0 \|_2^2
        \right).
\]
If $f_n(z_n, \vx_n) = 0$ and $f_0(z_0, \vx_0) = 0$, then we get
\[
    0
    =
    \frac{1}{\sqrt{n}} f_{n,1} (z_0, \vx_0)
        +
        D f_n( z_0, \vx_0 )
        \left(
        \begin{matrix}
            z_n - z_0 \\
            \vx_n - \vx_0
        \end{matrix}
        \right)
        +
        \OhP \left(
            (z_n - z_0)^2
            +
            \| \vx_n - \vx_0 \|_2^2
        \right).
\]
If $(z_n, \vx_n) \toas (z_0, \vx_0)$, then 
the differences $z_n - z_0$ and $\vx_n - \vx_0$ must be of size
$\OhP\left( \frac{1}{\sqrt{n}} \right)$ and the error term in the Taylor
expanson is size $\OhP\left( \frac{1}{n} \right)$.  The final simplification
we can make is from the length constraint on $\vx_n$ and $\vx_0$.  We have
\begin{align*}
    1 &= \vx_n^\trans \vx_n \\
      &= \big( \vx_0 + (\vx_n - \vx_0) \big)^\trans
         \big( \vx_0 + (\vx_n - \vx_0) \big) \\
      &= 1 + 2 \vx_0^\trans ( \vx_n - \vx_0 ) + \| \vx_n - \vx_0 \|_2^2,
\end{align*}
so that $\vx_0^\trans ( \vx_n - \vx_0 ) = \OhP\left( \frac{1}{n} \right)$.  With a little more effort, we can solve for $z_n - z_0$ and $\vx_n - \vx_0$.

\begin{lemma}
    If $(z_n, \vx_n)$ is a sequence converging almost surely to 
    $(\bmu_i, \ve_i)$, such that $f_n(z_n, \vx_n) = 0$ and $\mu_i \neq \mu_j$
    for $i \neq j$, then:
    \begin{enumerate}[(i)]
        \item \begin{equation}
                  \sqrt{n} ( z_n - \bmu_i) 
                  = 
                  -
                  \frac{\big(\mT_{n,1}(\bmu_i) \big)_{ii}}
                       {\big(\mT_0'(\bmu_i) \big)_{ii}}
                  +
                  \ohP(1),
              \end{equation}
        \item \begin{equation}
                   \sqrt{n} \left( x_{n,i} - 1 \right)
                   =
                   \ohP\left( 1 \right), \quad \text{and}
              \end{equation}
        \item \begin{equation}
                  \sqrt{n} \, x_{n,j}
                  = 
                  -
                  \frac{\big(\mT_{n,1}(\bmu_i) \big)_{ji}}
                       {\big(\mT_0(\bmu_i) \big)_{jj}}
                  +
                  \ohP(1)
                  \quad
                  \text{for $i \neq j$.}
              \end{equation}
    \end{enumerate}
    \begin{proof}
        We have done most of the work in the exposition above.  In particular,
        we already know that ($ii$) holds.  Using the Taylor expansion, we 
        have
        \[
            0
            =
            \frac{1}{\sqrt{n}}
            \mT_{n,1}(\bmu_i) \, \ve_i
            +
            \left(
            \begin{matrix}
                \mT_0'(\bmu_i) \, \ve_i & \mT_0(\bmu_i)
            \end{matrix}
            \right)
            \left(
            \begin{matrix}
                z_n - \bmu_i \\
                \vx_n - \ve_i
            \end{matrix}
            \right)
            +
            \ohP\left( \frac{1}{\sqrt{n}} \right).
        \]
        Since $\mT_0$ and $\mT_0'$ are diagonal, using ($ii$) we get
        \begin{align*}
            0
                &=
                \frac{1}{\sqrt{n}}
                \big(\mT_{n,1}(\bmu_i) \big)_{ii}
                +
                \big(\mT_0'(\bmu_i) \big)_{ii} ( z_n - \bmu_i )
                +
                \ohP\left( \frac{1}{\sqrt{n}} \right), \\
            0
                &=
                \frac{1}{\sqrt{n}}            
                \big(\mT_{n,1}(\bmu_i) \big)_{ji}
                +
                \big(\mT_0(\bmu_i) \big)_{\!jj} \,
                x_{n,j} 
                +
                \ohP\left( \frac{1}{\sqrt{n}} \right)
                \qquad \text{for $i \neq j$.}
        \end{align*}
        Therefore, ($i$) and ($iii$) follow.
    \end{proof}
\end{lemma}

At this point, it behooves us to do some simplification.  For $\mu_i, \mu_j > \gamma^{-1/2}$, we have
\[
    m(\bmu_i) = - \frac{1}{\mu_i + \gamma^{-1}}.
\]
Using \eqref{E:stieltjes-inverse}, this implies
\begin{align*}
    \bmu_i
        &= \mu_i + 1 + \gamma^{-1} + \frac{ \gamma^{-1} }{ \mu_i } \\
        &= \Big( \mu_i + 1 \Big)
           \left( \frac{ \mu_i + \gamma^{-1} }{ \mu_i } \right).
\end{align*}
Note that this agrees with the definition of $\bmu_i$ in Theorem~\ref{T:spiked-eigenvalue-limits}.  Now,
\[
    \frac{ m(\bmu_i) - m(\bmu_j) }
         { \bmu_i - \bmu_j }
    =
    \frac{1}{ ( \mu_i + \gamma^{-1} ) ( \mu_j + \gamma^{-1} ) }
    \cdot
    \frac{ \mu_i \mu_j }{ \mu_i \mu_j - \gamma^{-1} },
\]
so that
\[
    m'(\bmu_i)
    =
    \frac{ 1 }{ ( \mu_i + \gamma^{-1} )^2 }
    \cdot
    \frac{ \mu_i^2 }{ \mu_i^2 - \gamma^{-1} }.
\]
Also,
\[
    \frac{ \bmu_i \, m( \bmu_i ) - \bmu_j \, m( \bmu_j ) }
         { \bmu_i - \bmu_j }
        =
            \frac{1}{\mu_i \, \mu_j - \gamma^{-1}}.
\]
We can compute
\begin{align*}
    \big( \mT_0(\bmu_i) \big)_{jj}
        &=
            ( \mu_i - \mu_j )
            \left( \frac{ \mu_i + \gamma^{-1} }{ \mu_i } \right) 
                \qquad\text{ for $j \neq i$,} \\
    \big( \mT_0'(\bmu_i) \big)_{ii}
        &=
            -
            \left(
                \frac{ \mu_i^2 }{ \mu_i^2 - \gamma^{-1} }
            \right)
            \left(
                \frac{\mu_i + \gamma^{-1}}{ \mu_i }
            \right).
\end{align*}
Since
\(
    \big( 1 + \gamma^{-1} m( \bmu_i ) \big)^{-1}
        = \frac{ \mu_i + \gamma^{-1} }{ \mu_i },
\)
we get
\[
    \begin{split}
        \mT_{n,1}(\bmu_i)
           &=
                \left(
                    \frac{1}{\sqrt{n}}
                    \mE_{n,31}^\trans \mE_{n,31}
                    -
                    \sqrt{n}
                    (1 - \gamma^{-1})
                    \mI_k
                \right) \\
            &\quad-
                \left(
                    \frac{\mu_i + \gamma^{-1}}{\mu_i}
                \right)^2
                \mD \mF_n (\bmu_i ) \mD \\
            &
                \begin{split}
                \quad+
                \left(
                    \frac{\mu_i + \gamma^{-1}}{\mu_i}
                \right)
                \Big(
                    \sqrt{n} ( \mD_n^2 - \mD )
                    &+
                    \mD
                    \big( \mE_{n,11} - \mG_{n} (\bmu_i) \big) \\
                    &+
                    \big( \mE_{n,11} - \mG_{n} (\bmu_i) \big)^\trans                    
                    \mD
                \Big) 
                \end{split}\\
            &\quad-
                (\mu_i + 1)
                \left(
                    \frac{\mu_i + \gamma^{-1}}{\mu_i}
                \right)
                \mH_n(\bmu_i).
    \end{split}
\]
The first term converges in distribution to a mean-zero multivariate normal
with variance $2$ along the diagonal and variance $1$ otherwise; the elements
are all uncorrelated except for the obvious symmetry.  Also, we have
\begin{align*}
    \cov\big( F_{ij} ( \bmu_1 ), F_{ij} ( \bmu_2 ) \big)
        &= 
        \gamma^{-1} ( 1 + \delta_{ij} )
        \cdot
        \frac{1}{ ( \mu_1 + \gamma^{-1} ) ( \mu_2 + \gamma^{-1} ) }
        \cdot
        \frac{ \mu_1 \mu_2 }{ \mu_1 \mu_2 - \gamma^{-1} }, \\
    \cov\big( G_{ij} ( \bmu_1 ), G_{ij} ( \bmu_2 ) \big)
        &= 
        \gamma^{-1} 
        \cdot
        \frac{1}{ \mu_1 \mu_2 - \gamma^{-1} }, \\
    \cov\big( H_{ij} ( \bmu_1 ), H_{ij} ( \bmu_2 ) \big)
        &= 
        \gamma^{-1} ( 1 + \delta_{ij} )
        \cdot
        \frac{1}{ ( \mu_1 + \gamma^{-1} ) ( \mu_2 + \gamma^{-1} ) }
        \cdot
        \frac{ \mu_1 \mu_2 }{ \mu_1 \mu_2 - \gamma^{-1} }.
\end{align*}
Therefore, for $j \neq i$ we have variances
\begin{align*}
    \begin{split}
    \var \left(
        \big(
            \mT_{n,1}( \bmu_i ) \,
            \ve_i
        \big)_i
    \right)
        &= 
        \sigma_{ii}
        \cdot
        \left(
            \frac{\mu_i + \gamma^{-1}}{\mu_i}
        \right)^2 \\
        &\qquad+
        2
        \left(
            2 \mu_i + 1 + \gamma^{-1}
        \right)
        \frac{ (\mu_i + \gamma^{-1})^2 }
             { \mu_i^2 - \gamma^{-1} } + \oh(1) \qquad\text{and}
    \end{split} \\
    \var \left(
        \big(
            \mT_{n,1}( \bmu_i ) \,
            \ve_i
        \big)_j
    \right)
        &=
        \left(
            2 \mu_i + 1 + \gamma^{-1}
        \right)
        \frac{ (\mu_i + \gamma^{-1})^2 }
             { \mu_i^2 - \gamma^{-1} } + \oh(1).
\end{align*}
The nontrivial covariances are given by
\begin{align*}
    \cov \left( 
        \big(
            \mT_{n,1}( \bmu_i ) \,
            \ve_i
        \big)_i, \,
        \big(
            \mT_{n,1}( \bmu_j ) \,
            \ve_j
        \big)_j
    \right)
        &=
        \sigma_{ij}
        \cdot
        \frac{\mu_i + \gamma^{-1}}{ \mu_i }
        \cdot
        \frac{\mu_j + \gamma^{-1}}{ \mu_j } + \oh(1) \quad\text{and} \\
    \begin{split}
    \cov \left(
        \big(
            \mT_{n,1}( \bmu_i ) \,
            \ve_i
        \big)_j, \,
        \big(
            \mT_{n,1}( \bmu_j ) \,
            \ve_j
        \big)_i    
    \right)
        &=
        \left(
            \mu_i + \mu_j + 1 + \gamma^{-1}
        \right) \\
        &\qquad \cdot
            \frac{ ( \mu_i + \gamma^{-1} )( \mu_j + \gamma^{-1} ) }
                 { \mu_i \mu_j - \gamma^{-1} }
        + \oh(1).
    \end{split}
\end{align*}
All other covariances between the elements of
$\mT_{n,1}(\bmu_i) \, \ve_i$ and $\mT_{n,1}(\bmu_j) \, \ve_j$ are zero.

\section{Singular values and singular vectors}\label{S:singular-values-and-vectors}

The results about solutions to the secular equation translate directly
to results about the singular values and right singular vectors of
$\frac{1}{\sqrt{n}} \mtX_n$.

\subsection{Singular values}

Every value $z$ with $f_n(z, \vx) = 0$ for some $\vx$ is the square of a
singular value of $\frac{1}{\sqrt{n}} \mtX_n$.  Therefore,  Section~\ref{S:solutions-to-secular-eq} describes the behavior of the top $\bar k$ singular values.  To complete the proof of Theorem~\ref{T:spiked-eigenvalue-limits} for $\gamma \geq 1$, we only need to describe what happens to the singular values corresponding to the indices $i$ with $\mu_i \leq \gamma^{-1/2}$.

\begin{lemma}\label{L:eigenvalue-below-threshold-limit}
    If $\mu_i \leq \gamma^{-1/2}$ then the $i$th eigenvalue of
    $\mS_n$, converges almost surely to $b_\gamma$.
\end{lemma}
\begin{proof}
From Lemma~\ref{L:detT-solution-upper-bound}, we know that for $n$ large
enough and $\epsilon$ small, there are exactly
$\bar k = \max \{ i : \mu_i > \gamma^{-1/2} \}$ eigenvalues of $\mS_n$
in $\big( b_\gamma + \epsilon, \infty)$.  From the eigenvalue interleaving
inequalities, we know that the $i$th eigenvalue of $\mS_n$ is at least as
big as the $i$th eigenvalue of $\mS_{n,22}$.  

Denote by $\hmu_{n,i}$ the $i$th eigenvalue of $\mS_n$, with
$\bar k < i \leq k$.  Then almost surely,
\[
    \lim_{n\to\infty} \lambda_{n,i} 
        \leq
        \liminf_{n\to\infty} \hmu_{n,i}
        \leq
        \limsup_{n\to\infty} \hmu_{n,i}        
        \leq
        b_\gamma + \epsilon.
\]
Since $\lambda_{n,i} \toas b_\gamma$ and $\epsilon$ is arbitrary, this
forces $\hmu_{n,i} \toas b_\gamma$.
\end{proof}

\begin{lemma}
    If $\mu_i \leq \gamma^{-1/2}$, then
    \(
        \sqrt{n} ( \hmu_i - \bmu_i ) \toP 0,
    \)
    where $\hmu_i$ is the $i$th eigenvalue of $\mS_n$.
\end{lemma}
\begin{proof}
We use the same notation as in Lemma~\ref{L:eigenvalue-below-threshold-limit}.
Recall that in the present situation, $\bmu_i = b_\gamma$.
Since $\lambda_{n,i} = b_\gamma + \OhP(n^{-2/3})$, we have that
\[
    \sqrt{n} \big( \lambda_{n,i} - b_\gamma \big) \toP 0.
\]
This means that
\[
    \liminf_{n\to\infty} \sqrt{n} \big( \hmu_{n,i} - b_\gamma \big)
        \geq \ohP(1).
\]
The upper bound is a little more delicate.  By the Courant-Fischer min-max
characterization, $\hmu_i^{1/2}$ is bounded above by $\tmu_i^{1/2}$, the 
$i$th singular value of
\[
    \frac{1}{\sqrt{n}} \mX_n 
    + 
    \sqrt{n} \big( (\gamma^{-1/2} + \epsilon)^{1/2} - \mu_i^{1/2} \big)
    \vu_{n,i} \vv_{n,i}^\trans
\]
for any $\epsilon > 0$.  From the work in  Section~\ref{S:solutions-to-secular-eq}, we know that
\begin{align*}
    \tmu_i
        &=
            \left(
                1 + \gamma^{-1/2} + \epsilon
            \right)
            \left(
                1
                +
                \frac{ 1 }
                     { \gamma^{1/2} + \gamma \epsilon }
            \right)
            +
            \OhP \left(
                \frac{\tsigma_i}{ \sqrt{n} }
            \right) \\
        &=
            b_\gamma
            +
            \epsilon^2
            \frac{ \gamma^{1/2} - 1 }{ 1 + \gamma^{1/2} \epsilon }
            +
            \OhP \left(
                \frac{\tsigma_i}{ \sqrt{n} }
            \right),
\end{align*}
where
\begin{align*}
    \begin{split}
    \tsigma_i^2
    &=
    \sigma_{ii}
    \left(
        1 - \frac{1}{1 + 2 \gamma^{1/2} \epsilon + \gamma \epsilon^2}
    \right)^2 \\
    &\quad+
    2\big( 2 (\gamma^{-1/2} + \epsilon)  + 1 + \gamma^{-1} \big)
    \left(
        1 - \frac{1}{1 + 2 \gamma^{1/2} \epsilon + \gamma \epsilon^2}    
    \right)
    \end{split} \\
    &= \Oh( \epsilon ).
\end{align*}
Therefore, for all $0 < \epsilon < 1$, we have
\[
    \sqrt{n}
    \left(
        \hmu_{n,i}
        - 
        b_\gamma 
        - \epsilon^2
        \frac{ \gamma^{1/2} - 1 }{ 1 + \gamma^{1/2} \epsilon }
    \right)
    \leq
    \OhP \left( \epsilon^{1/2} \right).
\]
Letting $\epsilon \to 0$, we get
\[
    \limsup_{n\to\infty} 
        \sqrt{n}
        \big(
            \hmu_{n,i}
            -
            b_\gamma
        \big)
        \leq
        \ohP(1).
\]
Together with the lower bound, this implies
\(
    \sqrt{n}
    \big(
        \hmu_{n,i}
        -
        b_\gamma
    \big)
    \toP
    0.
\)
\end{proof}

\subsection{Right singular vectors}

For $\mu_i > \gamma^{-1/2}$, we can get a right singular vector of $\frac{1}{\sqrt{n}} \mtX_n$ from the sequence of solution pairs 
$(z_{n,i}, \vx_{n,i})$ satisfiying 
$f_n(z_{n,i}, \vx_{n,i}) = 0$ and $z_{n,i} \toas \bmu_i$.  The vector
is parallel to
\[
    \vtx_{n,i}
    =
    \left(
    \begin{matrix}
        \vx_n \\
        - ( \mS_{n,22} - z_n \mI_{p-k} )^{-1} \mS_{n,21} \, \vx_n
    \end{matrix}
    \right).
\]
We just need to normalize this vector to have unit length.  The length
of $\vtx_n$ is given by
\[
    \| \vtx_n \|_2^2
        =
        \vx_n^\trans
        \left(
            \mI_{k}
            +
            \mS_{n,12}
            ( \mS_{n,22} - z_n \mI_{p-k} )^{-2} 
            \mS_{n,21}
        \right)
        \vx_n.
\]
It is straightforward to show that for $z > b_\gamma$,
\begin{align*}
    \mI_{k}
    +
    \mS_{n,12}
    ( \mS_{n,22} - z \mI_{p-k} )^{-2} 
    \mS_{n,21}
        &\toas
            -\mT_0'(z) \\
        &= \frac{ \gamma^{-1} m'(z) }
                { \big( 1 + \gamma^{-1} m(z) \big)^2 }
           \mD^2
           +
           \frac{ m'(z) }{ \big( m(z) \big)^2 }
           \mI_k
\end{align*}
uniformly for $z$ in any compact subset of $\big( b_\gamma, \infty \big)$.
It is also not hard to compute for $\mu_i > \gamma^{-1/2}$ that
\[
    -\mT_0'( \bmu_i )
        =
            \frac{ \gamma^{-1} }{ \mu_i^2 - \gamma^{-1} }
            \mD^2
            +
            \frac{ \mu_i^2 }{ \mu_i^2 - \gamma^{-1} }
            \mI_k.
\]
Therefore, if $\mu_i > \gamma^{-1/2}$ and 
$(z_{n,i}, \vx_{n,i}) \toas (\bmu_i, \ve_i)$, then
\begin{align*}
    \| \vtx_{n,i} \|_2^2
        &\toas
            \frac{ \mu_i ( \mu_i + \gamma^{-1} ) }
                 { \mu_i^2 - \gamma^{-1} } \\
        &=
            \frac{ 1 + \tfrac{1}{ \gamma \mu_i} }
                 { 1 - \tfrac{1}{ \gamma \mu^2} }.
\end{align*}

The behavior of the right singular vectors when $\mu_i \leq \gamma^{1/2}$ is
a little more difficult to get at.  We will use a variant of an argument from Paul~\cite{paul2007ase} to show that $\| \vtx \|_2 \toas \infty$, which implies that $\frac{ \vx_{n,i} }{ \| \vtx_{n,i} \|_2 } \toas 0$.  We can
do this by showing the smallest eigenvalue of 
\(
    \mS_{n,12}
    ( \mS_{n,22} - z_{n,i} \mI_{p-k} )^{-2} 
    \mS_{n,21}
\)
goes to $\infty$.

Write
\[
    \mS_{n,12}
    ( \mS_{n,22} - z_{n,i} \mI_{p-k} )^{-2} 
    \mS_{n,21}
        =
        \sum_{\alpha=1}^{p-k}
            \frac{ \vs_{n,\alpha} \, \vs_{n,\alpha}^\trans }
                 { ( \tlambda_{n,\alpha} - z_{n,i} )^2 },
\]
where $\vs_{n,1}, \vs_{n,2}, \ldots, \vs_{n,p-k}$ are the 
eigenvectors of $\mS_{n,22}$ multiplied by $\mS_{n,12} = \mS_{n,21}^\trans$, and
$\tlambda_{n,1}, \tlambda_{n,2}, \ldots, \tlambda_{n,p-k}$ are the eigenvalues
of $\mS_{n,22}$.  For $\epsilon > 0$, define the event 
\(
    J_{n}(\epsilon)
    =
    \left\{
        z_{n,i} < b_\gamma + \epsilon
    \right\}.
\)
From the interleaving inequality, we have $z_{n,i} \geq \tlambda_{n,i}$.
With respect to the ordering on positive-definite matrices, we have
\begin{align*}
    \sum_{\alpha=1}^{p-k}
            \frac{ \vs_{n,\alpha} \, \vs_{n,\alpha}^\trans }
                 { ( \tlambda_{n,\alpha} - z_{n,i} )^2 }
        &\succeq
            \sum_{\alpha=i}^{p-k}
                    \frac{ \vs_{n,\alpha} \, \vs_{n,\alpha}^\trans }
                         { ( \tlambda_{n,\alpha} - z_{n,i} )^2 } \\
        &\succeq
            \sum_{\alpha=i}^{p-k}
                    \frac{ \vs_{n,\alpha} \, \vs_{n,\alpha}^\trans }
                         { ( b_\gamma + \epsilon - z_{n,i} )^2 }
            \qquad\text{on $J_n(\epsilon)$.}
\end{align*}
It is not hard to show that
\(
    \left\|
        \sum_{\alpha=1}^{i-1}
                \frac{ \vs_{n,\alpha} \, \vs_{n,\alpha}^\trans }
                     { ( b_\gamma + \epsilon - z_{n,i} )^2 }
    \right\|
        \toas 0.
\)
Therefore,
\[
    \sum_{\alpha=i}^{p-k}
            \frac{ \vs_{n,\alpha} \, \vs_{n,\alpha}^\trans }
                 { ( b_\gamma + \epsilon - z_{n,i} )^2 }
        \toas
        \frac{ \gamma^{-1} m'\big( b_\gamma + \epsilon \big) }
             { 1 + \gamma^{-1} m\big( b_\gamma + \epsilon \big)}
        \mD^2
        +
        \frac{ m'\big( b_\gamma + \epsilon \big) }
             { \left[ m\big( b_\gamma + \epsilon \big) \right]^2}
        \mI_k.
\]
As $n\to\infty$, we have $\Prob\big( J_n(\epsilon) \big) \to 1$.  So,
since $m'\big( b_\gamma + \epsilon \big) \geq \frac{C}{\sqrt{\epsilon}}$
for some constant $C$, 
letting $\epsilon \to 0$ we must have that the smallest eigenvalue of
\(
    \mS_{n,12}
    ( \mS_{n,22} - z_{n,i} \mI_{p-k} )^{-2} 
    \mS_{n,21}
\)
goes to $\infty$.

\subsection{Left singular vectors}

We can get the left singular vectors from the right from multiplication
by $\frac{1}{\sqrt{n}} \mtX_n$.  Specifically, if $\vtv_{n,i}$ is a right
singular vector of $\frac{1}{\sqrt{n}} \mtX_n$ with singular value
$z_{n,i}^{1/2}$, then $\vtu_{n,i}$, the corresponding left singular vector,
is defined by
\[
    z_{n,i}^{1/2} \,
    \vtu_{n,i}
        = 
        \frac{1}{\sqrt{n}}
        \mtX_n \,
        \vtv_{n,i}.
\]
We are only interested in the first $k$ components of $\vtu_{n,i}$.  If
\[
    \vtv_{n,i}
    =
    \frac{1}{\| \vtx_{n,i} \|_2 }
    \left(
    \begin{matrix}
        \vx_{n,i} \\
        -( \mS_{n,22} - z_{n,i} \mI_{p-k} )^{-1} \mS_{n,21} \, \vx_{n,i}
    \end{matrix}
    \right),
\]
then these are given by
\(
    \frac{1}{\| \vtx_{n,i} \|_2 } \mR_n(z_{n,i}) \, \vx_{n,i},
\)
where
\[
    \mR_n(z)
    =
    \mD_n 
    + 
    \frac{1}{\sqrt{n}}
    \mE_{n,11}
    - 
    \frac{1}{\sqrt{n}}
    \mE_{n,12}
    \left( \mS_{n,22} - z \mI_{p-k} \right)^{-1}
    \mS_{n,21}.
\]
It is not hard to show that
\(
    \mR_n(z) \toas \mR_0(z),
\)
uniformly for $z > b_\gamma$, and
\(
    \mR_n(z) 
        = 
            \mR_0(z) 
            + 
            \frac{1}{\sqrt{n}} \mR_{n,1}(z) 
            + 
            \ohP\left( \frac{1}{\sqrt{n}} \right)
\)
pointwise for $z > b_\gamma$.  Here,
\[
    \mR_0(z) = \frac{1}{ 1 + \gamma^{-1} m(z) } \mD,
\]
and
\[
    \begin{split}
    \mR_{n,1}(z)
    &=
    \big( 1 + \gamma^{-1} m(z) \big)^{-1}
    \Big(
        \sqrt{n} \left( \mD_n  - \mD \right)
        +
        \mE_{n,11}
    \Big) \\
    &\quad- 
    \big( 1 + \gamma^{-1} m(z) \big)^{-2}
    \mF_{n}(z) \mD
    - 
    \mG_n(z).
    \end{split}
\]
Let $y_{n,i} = \frac{1}{\| \vtx_{n,i} \|_2 } \mR_n(z_{n,i}) \, \vx_{n,i}$.
A straightforward calculation shows the following:

\begin{lemma}
    If $\mu_i > \gamma^{-1/2}$ and 
    $(z_{n,i}, \vx_{n,i}) \toas (\bmu_i, \ve_i)$, then 
    \[
        \vy_{n,i}
            \toas
                \left(
                \frac{1 - \tfrac{ 1 }{ \gamma \mu_i^2 } }
                     { 1 + \tfrac{1}{\mu_i} } \right)^{1/2} \,
                \ve_i .
    \]
    If $\mu_i \leq \gamma^{-1/2}$ and $z_{n,i} \toas b_\gamma$, then
    \[
        \vy_{n,i} \toas 0.
    \]
\end{lemma}

\section{Results for $\gamma \in (0,1)$}\label{S:lowrank-gamma-lt-1}

Remarkably the formulas for the limiting quantities still hold when $\gamma < 1$.  To see this, we can get the behavior for $\gamma < 1$ by taking the transpose of $\mX_n$ and applying the theorem for $\gamma \geq 1$.  We switch the roles of $n$ and $p$, replace $\mu_i$ by $\mu_i' = \gamma \mu_i$, and replace $\gamma$ by $\gamma' = \gamma^{-1}$.  Then, for instance, the critical cutoff becomes
\begin{align*}
    \mu_i' &> \frac{1}{\sqrt{\gamma'}}, \quad\text{i.e.} \\
    \gamma \mu_i &> \gamma^{1/2},
\end{align*}
which is the same formula for $\gamma \geq 1$.  The almost-sure limit of
the square of the $i$th eigenvalue of $\frac{1}{p} \mX_n \mX_n^T$ becomes
\begin{align*}
    \bmu_i'
        &= ( \gamma \mu_i + 1 )
           \left( 1 + \frac{\gamma}{ \gamma \mu_i } \right) \\
        &= \gamma ( \mu_i + 1 ) \left( 1 + \frac{1}{\gamma \mu} \right) \\
        &= \gamma \bmu_i.
\end{align*}
The formulas for the other quantities also still remain true.

\section{Related work, extensions, and future work}\label{S:lowrank-related-extensions}

With the proofs completed, we now discuss some extensions and related work.

\subsection{Related work}

When Johnstone~\cite{johnstone2001dle} worked out the distribution of the largest eigenvalue in the null (no signal) case, he proposed studying ``spiked'' alternative models.  Spiked data consists of vector-valued observations with population covariance of the form
\[
    \Sigma = \diag( \mu_1, \mu_2, \ldots, \mu_k, 1, \ldots, 1 ).
\]
Work on the spiked model started with Baik et al.~\cite{baik2005ptl}, 
Baik \& Silverstein~\cite{baik2006els}, and Paul~\cite{paul2007ase}.  Baik et al. showed that for complex Gaussian data, a phase transition phenomenon exists, depending on the relative magnitude of the spike.  Baik \& Silverstein gave the almost-sure limits of the eigenvalues from the sample covariance matrix without assuming Gaussianity.  Paul worked with real Gaussian data and gave the limiting distributions when $\gamma > 1$.  Paul also gives some results about the eigenvectors.

After this initial work, Bai \& Yao~\cite{bai2008clt}~\cite{bai2008lts} derived the almost-sure limits and prove a central limit theorem for eigenvalues for a general class of data that includes colored noise and non-Gaussianity.  Chen et al~\cite{chen2009ppc} consider another type of spiked model with correlation.  Nadler~\cite{nadler2008fsa} derived the behavior of the first eigenvector in a spiked model with one spike.

The above authors all consider data of the form $\mX = \mZ \mSigma^{1/2}$.
Onatski~\cite{onatski2009}, like us, examines data of the form 
$\mX = \mU \mD \mV^T + \mE$.  With slightly different assumptions than ours, he is able to give the probability limits of the top eigenvalues, along with the marginal distributions of the scaled eigenvalues and singular vectors.  However, Onatski does not work in a ``transpose-agnostic'' framework like we do, so his methods do not allow getting at the joint distribution of the left and right singular vectors.

\subsection{Extensions and future work}

We have stopped short of computing the second-order behavior of the singular vectors, but no additional theory is required to get at these quantities.  Anyone patient enough can use our results to compute the joint distribution of 
$\| \vtx_{n,i} \|_2$, $\vx_{n,i}$, and $\vy_{n,i}$.  This in turn will give the joint behavior of the singular vectors.  

Most of the proof remains unchanged for complex Gaussian noise, provided
transpose ($^\trans$) is replaced by conjugate-transpose ($^\herm$).  The
variance formulas need a small modification, since the fourth-moment of
a real Gaussian is $3$ and that of a complex Gaussian is $2$.

For colored or non-Gaussian noise, we no longer have orthogonal invariance, so the change of basis in Section~\ref{S:lowrank-preliminaries} is a little trickier.  It is likely that comparable results can still be found, perhaps using results on the eigenvectors of general sample covariance matrices from Bai et al. \cite{bai2007ael}.

