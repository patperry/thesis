
\chapter{Cross-validation for unsupervised learning}

As Moore's Law progresses, data sets measuring on the order of hundreds or
thousands of variables are becoming increasingly more common. Making sense of
data of this size is simply not tractable without imposing a simplifying
model. One popular simplification is to posit existence of a small number of
common factors that drive the dynamics of the data, which are usually
estimated by principal components analysis (PCA) or some variation thereof.
The $n \times p$ data matrix $\mX$ is approximated as a low-rank product $\mX
\approx \mU \mV^\trans$, where $\mU$ and $\mV$ are $n \times k$ and $p \times
k$, respectively, with $k$ much smaller than $n$ and $p$.

The number of algorithms for approximating matrices by low-rank products has
exploded in recent years. These algorithms include archetypal analysis
\cite{cutler1994aa}, the semi-discrete decomposition (SDD)
\cite{kolda1998smd}, the non-negative matrix factorization (NMF)
\cite{lee1999lpo}, the plaid model \cite{lazzeroni2002pmg}, the $CUR$
decomposition \cite{drineas2007fmc}, and regularized versions thereof. They
also include some clustering methods, in particular $k$-means and fuzzy
$k$-means \cite{bezdek1980fmp}.

A prevailing question is: How many common factors underly a data set? In other
words, how should one choose $k$? In general, the answer to this question is
application-specific. If we are trying to use $\mX$ to predict a response,
$y$, then the optimal $k$ is the one that gives the best prediction error
for $y$. The situation is not always this simple, though. For exploratory
analysis, there is no external response, and we want to choose the $k$ that is
``intrinsic'' to $\mX$. For other applications, we don't just have a single
response, $y$, we have \emph{many} responses $y_1, y_2, \ldots, y_m$.
For computational reasons, we may not know all of the $y_i$ when we are
processing $\mX$. In these situations, we want a $k$ that has good
average-case or worst-case prediction properties for a large class of 
responses.


\section{The latent factor model}\label{S:latent-factor-model}

Suppose that we have $n$ multivariate observations $\vx_{1}, \vx_{2}, \ldots,
\vx_{n} \in \reals^p$. In a microarray setting, we will have about $n=50$
arrays measuring the activations of around $p=5000$ genes. Alternatively, for
financial applications $\vx_i$ will measure the market value of around
$p=1000$ assets on day $i$, and we may be looking at data from the last three
years, so $n \approx 1000$. In these situations and others like them, we can
often convince ourselves that there aren't really $1000$ different things
going on in the data. Probably, there are a small number, $k$ of unobserved
factors driving the dynamics of the data. Typically, we think $k$ is on the
order of around $5$ or $10$.

For genomics applications, we don't really think that all $p=5000$ measured
genes are behaving independently. To the contrary, we think that there are a
small number of biological processes that determine how much of each protein
gets produced. In finance, while it is true that the stock prices of
individual companies have a certain degree of independence, often macroscopic
effects like industry- and market-wide trends account for a substantial
portion of the value.

\subsection{The spiked model}

One way to model latent effects is to assume that the $\vx_i$ are \iid and
that their covariance is ``spiked''. We think that $\vx_i$ is a weighted
combination of $k$ latent factors corrupted by additive white noise. In this
case, $\vx_i$ can be decomposed as
\begin{align}
    \vx_i
        &=
        \sum_{j=1}^k
            \va_j
            s_{i,j}
        +
        \vepsilon_i \notag \\
        &=
        \mA \vs_i + \vepsilon_i,
\end{align}
where
\(
    \mA
    =
    \left(
    \begin{matrix}
        \va_1 & \va_2 & \cdots & \va_k
    \end{matrix}
    \right)
\)
is a $p \times k$ matrix of latent factors common to all observations and
$\vs_i$ is a vector of loadings for the $i$th observation. We assume that the
noise vector $\vepsilon_i$ is distributed as $\Normal( 0, \, \sigma^2 \mI_p
)$. If the loadings have mean zero and covariance $\mSigma_\text{S} \in
\reals^{k \times k}$, and if they are also independent of the noise, then
$\vx_1$ has covariance
\begin{equation}\label{E:spiked-cov-natural-form}
    \mSigma
        \define
        \E \left[ \vx_1 \, \vx_1^\trans \right]
            =
                \mA \mSigma_\text{S} \mA^\trans
                +
                \sigma^2
                \mI_p.
\end{equation}
The decomposition in \eqref{E:spiked-cov-natural-form} can be reparametrized
as
\begin{equation}\label{E:spiked-cov-identifiable-form}
    \mSigma
        =
        \mQ \mLambda \mQ^\trans
        +
        \sigma^2
        \mI_p,
\end{equation}
where $\mQ^\trans \mQ = \mI_k$ and
\(
    \mLambda
        = 
        \diag \left( \lambda_1, \lambda_2, \ldots, \lambda_k \right),
\)
with $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_k \geq 0$.
Equation~\eqref{E:spiked-cov-identifiable-form} makes it apparent that
$\mSigma$ is ``spiked'', in the sense that most of its eigenvalues are equal,
but $k$ eigenvalues stand out above the bulk. The first $k$ eigenvalues are
$\lambda_1 + \sigma^2, \lambda_2 + \sigma^2, \ldots, \lambda_k + \sigma^2$,
and the remaining $p-k$ eigenvalues are equal to $\sigma^2$.

\subsection{More general matrix models}

We can introduce a model more general than the spiked one by specifying
a distribution for the $n\times p$ data matrix 
\(
    \mX
    =
    \left(
    \begin{matrix}
        \vx_1 &
        \vx_2 &
        \cdots &
        \vx_n
    \end{matrix}
    \right)^\trans
\)
that includes dependence between the rows.  In the spiked model, the
distribution of $\mX$ can be described as
\begin{equation}\label{E:data-matrix-spiked}
    \mX
        \eqd
        \mZ
        \mLambda^{1/2}
        \mQ^\trans
        +
        \mE,
\end{equation}
where 
\(
    \mE 
        = 
        \left(
        \begin{matrix}
            \vepsilon_1 & \vepsilon_2 & \cdots \vepsilon_n
        \end{matrix}
        \right)^\trans
\)
and $\mZ$ is an $n \times k$ matrix of independent 
$\Normal \left( 0, \, 1 \right)$ elements.  More generally, we can consider
data of the form
\begin{equation}\label{E:data-matrix-general}
    \mX
        \eqd
            \sqrt{n} \,
            \mU
            \mD
            \mV^\trans
            +
            \mE,
\end{equation}
where $\mU^\trans \mU = \mV^\trans \mV = \mI_k$, and $\mD = \diag( d_1, d_2,
\ldots, d_k )$ with $d_1 \geq d_2 \geq \cdots \geq d_k \geq 0$. We can get
\eqref{E:data-matrix-general} from \eqref{E:data-matrix-spiked} by letting
$\mZ \mLambda^{1/2} = \sqrt{n} \, \mU \mD \mC^\trans$ be the SVD of $\mZ
\mLambda^{1/2}$ and defining $\mV = \mQ \mC$. Unlike the spiked model,
\eqref{E:data-matrix-general} can model dependence between variables as well
as dependence between observations.

\section{An intrinsic notion of rank}

With Section~\ref{S:latent-factor-model}'s latent factor model in mind, we
turn our attention to defining the intrinsic rank of a data set. This
definition will be motivated both by the generative model for $\mX$ and by
predictive power considerations.  When 
\(
    \mX = \sqrt{n} \, \mU \mD \mV^\trans + \mE,
\)
we think of $\sqrt{n} \, \mU \mD \mV^\trans$ as ``signal'' and $\mE$ as ``noise''.  We make a distinction between the generative rank and the effective rank.

\begin{definition}
    If the $n \times p$ matrix $\mX$ is distributed as
    \(
        \mX = \sqrt{n} \, \mU \mD \mV^\trans + \mE,
    \)
    where $\mU^\trans \mU = \mV^\trans \mV = \mI_{k_0}$, $\mD$ is a
    $k_0 \times k_0$ diagonal matrix with positive diagonal entries, and $\mE$
    is a noise matrix independent of the signal term whose elements are
    \iid $\Normal( 0, \, \sigma^2 )$, then
    we denote by $k$ the \emph{generative rank} of $\mX$.
\end{definition}

\noindent 
Intuitively the generative rank is the rank of the signal part of $\mX$.

The effective rank is defined in terms of how well the first terms of the
SVD of $\mX$ approximates the signal $\sqrt{n} \, \mU \mD \mV^\trans$.  We
let $\mX = \sqrt{n} \mhU \mhD \mhV^\trans$ be the full SVD of $\mX$, where
\(
    \mhU
    =
    \left(
    \begin{matrix}
        \vhu_1 & \vhu_2 & \cdots & \vhu_{n\wedge p}
    \end{matrix}
    \right),
\)
\(
    \mhV
    =
    \left(
    \begin{matrix}
        \vhv_1 & \vhv_2 & \cdots & \vhv_{n\wedge p}
    \end{matrix}
    \right),
\)
and
\(
    \mhD
    =
    \diag \left(
        \hd_1, \hd_2, \ldots, \hd_{n\wedge p}
    \right).
\)
If we let
\(
    \mhD(k)
    =
    \diag \left(
        \hd_1, \hd_2, \ldots, \hd_k, 0, 0, \ldots, 0
    \right),
\)
then the SVD truncated to $k$ terms is
\(
    \mhX(k)
    =
    \sqrt{n} \,
    \mhU \mhD(k) \mhV^\trans.
\)
We are now in a position to define effective rank

\begin{definition}
    Given a loss function
    \(
        L : \reals^{n \times p} \times \reals^{n \times p} \to \reals,
    \)
    the effective rank of $\mX$ with respect to $L$ is equal to
    \[
        k^\ast
        \define
        \argmin_k \, 
            L \left( 
                \sqrt{n} \mU \mD \mV^\trans\!, \, \mhX(k) 
            \right).
    \]
\end{definition}

The effective rank depends on the choice of loss function.  In some settings it is preferable to choose an application-specific loss function.  We appeal to simplicity and convenience, choosing squared Frobenius loss.  Specifically,
\[
    L_F ( \mA, \, \mA' )
        = \| \mA - \mA' \|_F^2.
\]
One way to motivate this loss function is that it measures average squared error over all linear functionals of the form 
$y( \valpha, \, \vbeta) = \valpha^\trans \mA \, \vbeta$;
\[
    \frac{1}{np} \,
    \| \mA - \mA' \|_F^2
    =
    \int_{ \| \valpha \|_2 = 1}
    \int_{ \| \vbeta \|_2 = 1}
        \left(
            \valpha^\trans \mA \, \vbeta
            -
            \valpha^\trans \mA' \vbeta
        \right)^2
        \,
        d\vbeta \, d\valpha.
\]
In the context of the latent factor model, the effective rank with respect
to $L_F$ is the rank that gives the best average-case predictions of linear functionals of the signal part (with respect to squared-error loss). 

A common alternative to Frobenius loss is spectral loss, given by
\[
    L_2 ( \mA, \, \mA' )
        = \| \mA - \mA' \|_2^2.
\]
This can be interpreted as worst-case squared error over the class of all linear functionals,
\[
    \| \mA - \mA' \|_2^2
        =
            \sup_{ \| \valpha \|_2 = 1 }
            \sup_{ \| \vbeta \|_2  = 1 }
                \left(
                    \valpha^\trans \mA \, \vbeta
                    -
                    \valpha^\trans \mA' \vbeta
                \right)^2.
\]
